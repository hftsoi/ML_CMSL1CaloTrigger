{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Autoencoder Training for Anomaly Detection @ L1Trigger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "import keras_tuner\n",
    "from keras_tuner import Hyperband\n",
    "\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input files reading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All input files are already sorted in Calo regions (i, j) ~ (18, 14)<br>\n",
    "Where i = 0 -> 17 corresponds to GCT_Phi = 0 -> 17<br>\n",
    "Where j = 0 -> 13 corresponds to RCT_Eta = 4 -> 17\n",
    "\n",
    "Keep this ordering as is when feeding into neural nets. Also keep this in mind when generating/preparing new samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zerobias and MC signal files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ZeroBias = np.concatenate((h5py.File('bkg/ZeroBias_0.h5', 'r')['CaloRegions'][()],\n",
    "                           h5py.File('bkg/ZeroBias_1.h5', 'r')['CaloRegions'][()],\n",
    "                           h5py.File('bkg/ZeroBias_2.h5', 'r')['CaloRegions'][()]))\n",
    "ZeroBias = ZeroBias.astype(dtype = 'float32').reshape(-1, 18, 14, 1)\n",
    "print('ZeroBias shape: ' + str(ZeroBias.shape))\n",
    "\n",
    "MC_files = []\n",
    "MC_files.append('bkg/110X/QCD_0.h5')#i=0\n",
    "#MC_files.append('bkg/110X/QCD_1.h5')\n",
    "#MC_files.append('bkg/110X/QCD_2.h5')\n",
    "MC_files.append('bkg/120X/SingleNeutrino_E-10_0.h5')#i=1\n",
    "#MC_files.append('bkg/120X/SingleNeutrino_E-10_1.h5')\n",
    "#MC_files.append('bkg/120X/SingleNeutrino_E-10_2.h5')\n",
    "MC_files.append('bkg/120X/SingleNeutrino_Pt-2To20_0.h5')#i=2\n",
    "#MC_files.append('bkg/120X/SingleNeutrino_Pt-2To20_1.h5')\n",
    "#MC_files.append('bkg/120X/SingleNeutrino_Pt-2To20_2.h5')\n",
    "\n",
    "MC_files.append('sig/110X/GluGluToHHTo4B_node_SM_TuneCP5_14TeV.h5')#i=3\n",
    "MC_files.append('sig/110X/HTo2LongLivedTo4mu_MH-1000_MFF-450_CTau-10000mm_TuneCP5_14TeV.h5')\n",
    "MC_files.append('sig/110X/HTo2LongLivedTo4mu_MH-125_MFF-12_CTau-900mm_TuneCP5_14TeV.h5')\n",
    "MC_files.append('sig/110X/HTo2LongLivedTo4mu_MH-125_MFF-25_CTau-1500mm_TuneCP5_14TeV.h5')\n",
    "MC_files.append('sig/110X/HTo2LongLivedTo4mu_MH-125_MFF-50_CTau-3000mm_TuneCP5_14TeV.h5')\n",
    "MC_files.append('sig/110X/VBFHToTauTau_M125_TuneCUETP8M1_14TeV.h5')\n",
    "MC_files.append('sig/110X/VBF_HH_CV_1_C2V_1_C3_1_TuneCP5_PSweights_14TeV.h5')\n",
    "MC_files.append('sig/110X/VBF_HToInvisible_M125_TuneCUETP8M1_14TeV.h5')\n",
    "MC_files.append('sig/110X/VectorZPrimeToQQ_M100_pT300_TuneCP5_14TeV.h5')\n",
    "MC_files.append('sig/110X/VectorZPrimeToQQ_M200_pT300_TuneCP5_14TeV.h5')\n",
    "MC_files.append('sig/110X/VectorZPrimeToQQ_M50_pT300_TuneCP5_14TeV.h5')#i=13\n",
    "MC_files.append('sig/110X/ZprimeToZH_MZprime1000_MZ50_MH80_ZTouds_HTouds_narrow_TuneCP5_14TeV.h5')\n",
    "MC_files.append('sig/110X/ZprimeToZH_MZprime600_MZ50_MH80_ZTouds_HTouds_narrow_TuneCP5_14TeV.h5')\n",
    "MC_files.append('sig/110X/ZprimeToZH_MZprime800_MZ50_MH80_ZTouds_HTouds_narrow_TuneCP5_14TeV.h5')\n",
    "MC_files.append('sig/120X/GluGluHToTauTau_M-125_TuneCP5_14TeV.h5')\n",
    "MC_files.append('sig/120X/GluGluToHHTo4B_node_cHHH1_TuneCP5_14TeV.h5')\n",
    "MC_files.append('sig/120X/GluGluToHHTo4B_node_cHHH5_TuneCP5_14TeV.h5')\n",
    "MC_files.append('sig/120X/HTo2LongLivedTo4b_MH-1000_MFF-450_CTau-100000mm_TuneCP5_14TeV.h5')\n",
    "MC_files.append('sig/120X/HTo2LongLivedTo4b_MH-1000_MFF-450_CTau-10000mm_TuneCP5_14TeV.h5')\n",
    "MC_files.append('sig/120X/HTo2LongLivedTo4b_MH-125_MFF-12_CTau-9000mm_TuneCP5_14TeV.h5')\n",
    "MC_files.append('sig/120X/HTo2LongLivedTo4b_MH-125_MFF-12_CTau-900mm_TuneCP5_14TeV.h5')#i=23\n",
    "MC_files.append('sig/120X/HTo2LongLivedTo4b_MH-125_MFF-25_CTau-15000mm_TuneCP5_14TeV.h5')\n",
    "MC_files.append('sig/120X/HTo2LongLivedTo4b_MH-125_MFF-25_CTau-1500mm_TuneCP5_14TeV.h5')\n",
    "MC_files.append('sig/120X/HTo2LongLivedTo4b_MH-125_MFF-50_CTau-30000mm_TuneCP5_14TeV.h5')\n",
    "MC_files.append('sig/120X/HTo2LongLivedTo4b_MH-125_MFF-50_CTau-3000mm_TuneCP5_14TeV.h5')\n",
    "MC_files.append('sig/120X/HTo2LongLivedTo4b_MH-250_MFF-120_CTau-10000mm_TuneCP5_14TeV.h5')\n",
    "MC_files.append('sig/120X/HTo2LongLivedTo4b_MH-250_MFF-120_CTau-1000mm_TuneCP5_14TeV.h5')\n",
    "MC_files.append('sig/120X/HTo2LongLivedTo4b_MH-250_MFF-60_CTau-1000mm_TuneCP5_14TeV.h5')\n",
    "MC_files.append('sig/120X/HTo2LongLivedTo4b_MH-350_MFF-160_CTau-10000mm_TuneCP5_14TeV.h5')\n",
    "MC_files.append('sig/120X/HTo2LongLivedTo4b_MH-350_MFF-160_CTau-1000mm_TuneCP5_14TeV.h5')\n",
    "MC_files.append('sig/120X/HTo2LongLivedTo4b_MH-350_MFF-160_CTau-500mm_TuneCP5_14TeV.h5')#i=33\n",
    "MC_files.append('sig/120X/HTo2LongLivedTo4b_MH-350_MFF-80_CTau-10000mm_TuneCP5_14TeV.h5')\n",
    "MC_files.append('sig/120X/HTo2LongLivedTo4b_MH-350_MFF-80_CTau-1000mm_TuneCP5_14TeV.h5')\n",
    "MC_files.append('sig/120X/HTo2LongLivedTo4b_MH-350_MFF-80_CTau-500mm_TuneCP5_14TeV.h5')\n",
    "MC_files.append('sig/120X/HTo2LongLivedTo4mu_MH-1000_MFF-450_CTau-10000mm_TuneCP5_14TeV.h5')\n",
    "MC_files.append('sig/120X/HTo2LongLivedTo4mu_MH-125_MFF-12_CTau-900mm_TuneCP5_14TeV.h5')\n",
    "MC_files.append('sig/120X/HTo2LongLivedTo4mu_MH-125_MFF-25_CTau-1500mm_TuneCP5_14TeV.h5')\n",
    "MC_files.append('sig/120X/HTo2LongLivedTo4mu_MH-125_MFF-50_CTau-3000mm_TuneCP5_14TeV.h5')\n",
    "MC_files.append('sig/120X/SUSYGluGluToBBHToBB_NarrowWidth_M-1200_TuneCP5_13TeV-pythia814TeV.h5')\n",
    "MC_files.append('sig/120X/SUSYGluGluToBBHToBB_NarrowWidth_M-120_TuneCP5_14TeV.h5')\n",
    "MC_files.append('sig/120X/SUSYGluGluToBBHToBB_NarrowWidth_M-350_TuneCP5_14TeV.h5')#i=43\n",
    "MC_files.append('sig/120X/SUSYGluGluToBBHToBB_NarrowWidth_M-600_TuneCP5_14TeV.h5')\n",
    "MC_files.append('sig/120X/TprimeBToTH_M-650_LH_TuneCP5_14TeV.h5')\n",
    "MC_files.append('sig/120X/VBFHHTo4B_CV_1_C2V_2_C3_1_TuneCP5_14TeV.h5')\n",
    "MC_files.append('sig/120X/VBFHToInvisible_M125_TuneCP5_14TeV.h5')\n",
    "MC_files.append('sig/120X/VBFHToTauTau_M125_TuneCP5_14TeV.h5')\n",
    "MC_files.append('sig/120X/VectorZPrimeGammaToQQGamma_M-10_GPt-75_TuneCP5_14TeV.h5')\n",
    "MC_files.append('sig/120X/VectorZPrimeToQQ_M-100_Pt-300_TuneCP5_14TeV.h5')\n",
    "MC_files.append('sig/120X/VectorZPrimeToQQ_M-200_Pt-300_TuneCP5_14TeV.h5')#i=51\n",
    "\n",
    "MC = []\n",
    "AcceptanceFlag = []\n",
    "for i in range(len(MC_files)):\n",
    "    MC.append(h5py.File(MC_files[i], 'r')['CaloRegions'][()].astype(dtype = 'float32'))\n",
    "    MC[i] = MC[i].reshape(-1, 18, 14, 1)\n",
    "    #Read acceptance flag in MC signals\n",
    "    if i > 2:\n",
    "        AcceptanceFlag.append(h5py.File(MC_files[i], 'r')['AcceptanceFlag'][()])\n",
    "    else:\n",
    "        AcceptanceFlag.append(np.ones((MC[i].shape[0])))\n",
    "    \n",
    "MC[0] = MC[0][:100000,:,:,:]#QCD\n",
    "MC[1] = MC[1][:100000,:,:,:]#SingleNu_E10\n",
    "MC[2] = MC[2][:100000,:,:,:]#SingleNu_Pt2To20\n",
    "\n",
    "'''\n",
    "#/nfs_scratch/dasu/2022-02-04/L1TSignalZerobiasMixer/cms-vbfh.csv\n",
    "vbf = pd.read_csv('cms-vbfh.csv')\n",
    "vbf.columns = ['eta','phi','et','pos','ebit','tbit']\n",
    "vbf = vbf[251:]\n",
    "\n",
    "event_col = []\n",
    "for i in range(round(vbf.shape[0]/252)):\n",
    "    for j in range(252):\n",
    "        event_col.append(i)\n",
    "        \n",
    "vbf['event'] = event_col\n",
    "vbf = vbf.drop(['pos','ebit','tbit'],axis=1)\n",
    "vbf = vbf.sort_values(by=['event', 'phi', 'eta'], ascending = [True, True, True])\n",
    "vbf = vbf.reindex(columns=['event','phi','eta','et'])\n",
    "vbf = vbf.drop(['event'],axis=1)\n",
    "vbf = vbf.to_numpy()\n",
    "vbf = vbf.reshape((-1,18,14,3))\n",
    "vbf = vbf[:,:,:,2]\n",
    "vbf = vbf.reshape((-1,18,14,1))\n",
    "vbf.shape\n",
    "MC_files.append('/nfs_scratch/dasu/2022-02-04/L1TSignalZerobiasMixer/cms-vbfh.csv')\n",
    "MC.append(vbf)\n",
    "'''\n",
    "\n",
    "#Throw away MC signal events that failed to pass the acceptance cuts\n",
    "acceptance_filter = []\n",
    "for i in range(len(MC_files)):\n",
    "    acceptance_filter.append([])\n",
    "    for j in range(MC[i].shape[0]):\n",
    "        if AcceptanceFlag[i][j] == 1:\n",
    "            acceptance_filter[i].append(True)\n",
    "        else:\n",
    "            acceptance_filter[i].append(False)\n",
    "    MC[i] = MC[i][acceptance_filter[i],:,:,:]\n",
    "    print('i = ' + str(i) + ': ' + str(MC[i].shape) + '; accepted ' + str(np.round(np.mean(AcceptanceFlag[i]), 4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Throw away events with max pt > 1023 GeV, since the calo system cannot produce more than that (input pt is 10 bits)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filter1023_zb = []\n",
    "for i in range(ZeroBias.shape[0]):\n",
    "    if ZeroBias[i,:,:,0].max() > 1023:\n",
    "        filter1023_zb.append(False)\n",
    "    else:\n",
    "        filter1023_zb.append(True)\n",
    "ZeroBias = ZeroBias[filter1023_zb,:,:,:]\n",
    "print('ZeroBias shape = ' + str(ZeroBias.shape) + '; fraction left = ' + str(round(ZeroBias.shape[0]/len(filter1023_zb),4)))\n",
    "\n",
    "filter1023_mc = []\n",
    "for i in range(len(MC_files)):\n",
    "    filter1023_mc.append([])\n",
    "    for j in range(MC[i].shape[0]):\n",
    "        if MC[i][j,:,:,0].max() > 1023:\n",
    "            filter1023_mc[i].append(False)\n",
    "        else:\n",
    "            filter1023_mc[i].append(True)\n",
    "    MC[i] = MC[i][filter1023_mc[i],:,:,:]\n",
    "    print('i = ' + str(i) + ': ' + str(MC[i].shape) + '; fraction left = ' + str(round(MC[i].shape[0]/len(filter1023_mc[i]),4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MC samples are clean, so we need to overlay them with ZB to be more realistic before doing any training/testing. It can be achieved by simple region-by-region addition between the two: MC(i,j) = MC(i,j) + ZB(i,j), where the ZB can be chosen at random per MC event."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "MC_zb = []\n",
    "for i in range(len(MC)):\n",
    "    MC_zb.append(np.empty((MC[i].shape[0], 18, 14, 1)))\n",
    "    ZB_random_event = np.random.randint(low = 0, high = ZeroBias.shape[0], size = MC[i].shape[0])\n",
    "    for j in range(MC[i].shape[0]):\n",
    "        MC_zb[i][j, :, :, 0] = ZeroBias[ZB_random_event[j], :, :, 0] + MC[i][j, :, :, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the calo region plots before and after the overlay,\n",
    "\n",
    "where n = 0 (QCD), 1 (SingleNu_E10), 2 (SingleNu_Pt2To20), 3... (signals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 5\n",
    "for i in range(40,50):\n",
    "    fig, ax = plt.subplots(figsize = (10,10))\n",
    "    print(str(MC_files[n]))\n",
    "    ax = plt.subplot(2, 2, 1)\n",
    "    ax = sns.heatmap(MC[n][i,:,:,0].reshape(18, 14), vmin = 0, vmax = MC[n][i,:,:,0].max(), cmap = \"Reds\", cbar_kws = {'label': 'Normalized ET'})\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    ax.set_title('MC')\n",
    "    \n",
    "    ax = plt.subplot(2, 2, 2)\n",
    "    ax = sns.heatmap(MC_zb[n][i,:,:,0].reshape(18, 14), vmin = 0, vmax = MC[n][i,:,:,0].max(), cmap = \"Reds\", cbar_kws = {'label': 'Normalized ET'})\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    ax.set_title('MC+ZB')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at some ZB statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ZB_mean = np.mean(ZeroBias, axis = 0)\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (10,10))\n",
    "ax = plt.subplot(2, 2, 1)\n",
    "ax = sns.heatmap(ZB_mean.reshape(18, 14), vmin = 0, vmax = ZB_mean.max(), cmap = \"Reds\", cbar_kws = {'label': 'ET (GeV)'})\n",
    "ax.get_xaxis().set_visible(False)\n",
    "ax.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(ZeroBias.reshape((-1)), bins = 20, log = True)\n",
    "plt.xlabel(\"ZeroBias Et\")\n",
    "plt.show()\n",
    "\n",
    "print('Mean ZeroBias pT = ' + str(np.mean(ZeroBias.reshape(-1))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very few ZB events might have high pt regions since they could contain signal, do we want to put a cut on ZB before training?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_filter = []\n",
    "for i in range(ZeroBias.shape[0]):\n",
    "    if ZeroBias[i,:,:,0].max() < 30.0:\n",
    "        pt_filter.append(True)\n",
    "    else:\n",
    "        pt_filter.append(False)\n",
    "ZeroBias_ptcut = ZeroBias[pt_filter,:,:,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter searching (no quantization here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to train with custom loss functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "def custom_loss_for_train():\n",
    "    def func(y_true, y_pred):\n",
    "        #MSE(output, input)\n",
    "        loss = K.mean((y_pred - y_true)**2, axis = [1, 2, 3])\n",
    "        \n",
    "        #MSE(output, mean ZB)\n",
    "        #loss = K.mean((y_pred - ZB_mean)**2, axis = [1, 2, 3])\n",
    "        \n",
    "        #MSE(output, 0) for denoising\n",
    "        #loss = K.mean(y_pred**2, axis = [1, 2, 3])\n",
    "        \n",
    "        return loss\n",
    "    return func"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hypermodel for convolutional autoencoder (usually a teacher model for Knowledge Distillation later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hypermodel(hp):\n",
    "    hp_model = tf.keras.Sequential()\n",
    "    hp_model.add(tf.keras.layers.InputLayer(input_shape = (18, 14, 1)))\n",
    "    hp_model.add(layers.Conv2D(filters = hp.Int('filters_1',\n",
    "                                                min_value = 15,\n",
    "                                                max_value = 25,\n",
    "                                                step = 2),\n",
    "                               kernel_size = (3, 3),\n",
    "                               activation = 'relu',\n",
    "                               strides = 1,\n",
    "                               padding = 'same'))\n",
    "    \n",
    "    hp_model.add(layers.AveragePooling2D((2, 2)))\n",
    "    hp_model.add(layers.Conv2D(filters = hp.Int('filters_2',\n",
    "                                                min_value = 15,\n",
    "                                                max_value = 25,\n",
    "                                                step = 2),\n",
    "                               kernel_size = (3, 3),\n",
    "                               activation = 'relu',\n",
    "                               strides = 1,\n",
    "                               padding = 'same'))\n",
    "    \n",
    "    hp_model.add(layers.Conv2D(filters = 1,\n",
    "                               kernel_size = (3, 3),\n",
    "                               activation = 'relu',\n",
    "                               strides = 1,\n",
    "                               padding = 'same'))\n",
    "    \n",
    "    hp_model.add(layers.Conv2D(filters = hp.Int('filters_3',\n",
    "                                                min_value = 15,\n",
    "                                                max_value = 25,\n",
    "                                                step = 2),\n",
    "                               kernel_size = (3, 3),\n",
    "                               activation = 'relu',\n",
    "                               strides = 1,\n",
    "                               padding = 'same'))\n",
    "    \n",
    "    hp_model.add(layers.UpSampling2D((2, 2)))\n",
    "    hp_model.add(layers.Conv2D(filters = hp.Int('filters_4',\n",
    "                                                min_value = 15,\n",
    "                                                max_value = 25,\n",
    "                                                step = 2),\n",
    "                               kernel_size = (3, 3),\n",
    "                               activation = 'relu',\n",
    "                               strides = 1,\n",
    "                               padding = 'same'))\n",
    "    \n",
    "    hp_model.add(layers.Conv2D(filters = 1, kernel_size = (3, 3), activation = 'relu', strides = 1, padding = 'same'))\n",
    "    hp_model.compile(optimizer = 'adam', loss = custom_loss_for_train())\n",
    "    return hp_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hypermodel for shallowly dense (usually a student model for Knowledge Distillation later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hypermodel(hp):\n",
    "    hp_model = tf.keras.Sequential()\n",
    "    hp_model.add(tf.keras.layers.InputLayer(input_shape = (18, 14, 1)))\n",
    "    hp_model.add(tf.keras.layers.Flatten())\n",
    "    hp_model.add(tf.keras.layers.Dense(units = hp.Int('units_1',\n",
    "                                                      min_value = 10,\n",
    "                                                      max_value = 40,\n",
    "                                                      step = 2),\n",
    "                                       activation = 'relu'))\n",
    "    hp_model.add(tf.keras.layers.Dropout(rate = 0.3))\n",
    "    hp_model.add(tf.keras.layers.Dense(1, activation = 'relu'))\n",
    "    hp_model.compile(optimizer = 'adam', loss = 'mse')\n",
    "    return hp_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set configuration for tuner (Hyperband)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = Hyperband(hypermodel,\n",
    "                 objective = 'val_loss',\n",
    "                 max_epochs = 20,\n",
    "                 factor = 3, #number of models to train in a bracket = 1+log_factor(max_epochs)\n",
    "                 hyperband_iterations = 2, #number of times to iterate over the full Hyperband algorithm\n",
    "                 seed = 10,\n",
    "                 directory = 'hypertuning',\n",
    "                 project_name = 'tune',\n",
    "                 overwrite = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Partition the dataset into train/val/test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = ZeroBias\n",
    "\n",
    "train_ratio = 0.5\n",
    "val_ratio = 0.15\n",
    "test_ratio = 1 - train_ratio - val_ratio\n",
    "X_train_val, X_test = train_test_split(X, test_size = test_ratio, random_state = 123)\n",
    "X_train, X_val = train_test_split(X_train_val, test_size = val_ratio/(val_ratio + train_ratio), random_state = 123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the search. Mind the label when training for reconstruction or something else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.search(X_train, X_train,\n",
    "            epochs = 20,\n",
    "            validation_data = (X_val, X_val),\n",
    "            batch_size = 256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the best models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.results_summary(num_trials = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take one of them for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hp = tuner.get_best_hyperparameters()[0]\n",
    "model = tuner.hypermodel.build(best_hp)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolutional autoencoder to be trained for input reconstruction (to be used as a teacher model for Knowledge Distillation later).\n",
    "\n",
    "The encoder part, transforming the (18, 14) region input into a smaller latent space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input = tf.keras.Input(shape = (18, 14, 1))\n",
    "encoding = layers.Conv2D(21, (3, 3), activation = 'relu', strides = 1, padding = 'same')(encoder_input)\n",
    "encoding = layers.AveragePooling2D((2, 2))(encoding)\n",
    "encoding = layers.Conv2D(19, (3, 3), activation = 'relu', strides = 1, padding = 'same')(encoding)\n",
    "\n",
    "encoder_output = layers.Conv2D(1, (3, 3), activation = 'relu', strides = 1, padding = 'same')(encoding)\n",
    "\n",
    "encoder = tf.keras.models.Model(encoder_input, encoder_output)\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decoder part, reconstructing from latent space back to the (18, 14) region input. Note the Conv2DTranspose is not yet supported in hls4ml, but ok to use if it is going to be distilled to another network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoding = layers.Conv2D(25, (3, 3), activation = 'relu', strides = 1, padding = 'same')(encoder_output)\n",
    "decoding = layers.UpSampling2D((2, 2))(decoding)\n",
    "decoding = layers.Conv2D(25, (3, 3), activation = 'relu', strides = 1, padding = 'same')(decoding)\n",
    "\n",
    "decoder_output = layers.Conv2D(1, (3, 3), activation = 'relu', strides = 1, padding = 'same')(decoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Model(encoder_input, decoder_output)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = 'adam', loss = 'mse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dense model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fully connected dense model to be trained for input reconstruction (less useful since region correlation is lost to some degree in the Flatten layer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.InputLayer(input_shape = (18, 14, 1)))\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dense(20, activation = 'relu'))\n",
    "model.add(tf.keras.layers.Dense(20, activation = 'relu'))\n",
    "model.add(tf.keras.layers.Dense(10, activation = 'relu'))\n",
    "model.add(tf.keras.layers.Dense(20 , activation = 'relu'))\n",
    "model.add(tf.keras.layers.Dense(20 , activation = 'relu'))\n",
    "model.add(tf.keras.layers.Dense(252 , activation = 'sigmoid'))\n",
    "model.add(tf.keras.layers.Reshape((18, 14, 1)))\n",
    "model.summary()\n",
    "model.compile(optimizer = 'adam', loss = 'mse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# De-noising model (De-ZeroBias model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolutional autoencoder to be trained for ZB pattern removal. Experimental and for fun only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input = tf.keras.Input(shape = (18, 14, 1))\n",
    "encoding = layers.Conv2D(10, (3, 3), activation = 'relu', strides = 1, padding = 'same')(encoder_input)\n",
    "encoding = layers.AveragePooling2D((2, 2))(encoding)\n",
    "encoding = layers.Conv2D(10, (3, 3), activation = 'relu', strides = 1, padding = 'same')(encoding)\n",
    "\n",
    "encoder_output = layers.Conv2D(2, (3, 3), activation = 'relu', strides = 1, padding = 'same')(encoding)\n",
    "\n",
    "encoder = tf.keras.models.Model(encoder_input, encoder_output)\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoding = layers.Conv2D(10, (3, 3), activation = 'relu', strides = 1, padding = 'same')(encoder_output)\n",
    "decoding = layers.UpSampling2D((2, 2))(decoding)\n",
    "decoding = layers.Conv2D(10, (3, 3), activation = 'relu', strides = 1, padding = 'same')(decoding)\n",
    "\n",
    "decoder_output = layers.Conv2D(1, (3, 3), activation = 'sigmoid', strides = 1, padding = 'same')(decoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Model(encoder_input, decoder_output)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = 'adam', loss = 'binary_crossentropy')\n",
    "#model.compile(optimizer = 'adam', loss = 'mse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Partition the dataset into train/val/test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = ZeroBias\n",
    "\n",
    "train_ratio = 0.8\n",
    "val_ratio = 0.1\n",
    "test_ratio = 1 - train_ratio - val_ratio\n",
    "X_train_val, X_test = train_test_split(X, test_size = test_ratio, random_state = 123)\n",
    "X_train, X_val = train_test_split(X_train_val, test_size = val_ratio/(val_ratio + train_ratio), random_state = 123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!!! Only for de-noising model. Preparation of noisy and clean training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "RandomTowers = np.zeros((ZeroBias.shape[0], 18, 14, 1))\n",
    "\n",
    "random_phi1 = np.random.randint(low = 1, high = 17, size = np.int(RandomTowers.shape[0]/2))\n",
    "random_eta1 = np.random.randint(low = 1, high = 13, size = np.int(RandomTowers.shape[0]/2))\n",
    "random_pt1 = np.random.randint(low = 30, high = 100, size = np.int(RandomTowers.shape[0]/2))\n",
    "random_phi2 = np.random.randint(low = 1, high = 17, size = np.int(RandomTowers.shape[0]/2))\n",
    "random_eta2 = np.random.randint(low = 1, high = 13, size = np.int(RandomTowers.shape[0]/2))\n",
    "random_pt2 = np.random.randint(low = 30, high = 100, size = np.int(RandomTowers.shape[0]/2))\n",
    "\n",
    "for i in range(np.int(RandomTowers.shape[0]/2)):\n",
    "    RandomTowers[i, random_phi1[i], random_eta1[i], 0] = random_pt1[i]\n",
    "    RandomTowers[i, random_phi1[i]+1, random_eta1[i]+1, 0] = random_pt2[i]\n",
    "    RandomTowers[i, random_phi2[i], random_eta2[i], 0] = random_pt2[i]\n",
    "    RandomTowers[i, random_phi2[i]+1, random_eta2[i], 0] = random_pt1[i]\n",
    "\n",
    "Xnoise = 3*ZeroBias \n",
    "Xclean = ZeroBias\n",
    "\n",
    "train_ratio = 0.8\n",
    "val_ratio = 0.18\n",
    "test_ratio = 1 - train_ratio - val_ratio\n",
    "\n",
    "Xnoise_train_val, Xnoise_test = train_test_split(Xnoise, test_size = test_ratio, random_state = 123)\n",
    "Xnoise_train, Xnoise_val = train_test_split(Xnoise_train_val, test_size = val_ratio/(val_ratio + train_ratio), random_state = 123)\n",
    "\n",
    "Xclean_train_val, Xclean_test = train_test_split(Xclean, test_size = test_ratio, random_state = 123)\n",
    "Xclean_train, Xclean_val = train_test_split(Xclean_train_val, test_size = val_ratio/(val_ratio + train_ratio), random_state = 123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!!! Only for de-noising model. Plot and compare the noisy and clean training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(40,50):\n",
    "    fig, ax = plt.subplots(figsize = (10,10))\n",
    "    ax = plt.subplot(2, 2, 1)\n",
    "    ax = sns.heatmap(Xnoise_train[i,:,:,0].reshape(18, 14), vmin = 0, vmax = Xnoise_train[i,:,:,0].max(), cmap = \"Reds\", cbar_kws = {'label': 'Normalized ET'})\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    ax.set_title('X_noise')\n",
    "    \n",
    "    ax = plt.subplot(2, 2, 2)\n",
    "    ax = sns.heatmap(Xclean_train[i,:,:,0].reshape(18, 14), vmin = 0, vmax = Xnoise_train[i,:,:,0].max(), cmap = \"Reds\", cbar_kws = {'label': 'Normalized ET'})\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    ax.set_title('X_clean')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the training. Mind the label when training for reconstruction or something else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, X_train,\n",
    "                    epochs = 80,\n",
    "                    validation_data = (X_val, X_val),\n",
    "                    batch_size = 256,\n",
    "                    callbacks = [\n",
    "                        tf.keras.callbacks.EarlyStopping(monitor = \"val_loss\", patience = 5, mode = \"min\")\n",
    "                    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot loss vs epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15,10))\n",
    "axes = plt.subplot(2, 2, 1)\n",
    "axes.plot(history.history['loss'], label = 'train loss')\n",
    "#axes.set_yscale(value = \"log\")\n",
    "axes.plot(history.history['val_loss'], label = 'val loss')\n",
    "axes.legend(loc = \"upper right\")\n",
    "axes.set_xlabel('Epoch')\n",
    "axes.set_ylabel('Loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving/loading trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('saved_models/teacher/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_student.save('saved_models/student_quantized/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('saved_models/teacher')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_student = tf.keras.models.load_model('saved_models/student_quantized')\n",
    "model_student.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feed all datasets into the trained model to compute prediction outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_predict = model.predict(X_train)\n",
    "X_val_predict = model.predict(X_val)\n",
    "X_test_predict = model.predict(X_test)\n",
    "MC_zb_predict = []\n",
    "for i in range(len(MC_zb)):\n",
    "    MC_zb_predict.append(model.predict(MC_zb[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define loss function to use for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss_for_pred(y_true, y_pred, choice):\n",
    "    #Simple MSE\n",
    "    if choice == 0:\n",
    "        loss = np.mean((y_true - y_pred)**2, axis = (1, 2, 3))\n",
    "        return loss\n",
    "    \n",
    "    #Simple MSE for de-noising model\n",
    "    if choice == 1:\n",
    "        loss = np.mean(y_pred**2, axis = (1, 2, 3))\n",
    "        return loss\n",
    "    \n",
    "    #Same as custom_loss_for_train\n",
    "    if choice == 2:\n",
    "        loss_mse = np.mean((y_pred - y_true)**2, axis = (1, 2, 3))\n",
    "        #loss_reg = np.mean((y_pred - ZB_mean)**2, axis = (1, 2, 3))\n",
    "        loss_reg = np.mean(y_pred**2, axis = (1, 2, 3))\n",
    "        loss = loss_mse + 0.2 * loss_reg\n",
    "        return loss\n",
    "    \n",
    "    #Different weights in eta\n",
    "    if choice == 3:\n",
    "        loss = np.mean((y_true - y_pred)**2, axis = (1, 3))\n",
    "        scale = np.array([0.5, 1, 2, 4, 6, 8, 10, 10, 8, 6, 4, 2, 1, 0.5])\n",
    "        loss = loss * scale\n",
    "        loss = np.mean(loss, axis = 1)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute loss for all samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_choice = 0\n",
    "\n",
    "X_train_loss = custom_loss_for_pred(X_train, X_train_predict, loss_choice)\n",
    "X_val_loss = custom_loss_for_pred(X_val, X_val_predict, loss_choice)\n",
    "X_test_loss = custom_loss_for_pred(X_test, X_test_predict, loss_choice)\n",
    "\n",
    "MC_zb_loss = []\n",
    "for i in range(len(MC_zb)):\n",
    "    MC_zb_loss.append(custom_loss_for_pred(MC_zb[i], MC_zb_predict[i], loss_choice))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot loss distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbins = 40\n",
    "rmin = 0\n",
    "rmax = 100\n",
    "plt.hist(X_train_loss, density = 1, bins = nbins, alpha = 0.3, label = 'train (ZeroBias)', range = (rmin, rmax), log = True)\n",
    "plt.hist(X_test_loss, density = 1, bins = nbins, alpha = 0.3, label = 'test (ZeroBias)', range = (rmin, rmax))\n",
    "plt.hist(MC_zb_loss[0], density = 1, bins = nbins, label = 'QCD', alpha = 0.1, histtype = 'stepfilled', range = (rmin, rmax))\n",
    "plt.hist(MC_zb_loss[1], density = 1, bins = nbins, label = 'SingleNu', alpha = 0.1, histtype = 'stepfilled', range = (rmin, rmax))\n",
    "for i in range(40,50):\n",
    "    plt.hist(MC_zb_loss[i], density = 1, bins = nbins, label = MC_files[i], histtype = 'step', range = (rmin, rmax))\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "plt.xlabel(\"loss\")\n",
    "#plt.xticks(np.arange(rmin, rmax, step = 0.0002))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison between original and reconstructed inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Original vs Reconstructed\n",
    "show_ZB = True\n",
    "#show_ZB = False\n",
    "n = 19\n",
    "for i in range(580,590):\n",
    "    fig, ax = plt.subplots(figsize = (17,17))\n",
    "    if show_ZB == True:\n",
    "        print('ZB test\\nloss = ' + str(X_test_loss[i]))\n",
    "    else:\n",
    "        print(str(MC_files[n]) + '\\nloss = ' + str(MC_zb_loss[n][i]))\n",
    "    ax = plt.subplot(3, 3, 1)\n",
    "    if show_ZB == True:\n",
    "        ax = sns.heatmap(X_test[i,:,:,0].reshape(18, 14), vmin = 0, vmax = X_test[i,:,:,0].max(), cmap = \"Reds\", cbar_kws = {'label': 'Normalized ET'})\n",
    "    else:\n",
    "        ax = sns.heatmap(MC_zb[n][i,:,:,0].reshape(18, 14), vmin = 0, vmax = MC_zb[n][i,:,:,0].max(), cmap = \"Reds\", cbar_kws = {'label': 'Normalized ET'})\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    ax.set_title('Original')\n",
    "    \n",
    "    ax = plt.subplot(3, 3, 2)\n",
    "    if show_ZB == True:\n",
    "        ax = sns.heatmap(X_test_predict[i,:,:,0].reshape(18, 14), vmin = 0, vmax = X_test[i,:,:,0].max(), cmap = \"Reds\", cbar_kws = {'label': 'Normalized ET'})\n",
    "    else:\n",
    "        ax = sns.heatmap(MC_zb_predict[n][i,:,:,0].reshape(18, 14), vmin = 0, vmax = MC_zb[n][i,:,:,0].max(), cmap = \"Reds\", cbar_kws = {'label': 'Normalized ET'})\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    ax.set_title('Reconstructed')\n",
    "    \n",
    "    ax = plt.subplot(3, 3, 3)\n",
    "    if show_ZB == True:\n",
    "        ax = sns.heatmap(np.absolute(X_test_predict[i,:,:,0] - X_test[i,:,:,0]).reshape(18, 14), vmin = 0, vmax = X_test[i,:,:,0].max(), cmap = \"Reds\", cbar_kws = {'label': 'Normalized ET'})\n",
    "    else:\n",
    "        ax = sns.heatmap(np.absolute(MC_zb_predict[n][i,:,:,0] - MC_zb[n][i,:,:,0]).reshape(18, 14), vmin = 0, vmax = MC_zb[n][i,:,:,0].max(), cmap = \"Reds\", cbar_kws = {'label': 'Normalized ET'})\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    ax.set_title('abs(original-reconstructed)')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Knowledge Distillation (+ quantizing with QKeras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qkeras import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct student model without quantization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_in = layers.Input(shape = (18, 14, 1))\n",
    "x = layers.Flatten()(x_in)\n",
    "x = layers.Dense(26)(x)\n",
    "x = layers.Activation('relu')(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Dropout(rate = 0.3)(x)\n",
    "x = layers.Dense(1)(x)\n",
    "x = layers.Activation('relu')(x)\n",
    "\n",
    "model_student = tf.keras.models.Model(x_in, x)\n",
    "model_student.summary()\n",
    "model_student.compile(optimizer = 'adam', loss = 'mse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct student model with pre-defined quantization configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For quantization-aware training\n",
    "x_in = layers.Input(shape = (18, 14, 1))\n",
    "x = layers.Flatten()(x_in)\n",
    "x = QDense(26,\n",
    "           kernel_quantizer = quantized_bits(10, 5, 1),\n",
    "           bias_quantizer = quantized_bits(6, 3, 1))(x)\n",
    "x = QActivation('quantized_relu(bits=10, integer=5)')(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Dropout(rate = 0.3)(x)\n",
    "x = QDense(1,\n",
    "           kernel_quantizer = quantized_bits(10, 5, 1),\n",
    "           bias_quantizer = quantized_bits(6, 3, 1))(x)\n",
    "x = QActivation('quantized_relu(bits=10, integer=6)')(x)\n",
    "\n",
    "model_student = tf.keras.models.Model(x_in, x)\n",
    "model_student.summary()\n",
    "model_student.compile(optimizer = 'adam', loss = 'mse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the student model with knowledge distilled from a pre-trained teacher model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history_student = model_student.fit(X_train, 10.*X_train_loss,\n",
    "                                    epochs = 100,\n",
    "                                    validation_data = (X_val, 10.*X_val_loss),\n",
    "                                    batch_size = 256,\n",
    "                                    callbacks = [\n",
    "                                        #tensorboard_callback,\n",
    "                                        tf.keras.callbacks.EarlyStopping(monitor = \"val_loss\", patience = 5, mode = \"min\")\n",
    "                                    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot loss vs epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15,10))\n",
    "axes = plt.subplot(2, 2, 1)\n",
    "axes.plot(history_student.history['loss'], label = 'train loss')\n",
    "#axes.set_yscale(value = \"log\")\n",
    "axes.plot(history_student.history['val_loss'], label = 'val loss')\n",
    "axes.legend(loc = \"upper right\")\n",
    "axes.set_xlabel('Epoch')\n",
    "axes.set_ylabel('Loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feed all datasets into the trained model to compute prediction outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_predict_student = model_student.predict(X_train)\n",
    "X_test_predict_student = model_student.predict(X_test)\n",
    "MC_zb_predict_student = []\n",
    "for i in range(len(MC_zb)):\n",
    "    MC_zb_predict_student.append(model_student.predict(MC_zb[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot loss distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbins = 40\n",
    "rmin = 0\n",
    "rmax = 70\n",
    "plt.hist(X_train_predict_student, density = 1, bins = nbins, alpha = 0.3, label = 'train (ZeroBias)', range = (rmin, rmax), log = True)\n",
    "plt.hist(X_test_predict_student, density = 1, bins = nbins, alpha = 0.3, label = 'test (ZeroBias)', range = (rmin, rmax))\n",
    "plt.hist(MC_zb_predict_student[0], density = 1, bins = nbins, label = 'QCD', alpha = 0.1, histtype = 'stepfilled', range = (rmin, rmax))\n",
    "plt.hist(MC_zb_predict_student[1], density = 1, bins = nbins, label = 'SingleNu', alpha = 0.1, histtype = 'stepfilled', range = (rmin, rmax))\n",
    "for i in range(40,50):\n",
    "    plt.hist(MC_zb_predict_student[i], density = 1, bins = nbins, label = MC_files[i], histtype = 'step', range = (rmin, rmax))\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "plt.xlabel(\"loss\")\n",
    "#plt.xticks(np.arange(rmin, rmax, step = 0.0002))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Knowledge Distillation (+ quantizing with AutoQKeras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qkeras import *\n",
    "from qkeras.autoqkeras import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorBoard (less useful)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "rm -rf ./logs/\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir = \"logs/fit\", histogram_freq = 1)\n",
    "%tensorboard --logdir logs/fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROC plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assigning labels and arranging for ROC plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Baseline\n",
    "#Assuming only the mean ZB is learned\n",
    "#Take mean ZB as outputs no matter what inputs are\n",
    "#Classifier of baseline = MSE(inputs, ZeroBias_mean)\n",
    "ZeroBias_mean = np.mean(ZeroBias, axis = 0)\n",
    "\n",
    "baseline_zb = np.mean((X_test - ZeroBias_mean)**2, axis = (1, 2))\n",
    "baseline_mc = []\n",
    "for i in range(len(MC_zb)):\n",
    "    baseline_mc.append(np.mean((MC_zb[i] - ZeroBias_mean)**2, axis = (1, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assign labels for various signals (y = 1) and backgrounds (y = 0)\n",
    "Y_zb = np.zeros((X_test.shape[0], 1))\n",
    "Y_mc = []\n",
    "for i in range(len(MC)):\n",
    "    Y_mc.append(np.ones((MC_zb[i].shape[0], 1)))\n",
    "\n",
    "#Concatenate datasets to make ROC curves, i.e. QCD/SingleNu/signals vs ZB\n",
    "\n",
    "#True labels\n",
    "Y_true = []\n",
    "#Baseline scores\n",
    "Y_baseline = []\n",
    "#Model scores\n",
    "Y_model = []\n",
    "#Student model scores\n",
    "Y_student = []\n",
    "for i in range(len(MC)):\n",
    "    Y_true.append(np.concatenate((Y_mc[i], Y_zb)))\n",
    "    Y_baseline.append(np.concatenate((baseline_mc[i], baseline_zb)))\n",
    "    Y_model.append(np.concatenate((MC_zb_loss[i], X_test_loss)))\n",
    "    Y_student.append(np.concatenate((MC_zb_predict_student[i], X_test_predict_student)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (13, 13))\n",
    "axes = plt.subplot(2, 2, 1)\n",
    "fpr_baseline = []\n",
    "tpr_baseline = []\n",
    "thresholds_baseline = []\n",
    "roc_auc_baseline = []\n",
    "for i in range(len(MC)):\n",
    "    fpr_baseline.append(np.empty((Y_true[i].shape[0],1)))\n",
    "    tpr_baseline.append(np.empty((Y_true[i].shape[0],1)))\n",
    "    thresholds_baseline.append(np.empty((Y_true[i].shape[0],1)))\n",
    "    roc_auc_baseline.append(np.empty((Y_true[i].shape[0],1)))\n",
    "    fpr_baseline[i], tpr_baseline[i], thresholds_baseline[i] = roc_curve(Y_true[i], Y_baseline[i])\n",
    "    roc_auc_baseline[i] = auc(fpr_baseline[i], tpr_baseline[i])\n",
    "    if i == 0:\n",
    "        axes.plot(fpr_baseline[i], tpr_baseline[i], linestyle = '--', color = 'r', lw = 2, label = MC_files[i] + ' (AUC = %.8f)' % (roc_auc_baseline[i]))\n",
    "    elif i == 1 or i == 2:\n",
    "        axes.plot(fpr_baseline[i], tpr_baseline[i], linestyle = ':', lw = 2, label = MC_files[i] + ' (AUC = %.8f)' % (roc_auc_baseline[i]))\n",
    "    else:\n",
    "        axes.plot(fpr_baseline[i], tpr_baseline[i], linestyle = '-', lw = 2, label = MC_files[i] + ' (AUC = %.8f)' % (roc_auc_baseline[i]))\n",
    "#axes.plot([0, 1], [0, 1], linestyle = '--', lw = 2, color = 'black', label = 'random chance')\n",
    "axes.plot([0.002, 0.002], [0, 1], linestyle = '--', lw = 2, color = 'black', label = 'FPR = 0.2% ~ (100 kHz)/(ZB rate)')\n",
    "axes.set_xlim([0.0001, 1.0])\n",
    "#axes.set_xlim([0, 1.0])\n",
    "axes.set_ylim([0, 1.0])\n",
    "#axes.set_ylim([0.9, 1.0])\n",
    "axes.set_xscale(value = \"log\")\n",
    "#axes.set_yscale(value = \"log\")\n",
    "axes.set_xlabel('False Positive Rate (FPR)')\n",
    "axes.set_ylabel('True Positive Rate (TPR)')\n",
    "axes.set_title('Baseline ROC')\n",
    "axes.legend(loc='center left', bbox_to_anchor = (1.15, 0.5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teacher model ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (13, 13))\n",
    "axes = plt.subplot(2, 2, 1)\n",
    "fpr_model = []\n",
    "tpr_model = []\n",
    "thresholds_model = []\n",
    "roc_auc_model = []\n",
    "for i in range(len(MC)):\n",
    "    fpr_model.append(np.empty((Y_true[i].shape[0],1)))\n",
    "    tpr_model.append(np.empty((Y_true[i].shape[0],1)))\n",
    "    thresholds_model.append(np.empty((Y_true[i].shape[0],1)))\n",
    "    roc_auc_model.append(np.empty((Y_true[i].shape[0],1)))\n",
    "    fpr_model[i], tpr_model[i], thresholds_model[i] = roc_curve(Y_true[i], Y_model[i])\n",
    "    roc_auc_model[i] = auc(fpr_model[i], tpr_model[i])\n",
    "    if i == 0:\n",
    "        axes.plot(fpr_model[i], tpr_model[i], linestyle = '--', color = 'r', lw = 2, label = MC_files[i] + ' (AUC = %.8f)' % (roc_auc_model[i]))\n",
    "    elif i == 1 or i == 2:\n",
    "        axes.plot(fpr_model[i], tpr_model[i], linestyle = ':', lw = 2, label = MC_files[i] + ' (AUC = %.8f)' % (roc_auc_model[i]))\n",
    "    else:\n",
    "        axes.plot(fpr_model[i], tpr_model[i], linestyle = '-', lw = 2, label = MC_files[i] + ' (AUC = %.8f)' % (roc_auc_model[i]))\n",
    "#axes.plot([0, 1], [0, 1], linestyle = '--', lw = 2, color = 'black', label = 'random chance')\n",
    "axes.plot([0.002, 0.002], [0, 1], linestyle = '--', lw = 2, color = 'black', label = 'FPR = 0.2% ~ (100 kHz)/(ZB rate)')\n",
    "axes.set_xlim([0.0001, 1.0])\n",
    "#axes.set_xlim([0, 1.0])\n",
    "axes.set_ylim([0, 1.0])\n",
    "#axes.set_ylim([0.9, 1.0])\n",
    "axes.set_xscale(value = \"log\")\n",
    "#axes.set_yscale(value = \"log\")\n",
    "axes.set_xlabel('False Positive Rate (FPR)')\n",
    "axes.set_ylabel('True Positive Rate (TPR)')\n",
    "axes.set_title('Model ROC')\n",
    "axes.legend(loc='center left', bbox_to_anchor = (1.15, 0.5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Student model ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (13, 13))\n",
    "axes = plt.subplot(2, 2, 1)\n",
    "fpr_student = []\n",
    "tpr_student = []\n",
    "thresholds_student = []\n",
    "roc_auc_student = []\n",
    "for i in range(len(MC)):\n",
    "    fpr_student.append(np.empty((Y_true[i].shape[0],1)))\n",
    "    tpr_student.append(np.empty((Y_true[i].shape[0],1)))\n",
    "    thresholds_student.append(np.empty((Y_true[i].shape[0],1)))\n",
    "    roc_auc_student.append(np.empty((Y_true[i].shape[0],1)))\n",
    "    fpr_student[i], tpr_student[i], thresholds_student[i] = roc_curve(Y_true[i], Y_student[i])\n",
    "    roc_auc_student[i] = auc(fpr_student[i], tpr_student[i])\n",
    "    if i == 0:\n",
    "        axes.plot(fpr_student[i], tpr_student[i], linestyle = '--', color = 'r', lw = 2, label = MC_files[i] + ' (AUC = %.8f)' % (roc_auc_student[i]))\n",
    "    elif i == 1 or i == 2:\n",
    "        axes.plot(fpr_student[i], tpr_student[i], linestyle = ':', lw = 2, label = MC_files[i] + ' (AUC = %.8f)' % (roc_auc_student[i]))\n",
    "    else:\n",
    "        axes.plot(fpr_student[i], tpr_student[i], linestyle = '-', lw = 2, label = MC_files[i] + ' (AUC = %.8f)' % (roc_auc_student[i]))\n",
    "#axes.plot([0, 1], [0, 1], linestyle = '--', lw = 2, color = 'black', label = 'random chance')\n",
    "axes.plot([0.002, 0.002], [0, 1], linestyle = '--', lw = 2, color = 'black', label = 'FPR = 0.2% ~ (100 kHz)/(ZB rate)')\n",
    "axes.set_xlim([0.0001, 1.0])\n",
    "#axes.set_xlim([0, 1.0])\n",
    "axes.set_ylim([0, 1.0])\n",
    "#axes.set_ylim([0.1, 1.0])\n",
    "axes.set_xscale(value = \"log\")\n",
    "#axes.set_yscale(value = \"log\")\n",
    "axes.set_xlabel('False Positive Rate (FPR)')\n",
    "axes.set_ylabel('True Positive Rate (TPR)')\n",
    "axes.set_title('Student model ROC')\n",
    "axes.legend(loc='center left', bbox_to_anchor = (1.15, 0.5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tabulating TPR at fixed FPR = 0.2% (baseline, model, change)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_tpr_baseline = []\n",
    "table_tpr_model = []\n",
    "table_tpr_student = []\n",
    "table_tpr_change = []\n",
    "for i in range(len(fpr_baseline)):\n",
    "    for j in range(len(fpr_baseline[i])):\n",
    "        if fpr_baseline[i][j] > 0.002:\n",
    "            table_tpr_baseline.append(tpr_baseline[i][j] * 100)\n",
    "            break\n",
    "    for j in range(len(fpr_model[i])):\n",
    "        if fpr_model[i][j] > 0.002:\n",
    "            table_tpr_model.append(tpr_model[i][j] * 100)\n",
    "            break\n",
    "    for j in range(len(fpr_student[i])):\n",
    "        if fpr_student[i][j] > 0.002:\n",
    "            table_tpr_student.append(tpr_student[i][j] * 100)\n",
    "            break\n",
    "\n",
    "for i in range(len(MC)):\n",
    "    #table_tpr_change.append(100 * (table_tpr_model[i] - table_tpr_baseline[i])/table_tpr_baseline[i])\n",
    "    table_tpr_change.append(table_tpr_student[i] - table_tpr_model[i])\n",
    "\n",
    "table_tpr = pd.DataFrame({'Baseline TPR@FPR=0.2%': table_tpr_baseline,\n",
    "                          'CNN AE TPR@FPR=0.2%': table_tpr_model,\n",
    "                          'Distilled TPR@FPR=0.2%': table_tpr_student,\n",
    "                          'delta(Distilled, CNN AE)': table_tpr_change},\n",
    "                        index = MC_files)\n",
    "table_tpr = table_tpr.sort_values(by = 'CNN AE TPR@FPR=0.2%', ascending = False)\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "table_tpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
