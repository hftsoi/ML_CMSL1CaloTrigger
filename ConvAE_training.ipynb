{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Autoencoder Training for Anomaly Detection @ L1Trigger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pydot\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "import keras_tuner\n",
    "from keras_tuner import Hyperband\n",
    "\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read input files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All input files are already sorted in Calo regions (i, j) ~ (18, 14)<br>\n",
    "Where i = 0 -> 17 corresponds to GCT_Phi = 0 -> 17<br>\n",
    "Where j = 0 -> 13 corresponds to RCT_Eta = 4 -> 17\n",
    "\n",
    "Keep this ordering as is when feeding into neural nets. Also keep this in mind when generating/preparing new samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backgrounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ZeroBias = np.concatenate((h5py.File('bkg/ZeroBias_0.h5', 'r')['CaloRegions'][()],\n",
    "                           h5py.File('bkg/ZeroBias_1.h5', 'r')['CaloRegions'][()],\n",
    "                           h5py.File('bkg/ZeroBias_2.h5', 'r')['CaloRegions'][()]))\n",
    "ZeroBias = ZeroBias.astype(dtype = 'float32').reshape(-1, 18, 14, 1)\n",
    "print(ZeroBias.shape)\n",
    "\n",
    "MC_files = []\n",
    "MC_files.append('bkg/110X/QCD_0.h5')#i=0\n",
    "#MC_files.append('bkg/110X/QCD_1.h5')\n",
    "#MC_files.append('bkg/110X/QCD_2.h5')\n",
    "MC_files.append('bkg/120X/SingleNeutrino_E-10_0.h5')#i=1\n",
    "#MC_files.append('bkg/120X/SingleNeutrino_E-10_1.h5')\n",
    "#MC_files.append('bkg/120X/SingleNeutrino_E-10_2.h5')\n",
    "MC_files.append('bkg/120X/SingleNeutrino_Pt-2To20_0.h5')#i=2\n",
    "#MC_files.append('bkg/120X/SingleNeutrino_Pt-2To20_1.h5')\n",
    "#MC_files.append('bkg/120X/SingleNeutrino_Pt-2To20_2.h5')\n",
    "\n",
    "MC_files.append('sig/110X/GluGluToHHTo4B_node_SM_TuneCP5_14TeV.h5')#i=3\n",
    "MC_files.append('sig/110X/HTo2LongLivedTo4mu_MH-1000_MFF-450_CTau-10000mm_TuneCP5_14TeV.h5')\n",
    "MC_files.append('sig/110X/HTo2LongLivedTo4mu_MH-125_MFF-12_CTau-900mm_TuneCP5_14TeV.h5')\n",
    "MC_files.append('sig/110X/HTo2LongLivedTo4mu_MH-125_MFF-25_CTau-1500mm_TuneCP5_14TeV.h5')\n",
    "MC_files.append('sig/110X/HTo2LongLivedTo4mu_MH-125_MFF-50_CTau-3000mm_TuneCP5_14TeV.h5')\n",
    "MC_files.append('sig/110X/VBFHToTauTau_M125_TuneCUETP8M1_14TeV.h5')\n",
    "MC_files.append('sig/110X/VBF_HH_CV_1_C2V_1_C3_1_TuneCP5_PSweights_14TeV.h5')\n",
    "MC_files.append('sig/110X/VBF_HToInvisible_M125_TuneCUETP8M1_14TeV.h5')\n",
    "MC_files.append('sig/110X/VectorZPrimeToQQ_M100_pT300_TuneCP5_14TeV.h5')\n",
    "MC_files.append('sig/110X/VectorZPrimeToQQ_M200_pT300_TuneCP5_14TeV.h5')\n",
    "MC_files.append('sig/110X/VectorZPrimeToQQ_M50_pT300_TuneCP5_14TeV.h5')#i=13\n",
    "MC_files.append('sig/110X/ZprimeToZH_MZprime1000_MZ50_MH80_ZTouds_HTouds_narrow_TuneCP5_14TeV.h5')\n",
    "MC_files.append('sig/110X/ZprimeToZH_MZprime600_MZ50_MH80_ZTouds_HTouds_narrow_TuneCP5_14TeV.h5')\n",
    "MC_files.append('sig/110X/ZprimeToZH_MZprime800_MZ50_MH80_ZTouds_HTouds_narrow_TuneCP5_14TeV.h5')\n",
    "MC_files.append('sig/120X/GluGluHToTauTau_M-125_TuneCP5_14TeV.h5')\n",
    "MC_files.append('sig/120X/GluGluToHHTo4B_node_cHHH1_TuneCP5_14TeV.h5')\n",
    "MC_files.append('sig/120X/GluGluToHHTo4B_node_cHHH5_TuneCP5_14TeV.h5')\n",
    "MC_files.append('sig/120X/HTo2LongLivedTo4b_MH-1000_MFF-450_CTau-100000mm_TuneCP5_14TeV.h5')\n",
    "MC_files.append('sig/120X/HTo2LongLivedTo4b_MH-1000_MFF-450_CTau-10000mm_TuneCP5_14TeV.h5')\n",
    "MC_files.append('sig/120X/HTo2LongLivedTo4b_MH-125_MFF-12_CTau-9000mm_TuneCP5_14TeV.h5')\n",
    "MC_files.append('sig/120X/HTo2LongLivedTo4b_MH-125_MFF-12_CTau-900mm_TuneCP5_14TeV.h5')#i=23\n",
    "MC_files.append('sig/120X/HTo2LongLivedTo4b_MH-125_MFF-25_CTau-15000mm_TuneCP5_14TeV.h5')\n",
    "MC_files.append('sig/120X/HTo2LongLivedTo4b_MH-125_MFF-25_CTau-1500mm_TuneCP5_14TeV.h5')\n",
    "MC_files.append('sig/120X/HTo2LongLivedTo4b_MH-125_MFF-50_CTau-30000mm_TuneCP5_14TeV.h5')\n",
    "MC_files.append('sig/120X/HTo2LongLivedTo4b_MH-125_MFF-50_CTau-3000mm_TuneCP5_14TeV.h5')\n",
    "MC_files.append('sig/120X/HTo2LongLivedTo4b_MH-250_MFF-120_CTau-10000mm_TuneCP5_14TeV.h5')\n",
    "MC_files.append('sig/120X/HTo2LongLivedTo4b_MH-250_MFF-120_CTau-1000mm_TuneCP5_14TeV.h5')\n",
    "MC_files.append('sig/120X/HTo2LongLivedTo4b_MH-250_MFF-60_CTau-1000mm_TuneCP5_14TeV.h5')\n",
    "MC_files.append('sig/120X/HTo2LongLivedTo4b_MH-350_MFF-160_CTau-10000mm_TuneCP5_14TeV.h5')\n",
    "MC_files.append('sig/120X/HTo2LongLivedTo4b_MH-350_MFF-160_CTau-1000mm_TuneCP5_14TeV.h5')\n",
    "MC_files.append('sig/120X/HTo2LongLivedTo4b_MH-350_MFF-160_CTau-500mm_TuneCP5_14TeV.h5')#i=33\n",
    "MC_files.append('sig/120X/HTo2LongLivedTo4b_MH-350_MFF-80_CTau-10000mm_TuneCP5_14TeV.h5')\n",
    "MC_files.append('sig/120X/HTo2LongLivedTo4b_MH-350_MFF-80_CTau-1000mm_TuneCP5_14TeV.h5')\n",
    "MC_files.append('sig/120X/HTo2LongLivedTo4b_MH-350_MFF-80_CTau-500mm_TuneCP5_14TeV.h5')\n",
    "MC_files.append('sig/120X/HTo2LongLivedTo4mu_MH-1000_MFF-450_CTau-10000mm_TuneCP5_14TeV.h5')\n",
    "MC_files.append('sig/120X/HTo2LongLivedTo4mu_MH-125_MFF-12_CTau-900mm_TuneCP5_14TeV.h5')\n",
    "MC_files.append('sig/120X/HTo2LongLivedTo4mu_MH-125_MFF-25_CTau-1500mm_TuneCP5_14TeV.h5')\n",
    "MC_files.append('sig/120X/HTo2LongLivedTo4mu_MH-125_MFF-50_CTau-3000mm_TuneCP5_14TeV.h5')\n",
    "MC_files.append('sig/120X/SUSYGluGluToBBHToBB_NarrowWidth_M-1200_TuneCP5_13TeV-pythia814TeV.h5')\n",
    "MC_files.append('sig/120X/SUSYGluGluToBBHToBB_NarrowWidth_M-120_TuneCP5_14TeV.h5')\n",
    "MC_files.append('sig/120X/SUSYGluGluToBBHToBB_NarrowWidth_M-350_TuneCP5_14TeV.h5')#i=43\n",
    "MC_files.append('sig/120X/SUSYGluGluToBBHToBB_NarrowWidth_M-600_TuneCP5_14TeV.h5')\n",
    "MC_files.append('sig/120X/TprimeBToTH_M-650_LH_TuneCP5_14TeV.h5')\n",
    "MC_files.append('sig/120X/VBFHHTo4B_CV_1_C2V_2_C3_1_TuneCP5_14TeV.h5')\n",
    "MC_files.append('sig/120X/VBFHToInvisible_M125_TuneCP5_14TeV.h5')\n",
    "MC_files.append('sig/120X/VBFHToTauTau_M125_TuneCP5_14TeV.h5')\n",
    "MC_files.append('sig/120X/VectorZPrimeGammaToQQGamma_M-10_GPt-75_TuneCP5_14TeV.h5')\n",
    "MC_files.append('sig/120X/VectorZPrimeToQQ_M-100_Pt-300_TuneCP5_14TeV.h5')\n",
    "MC_files.append('sig/120X/VectorZPrimeToQQ_M-200_Pt-300_TuneCP5_14TeV.h5')#i=51\n",
    "\n",
    "MC = []\n",
    "AcceptanceFlag = []\n",
    "for i in range(len(MC_files)):\n",
    "    MC.append(h5py.File(MC_files[i], 'r')['CaloRegions'][()].astype(dtype = 'float32'))\n",
    "    MC[i] = MC[i].reshape(-1, 18, 14, 1)\n",
    "    \n",
    "    if i > 2 and i < 52:\n",
    "        AcceptanceFlag.append(h5py.File(MC_files[i], 'r')['AcceptanceFlag'][()])\n",
    "    \n",
    "MC[0] = MC[0][:100000,:,:,:]#QCD\n",
    "MC[1] = MC[1][:100000,:,:,:]#SingleNu_E10\n",
    "MC[2] = MC[2][:100000,:,:,:]#SingleNu_Pt2To20\n",
    "\n",
    "#/nfs_scratch/dasu/2022-02-04/L1TSignalZerobiasMixer/cms-vbfh.csv\n",
    "#with generator level eta cut on the jets\n",
    "vbf = pd.read_csv('cms-vbfh.csv')\n",
    "vbf.columns = ['eta','phi','et','pos','ebit','tbit']\n",
    "vbf = vbf[251:]\n",
    "\n",
    "event_col = []\n",
    "for i in range(round(vbf.shape[0]/252)):\n",
    "    for j in range(252):\n",
    "        event_col.append(i)\n",
    "        \n",
    "vbf['event'] = event_col\n",
    "vbf = vbf.drop(['pos','ebit','tbit'],axis=1)\n",
    "vbf = vbf.sort_values(by=['event', 'phi', 'eta'], ascending = [True, True, True])\n",
    "vbf = vbf.reindex(columns=['event','phi','eta','et'])\n",
    "vbf = vbf.drop(['event'],axis=1)\n",
    "vbf = vbf.to_numpy()\n",
    "vbf = vbf.reshape((-1,18,14,3))\n",
    "vbf = vbf[:,:,:,2]\n",
    "vbf = vbf.reshape((-1,18,14,1))\n",
    "vbf.shape\n",
    "\n",
    "MC_files.append('/nfs_scratch/dasu/2022-02-04/L1TSignalZerobiasMixer/cms-vbfh.csv')\n",
    "MC.append(vbf)\n",
    "print(MC[52].shape)\n",
    "'''\n",
    "#Apply acceptance cuts on MC signals\n",
    "acceptance_filter = []\n",
    "for i in range(len(AcceptanceFlag)):\n",
    "    acceptance_filter.append([])\n",
    "    for j in range(MC[i+3].shape[0]):\n",
    "        if AcceptanceFlag[i][j] == 1:\n",
    "            acceptance_filter[i].append(True)\n",
    "        else:\n",
    "            acceptance_filter[i].append(False)\n",
    "            \n",
    "for i in range(len(MC_files)):\n",
    "    if i > 2 and i < 52:\n",
    "        MC[i] = MC[i][acceptance_filter[i-3]]\n",
    "    print(str(i) + ': ' + str(MC[i].shape))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_mc_acceptance = []\n",
    "baseline_mc_acceptance = []\n",
    "MC_zb_loss_acceptance = []\n",
    "for i in range(len(MC)):\n",
    "    if i > 2 and i < 52:\n",
    "        Y_mc_acceptance.append(Y_mc[i][sig_filter[i-3],:])\n",
    "        baseline_mc_acceptance.append(baseline_mc[i][sig_filter[i-3],:])\n",
    "        MC_zb_loss_acceptance.append(MC_zb_loss[i][sig_filter[i-3]])\n",
    "    else:\n",
    "        Y_mc_acceptance.append(Y_mc[i])\n",
    "        baseline_mc_acceptance.append(baseline_mc[i])\n",
    "        MC_zb_loss_acceptance.append(MC_zb_loss[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create overlaid samples (ZB + MC) for test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Overlay MC events on top of ZB before testing\n",
    "#Simple addition tower by tower: overlaid MC = ZB + MC\n",
    "#ZB is chosen at random for each MC event before overlaying them\n",
    "np.random.seed(0)\n",
    "MC_zb = []\n",
    "for i in range(len(MC)):\n",
    "    MC_zb.append(np.empty((MC[i].shape[0], 18, 14, 1)))\n",
    "    ZB_random_event = np.random.randint(low = 0, high = ZeroBias.shape[0], size = MC[i].shape[0])\n",
    "    for j in range(MC[i].shape[0]):\n",
    "        MC_zb[i][j, :, :, 0] = ZeroBias[ZB_random_event[j], :, :, 0] + MC[i][j, :, :, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot and compare signals/QCD/SingleNu before and after the overlay\n",
    "#n = 0 (QCD), 1 (SingleNu_E10), 2 (SingleNu_Pt2To20), 3... (signals), 52 (VBFH)\n",
    "n = 52\n",
    "for i in range(40,50):\n",
    "    fig, ax = plt.subplots(figsize = (10,10))\n",
    "    print(str(MC_files[n]))\n",
    "    ax = plt.subplot(2, 2, 1)\n",
    "    ax = sns.heatmap(MC[n][i,:,:,0].reshape(18, 14), vmin = 0, vmax = MC[n][i,:,:,0].max(), cmap = \"Reds\", cbar_kws = {'label': 'ET (GeV)'})\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    ax.set_title('MC')\n",
    "    \n",
    "    ax = plt.subplot(2, 2, 2)\n",
    "    ax = sns.heatmap(MC_zb[n][i,:,:,0].reshape(18, 14), vmin = 0, vmax = MC[n][i,:,:,0].max(), cmap = \"Reds\", cbar_kws = {'label': 'ET (GeV)'})\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    ax.set_title('MC+ZB')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistics of the inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Take mean over phi to get a profile in eta\n",
    "ZeroBias_eta = np.mean(ZeroBias, axis = (0, 1, 3))\n",
    "MC_zb_eta = []\n",
    "for i in range(len(MC)):\n",
    "    MC_zb_eta.append(np.mean(MC_zb[i], axis = (0, 1, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalize the above mean Et\n",
    "from sklearn import preprocessing\n",
    "ZeroBias_eta_norm = preprocessing.normalize([ZeroBias_eta])\n",
    "ZeroBias_eta_norm = ZeroBias_eta_norm.reshape((14,))\n",
    "\n",
    "MC_zb_eta_norm = []\n",
    "for i in range(len(MC)):\n",
    "    MC_zb_eta_norm.append(preprocessing.normalize([MC_zb_eta[i]]))\n",
    "    MC_zb_eta_norm[i] = MC_zb_eta_norm[i].reshape((14,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = []\n",
    "for i in range(4, 18):\n",
    "    x.append(i)\n",
    "\n",
    "plt.plot(x, ZeroBias_eta, 'bo', label = 'ZeroBias')\n",
    "#plt.plot(x, MC_zb_eta[1], 'go', label = 'SingleNeutrino_E10')\n",
    "plt.plot(x, MC_zb_eta[0], 'ro', label = 'QCD')\n",
    "for i in range(4,15):\n",
    "    plt.plot(x, MC_zb_eta[i], label = MC_files[i])\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "plt.xlabel(\"RCT Eta\")\n",
    "plt.ylabel(\"Et averaged over phi (GeV)\")\n",
    "plt.xticks(np.arange(4, 18, step = 1))\n",
    "plt.show()\n",
    "\n",
    "plt.plot(x, ZeroBias_eta_norm, 'bo', label = 'ZeroBias')\n",
    "#plt.plot(x, MC_zb_eta_norm[1], 'go', label = 'SingleNeutrino_E10')\n",
    "plt.plot(x, MC_zb_eta_norm[0], 'ro', label = 'QCD')\n",
    "for i in range(4,15):\n",
    "    plt.plot(x, MC_zb_eta_norm[i], label = MC_files[i])\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "plt.xlabel(\"RCT Eta\")\n",
    "plt.ylabel(\"Et averaged over phi (GeV)\")\n",
    "plt.xticks(np.arange(4, 18, step = 1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Partition the whole training set into train/val/test sets\n",
    "X = ZeroBias\n",
    "\n",
    "train_ratio = 0.5\n",
    "val_ratio = 0.15\n",
    "test_ratio = 1 - train_ratio - val_ratio\n",
    "X_train_val, X_test = train_test_split(X, test_size = test_ratio, random_state = 123)\n",
    "X_train, X_val = train_test_split(X_train_val, test_size = val_ratio/(val_ratio + train_ratio), random_state = 123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ZB_mean = np.mean(ZeroBias, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (10,10))\n",
    "ax = plt.subplot(2, 2, 1)\n",
    "ax = sns.heatmap(ZB_mean.reshape(18, 14), vmin = 0, vmax = ZB_mean.max(), cmap = \"Reds\", cbar_kws = {'label': 'ET (GeV)'})\n",
    "ax.get_xaxis().set_visible(False)\n",
    "ax.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(ZeroBias.reshape((-1)), bins = 20, log = True)\n",
    "plt.xlabel(\"ZeroBias Et (GeV)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_filter = []\n",
    "for i in range(ZeroBias.shape[0]):\n",
    "    if ZeroBias[i,:,:,0].max() < 30:\n",
    "        pt_filter.append(True)\n",
    "    else:\n",
    "        pt_filter.append(False)\n",
    "ZeroBias_ptcut = ZeroBias[pt_filter,:,:,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimal hyperparameters searching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "def custom_loss_for_train():\n",
    "    def func(y_true, y_pred):\n",
    "        loss_mse = K.mean((y_pred - y_true)**2, axis = [1, 2, 3])\n",
    "        #loss_reg = K.mean((y_pred - ZB_mean)**2, axis = [1, 2, 3])\n",
    "        loss_reg = K.mean(y_pred**2, axis = [1, 2, 3])\n",
    "        loss = loss_mse + 0.1 * loss_reg\n",
    "        return loss\n",
    "    return func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define hypermodel for the search\n",
    "def hypermodel(hp):\n",
    "    hp_model = tf.keras.Sequential()\n",
    "    hp_model.add(tf.keras.layers.InputLayer(input_shape = (18, 14, 1)))\n",
    "    hp_model.add(layers.Conv2D(filters = hp.Int('filters_1',\n",
    "                                                min_value = 15,\n",
    "                                                max_value = 25,\n",
    "                                                step = 2),\n",
    "                               kernel_size = (3, 3),\n",
    "                               activation = 'relu',\n",
    "                               strides = 1,\n",
    "                               padding = 'same'))\n",
    "    \n",
    "    hp_model.add(layers.AveragePooling2D((2, 2)))\n",
    "    hp_model.add(layers.Conv2D(filters = hp.Int('filters_2',\n",
    "                                                min_value = 15,\n",
    "                                                max_value = 25,\n",
    "                                                step = 2),\n",
    "                               kernel_size = (3, 3),\n",
    "                               activation = 'relu',\n",
    "                               strides = 1,\n",
    "                               padding = 'same'))\n",
    "    \n",
    "    hp_model.add(layers.Conv2D(filters = 1,\n",
    "                               kernel_size = (3, 3),\n",
    "                               activation = 'relu',\n",
    "                               strides = 1,\n",
    "                               padding = 'same'))\n",
    "    \n",
    "    hp_model.add(layers.Conv2DTranspose(filters = hp.Int('filters_3',\n",
    "                                                min_value = 15,\n",
    "                                                max_value = 25,\n",
    "                                                step = 2),\n",
    "                               kernel_size = (3, 3),\n",
    "                               activation = 'relu',\n",
    "                               strides = 1,\n",
    "                               padding = 'same'))\n",
    "    \n",
    "    hp_model.add(layers.UpSampling2D((2, 2)))\n",
    "    hp_model.add(layers.Conv2DTranspose(filters = hp.Int('filters_4',\n",
    "                                                min_value = 15,\n",
    "                                                max_value = 25,\n",
    "                                                step = 2),\n",
    "                               kernel_size = (3, 3),\n",
    "                               activation = 'relu',\n",
    "                               strides = 1,\n",
    "                               padding = 'same'))\n",
    "    \n",
    "    hp_model.add(layers.Conv2DTranspose(filters = 1, kernel_size = (3, 3), activation = 'relu', strides = 1, padding = 'same'))\n",
    "    hp_model.compile(optimizer = 'adam', loss = custom_loss_for_train())\n",
    "    return hp_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define tuner model\n",
    "tuner = Hyperband(hypermodel,\n",
    "                 objective = 'val_loss',\n",
    "                 max_epochs = 25,\n",
    "                 factor = 3, #number of models to train in a bracket = 1+log_factor(max_epochs)\n",
    "                 hyperband_iterations = 1, #number of times to iterate over the full Hyperband algorithm\n",
    "                 seed = 10,\n",
    "                 directory = 'hypertuning',\n",
    "                 project_name = 'tune',\n",
    "                 overwrite = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run the search\n",
    "tuner.search(X_train, X_train,\n",
    "            epochs = 25,\n",
    "            validation_data = (X_val, X_val),\n",
    "            batch_size = 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show best sets of hyperparameters\n",
    "tuner.results_summary(num_trials = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hp = tuner.get_best_hyperparameters()[0]\n",
    "model = tuner.hypermodel.build(best_hp)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convolutional AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input = tf.keras.Input(shape = (18, 14, 1))\n",
    "encoding = layers.Conv2D(21, (3, 3), activation = 'relu', strides = 1, padding = 'same')(encoder_input)\n",
    "encoding = layers.AveragePooling2D((2, 2))(encoding)\n",
    "encoding = layers.Conv2D(19, (3, 3), activation = 'relu', strides = 1, padding = 'same')(encoding)\n",
    "\n",
    "encoder_output = layers.Conv2D(1, (3, 3), activation = 'relu', strides = 1, padding = 'same')(encoding)\n",
    "\n",
    "encoder = tf.keras.models.Model(encoder_input, encoder_output)\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#decoding = layers.Conv2DTranspose(3, (3, 3), activation = 'relu', strides = 1, padding = 'same')(encoder_output)\n",
    "#decoding = layers.UpSampling2D((2, 2))(decoding)\n",
    "#decoding = layers.Conv2DTranspose(5, (3, 3), activation = 'relu', strides = 1, padding = 'same')(decoding)\n",
    "\n",
    "decoding = layers.Conv2D(25, (3, 3), activation = 'relu', strides = 1, padding = 'same')(encoder_output)\n",
    "decoding = layers.UpSampling2D((2, 2))(decoding)\n",
    "decoding = layers.Conv2D(25, (3, 3), activation = 'relu', strides = 1, padding = 'same')(decoding)\n",
    "\n",
    "decoder_output = layers.Conv2D(1, (3, 3), activation = 'relu', strides = 1, padding = 'same')(decoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Model(encoder_input, decoder_output)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = 'adam', loss = 'mse')\n",
    "#model.compile(optimizer = 'adam', loss = custom_loss_for_train())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dense model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.InputLayer(input_shape = (18, 14, 1)))\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dense(20, activation = 'relu'))\n",
    "model.add(tf.keras.layers.Dense(20, activation = 'relu'))\n",
    "model.add(tf.keras.layers.Dense(10, activation = 'relu'))\n",
    "model.add(tf.keras.layers.Dense(20 , activation = 'relu'))\n",
    "model.add(tf.keras.layers.Dense(20 , activation = 'relu'))\n",
    "model.add(tf.keras.layers.Dense(252 , activation = 'relu'))\n",
    "model.add(tf.keras.layers.Reshape((18, 14, 1)))\n",
    "model.summary()\n",
    "model.compile(optimizer = 'adam', loss = 'mse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### De-noising AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input = tf.keras.Input(shape = (18, 14, 1))\n",
    "encoding = layers.Conv2D(10, (3, 3), activation = 'relu', strides = 1, padding = 'same')(encoder_input)\n",
    "encoding = layers.AveragePooling2D((2, 2))(encoding)\n",
    "encoding = layers.Conv2D(10, (3, 3), activation = 'relu', strides = 1, padding = 'same')(encoding)\n",
    "\n",
    "encoder_output = layers.Conv2D(2, (3, 3), activation = 'relu', strides = 1, padding = 'same')(encoding)\n",
    "\n",
    "encoder = tf.keras.models.Model(encoder_input, encoder_output)\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoding = layers.Conv2D(10, (3, 3), activation = 'relu', strides = 1, padding = 'same')(encoder_output)\n",
    "decoding = layers.UpSampling2D((2, 2))(decoding)\n",
    "decoding = layers.Conv2D(10, (3, 3), activation = 'relu', strides = 1, padding = 'same')(decoding)\n",
    "\n",
    "decoder_output = layers.Conv2D(1, (3, 3), activation = 'relu', strides = 1, padding = 'same')(decoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Model(encoder_input, decoder_output)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = 'adam', loss = 'binary_crossentropy')\n",
    "#model.compile(optimizer = 'adam', loss = 'mse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Re-partition the whole training set into train/val/test sets\n",
    "X = ZeroBias\n",
    "\n",
    "train_ratio = 0.5\n",
    "val_ratio = 0.2\n",
    "test_ratio = 1 - train_ratio - val_ratio\n",
    "X_train_val, X_test = train_test_split(X, test_size = test_ratio, random_state = 123)\n",
    "X_train, X_val = train_test_split(X_train_val, test_size = val_ratio/(val_ratio + train_ratio), random_state = 123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training data preparation for de-noising model\n",
    "np.random.seed(0)\n",
    "\n",
    "RandomTowers = np.zeros((ZeroBias.shape[0], 18, 14, 1))\n",
    "\n",
    "random_phi1 = np.random.randint(low = 1, high = 17, size = np.int(RandomTowers.shape[0]/2))\n",
    "random_eta1 = np.random.randint(low = 1, high = 13, size = np.int(RandomTowers.shape[0]/2))\n",
    "random_pt1 = np.random.randint(low = 30, high = 100, size = np.int(RandomTowers.shape[0]/2))\n",
    "random_phi2 = np.random.randint(low = 1, high = 17, size = np.int(RandomTowers.shape[0]/2))\n",
    "random_eta2 = np.random.randint(low = 1, high = 13, size = np.int(RandomTowers.shape[0]/2))\n",
    "random_pt2 = np.random.randint(low = 30, high = 100, size = np.int(RandomTowers.shape[0]/2))\n",
    "\n",
    "for i in range(np.int(RandomTowers.shape[0]/2)):\n",
    "    RandomTowers[i, random_phi1[i], random_eta1[i], 0] = random_pt1[i]\n",
    "    RandomTowers[i, random_phi1[i]+1, random_eta1[i]+1, 0] = random_pt2[i]\n",
    "    RandomTowers[i, random_phi2[i], random_eta2[i], 0] = random_pt2[i]\n",
    "    RandomTowers[i, random_phi2[i]+1, random_eta2[i], 0] = random_pt1[i]\n",
    "\n",
    "Xnoise = 3*ZeroBias \n",
    "Xclean = ZeroBias\n",
    "\n",
    "train_ratio = 0.8\n",
    "val_ratio = 0.18\n",
    "test_ratio = 1 - train_ratio - val_ratio\n",
    "\n",
    "Xnoise_train_val, Xnoise_test = train_test_split(Xnoise, test_size = test_ratio, random_state = 123)\n",
    "Xnoise_train, Xnoise_val = train_test_split(Xnoise_train_val, test_size = val_ratio/(val_ratio + train_ratio), random_state = 123)\n",
    "\n",
    "Xclean_train_val, Xclean_test = train_test_split(Xclean, test_size = test_ratio, random_state = 123)\n",
    "Xclean_train, Xclean_val = train_test_split(Xclean_train_val, test_size = val_ratio/(val_ratio + train_ratio), random_state = 123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(40,50):\n",
    "    fig, ax = plt.subplots(figsize = (10,10))\n",
    "    ax = plt.subplot(2, 2, 1)\n",
    "    ax = sns.heatmap(Xnoise_train[i,:,:,0].reshape(18, 14), vmin = 0, vmax = Xnoise_train[i,:,:,0].max(), cmap = \"Reds\", cbar_kws = {'label': 'ET (GeV)'})\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    ax.set_title('X_noise')\n",
    "    \n",
    "    ax = plt.subplot(2, 2, 2)\n",
    "    ax = sns.heatmap(Xclean_train[i,:,:,0].reshape(18, 14), vmin = 0, vmax = Xnoise_train[i,:,:,0].max(), cmap = \"Reds\", cbar_kws = {'label': 'ET (GeV)'})\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    ax.set_title('X_clean')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, X_train,\n",
    "                    epochs = 40,\n",
    "                    validation_data = (X_val, X_val),\n",
    "                    batch_size = 1024,\n",
    "                    callbacks = [\n",
    "                        tf.keras.callbacks.EarlyStopping(monitor = \"val_loss\", patience = 3, mode = \"min\")\n",
    "                    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For de-noising model\n",
    "history = model.fit(Xnoise_train, Xclean_train,\n",
    "                    epochs = 25,\n",
    "                    validation_data = (Xnoise_val, Xclean_val),\n",
    "                    batch_size = 1024\n",
    "                    #callbacks = [\n",
    "                    #    tf.keras.callbacks.EarlyStopping(monitor = \"val_loss\", patience = 3, mode = \"min\")\n",
    "                    #]\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot loss vs epoch\n",
    "plt.figure(figsize = (15,10))\n",
    "axes = plt.subplot(2, 2, 1)\n",
    "axes.plot(history.history['loss'], label = 'train loss')\n",
    "#axes.set_yscale(value = \"log\")\n",
    "axes.plot(history.history['val_loss'], label = 'val loss')\n",
    "axes.legend(loc = \"upper right\")\n",
    "axes.set_xlabel('Epoch')\n",
    "axes.set_ylabel('Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_predict = model.predict(X_train)\n",
    "X_test_predict = model.predict(X_test)\n",
    "MC_zb_predict = []\n",
    "for i in range(len(MC_zb)):\n",
    "    MC_zb_predict.append(model.predict(MC_zb[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('saved_models/convAE_model/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('saved_models/convAE_model')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate performance of the trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute and plot loss distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss_for_pred(y_true, y_pred, choice):\n",
    "    #Simple MSE\n",
    "    if choice == 0:\n",
    "        loss = np.mean((y_true - y_pred)**2, axis = (1, 2, 3))\n",
    "        return loss\n",
    "    \n",
    "    #Simple MSE for de-noising model\n",
    "    if choice == 1:\n",
    "        loss = np.mean(y_pred**2, axis = (1, 2, 3))\n",
    "        return loss\n",
    "    \n",
    "    #Same as custom_loss_for_train\n",
    "    if choice == 2:\n",
    "        loss_mse = np.mean((y_pred - y_true)**2, axis = (1, 2, 3))\n",
    "        #loss_reg = np.mean((y_pred - ZB_mean)**2, axis = (1, 2, 3))\n",
    "        loss_reg = np.mean(y_pred**2, axis = (1, 2, 3))\n",
    "        loss = loss_mse + 0.2 * loss_reg\n",
    "        return loss\n",
    "    \n",
    "    #Different weights in eta\n",
    "    if choice == 3:\n",
    "        loss = np.mean((y_true - y_pred)**2, axis = (1, 3))\n",
    "        scale = np.array([0.5, 1, 2, 4, 6, 8, 10, 10, 8, 6, 4, 2, 1, 0.5])\n",
    "        loss = loss * scale\n",
    "        loss = np.mean(loss, axis = 1)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_choice = 1\n",
    "\n",
    "X_train_loss = custom_loss_for_pred(X_train, X_train_predict, loss_choice)\n",
    "X_test_loss = custom_loss_for_pred(X_test, X_test_predict, loss_choice)\n",
    "\n",
    "MC_zb_loss = []\n",
    "for i in range(len(MC_zb)):\n",
    "    MC_zb_loss.append(custom_loss_for_pred(MC_zb[i], MC_zb_predict[i], loss_choice))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbins = 40\n",
    "rmin = 0\n",
    "rmax = 200\n",
    "plt.hist(X_train_loss, density = 1, bins = nbins, alpha = 0.3, label = 'train (ZeroBias)', range = (rmin, rmax), log = True)\n",
    "plt.hist(X_test_loss, density = 1, bins = nbins, alpha = 0.3, label = 'test (ZeroBias)', range = (rmin, rmax))\n",
    "plt.hist(MC_zb_loss[0], density = 1, bins = nbins, label = 'QCD', alpha = 0.1, histtype = 'stepfilled', range = (rmin, rmax))\n",
    "plt.hist(MC_zb_loss[1], density = 1, bins = nbins, label = 'SingleNu', alpha = 0.1, histtype = 'stepfilled', range = (rmin, rmax))\n",
    "for i in range(40,50):\n",
    "    plt.hist(MC_zb_loss[i], density = 1, bins = nbins, label = MC_files[i], histtype = 'step', range = (rmin, rmax))\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "plt.xlabel(\"loss\")\n",
    "#plt.xticks(np.arange(rmin, rmax, step = 0.0002))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot and compare the original and reconstructed inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Original vs Reconstructed\n",
    "#show_ZB = True\n",
    "show_ZB = False\n",
    "n = 19\n",
    "for i in range(580,590):\n",
    "    fig, ax = plt.subplots(figsize = (17,17))\n",
    "    if show_ZB == True:\n",
    "        print('ZB test\\nloss = ' + str(X_test_loss[i]))\n",
    "    else:\n",
    "        print(str(MC_files[n]) + '\\nloss = ' + str(MC_zb_loss[n][i]))\n",
    "    ax = plt.subplot(3, 3, 1)\n",
    "    if show_ZB == True:\n",
    "        ax = sns.heatmap(X_test[i,:,:,0].reshape(18, 14), vmin = 0, vmax = X_test[i,:,:,0].max(), cmap = \"Reds\", cbar_kws = {'label': 'ET (GeV)'})\n",
    "    else:\n",
    "        ax = sns.heatmap(MC_zb[n][i,:,:,0].reshape(18, 14), vmin = 0, vmax = MC_zb[n][i,:,:,0].max(), cmap = \"Reds\", cbar_kws = {'label': 'ET (GeV)'})\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    ax.set_title('Original')\n",
    "    \n",
    "    ax = plt.subplot(3, 3, 2)\n",
    "    if show_ZB == True:\n",
    "        ax = sns.heatmap(X_test_predict[i,:,:,0].reshape(18, 14), vmin = 0, vmax = X_test[i,:,:,0].max(), cmap = \"Reds\", cbar_kws = {'label': 'ET (GeV)'})\n",
    "    else:\n",
    "        ax = sns.heatmap(MC_zb_predict[n][i,:,:,0].reshape(18, 14), vmin = 0, vmax = MC_zb[n][i,:,:,0].max(), cmap = \"Reds\", cbar_kws = {'label': 'ET (GeV)'})\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    ax.set_title('Reconstructed')\n",
    "    \n",
    "    ax = plt.subplot(3, 3, 3)\n",
    "    if show_ZB == True:\n",
    "        ax = sns.heatmap(np.absolute(X_test_predict[i,:,:,0] - X_test[i,:,:,0]).reshape(18, 14), vmin = 0, vmax = X_test[i,:,:,0].max(), cmap = \"Reds\", cbar_kws = {'label': 'ET (GeV)'})\n",
    "    else:\n",
    "        ax = sns.heatmap(np.absolute(MC_zb_predict[n][i,:,:,0] - MC_zb[n][i,:,:,0]).reshape(18, 14), vmin = 0, vmax = MC_zb[n][i,:,:,0].max(), cmap = \"Reds\", cbar_kws = {'label': 'ET (GeV)'})\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    ax.set_title('abs(original-reconstructed)')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assign labels and concatenate testing sets for ROC plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Baseline\n",
    "#Assuming only the mean ZB is learned\n",
    "#Take mean ZB as outputs no matter what inputs are\n",
    "#Classifier of baseline = MSE(inputs, ZeroBias_mean)\n",
    "ZeroBias_mean = np.mean(ZeroBias, axis = 0)\n",
    "\n",
    "baseline_zb = np.mean((X_test - ZeroBias_mean)**2, axis = (1, 2))\n",
    "baseline_mc = []\n",
    "for i in range(len(MC_zb)):\n",
    "    baseline_mc.append(np.mean((MC_zb[i] - ZeroBias_mean)**2, axis = (1, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assign labels for various signals (y = 1) and backgrounds (y = 0)\n",
    "Y_zb = np.zeros((X_test.shape[0], 1))\n",
    "Y_mc = []\n",
    "for i in range(len(MC)):\n",
    "    Y_mc.append(np.ones((MC_zb[i].shape[0], 1)))\n",
    "\n",
    "#Concatenate datasets to make ROC curves, i.e. QCD/SingleNu/signals vs ZB\n",
    "\n",
    "#True labels\n",
    "Y_true = []\n",
    "#Baseline scores\n",
    "Y_baseline = []\n",
    "#Model scores\n",
    "Y_model = []\n",
    "for i in range(len(MC)):\n",
    "    Y_true.append(np.concatenate((Y_mc[i], Y_zb)))\n",
    "    Y_baseline.append(np.concatenate((baseline_mc[i], baseline_zb)))\n",
    "    Y_model.append(np.concatenate((MC_zb_loss[i], X_test_loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply acceptance cut on signal MC\n",
    "sig_filter = []\n",
    "for i in range(len(sig_acceptance)):\n",
    "    sig_filter.append([])\n",
    "    for j in range(MC[i+3].shape[0]):\n",
    "        if sig_acceptance[i][j] == 1:\n",
    "            sig_filter[i].append(True)\n",
    "        else:\n",
    "            sig_filter[i].append(False)\n",
    "    \n",
    "Y_mc_acceptance = []\n",
    "baseline_mc_acceptance = []\n",
    "MC_zb_loss_acceptance = []\n",
    "for i in range(len(MC)):\n",
    "    if i > 2 and i < 52:\n",
    "        Y_mc_acceptance.append(Y_mc[i][sig_filter[i-3],:])\n",
    "        baseline_mc_acceptance.append(baseline_mc[i][sig_filter[i-3],:])\n",
    "        MC_zb_loss_acceptance.append(MC_zb_loss[i][sig_filter[i-3]])\n",
    "    else:\n",
    "        Y_mc_acceptance.append(Y_mc[i])\n",
    "        baseline_mc_acceptance.append(baseline_mc[i])\n",
    "        MC_zb_loss_acceptance.append(MC_zb_loss[i])\n",
    "        \n",
    "#True labels\n",
    "Y_true_acceptance = []\n",
    "#Baseline scores\n",
    "Y_baseline_acceptance = []\n",
    "#Model scores\n",
    "Y_model_acceptance = []\n",
    "for i in range(len(MC)):\n",
    "    Y_true_acceptance.append(np.concatenate((Y_mc_acceptance[i], Y_zb)))\n",
    "    Y_baseline_acceptance.append(np.concatenate((baseline_mc_acceptance[i], baseline_zb)))\n",
    "    Y_model_acceptance.append(np.concatenate((MC_zb_loss_acceptance[i], X_test_loss)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot baseline ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (13, 13))\n",
    "axes = plt.subplot(2, 2, 1)\n",
    "fpr_baseline = []\n",
    "tpr_baseline = []\n",
    "thresholds_baseline = []\n",
    "roc_auc_baseline = []\n",
    "for i in range(len(MC)):\n",
    "    fpr_baseline.append(np.empty((Y_true[i].shape[0],1)))\n",
    "    tpr_baseline.append(np.empty((Y_true[i].shape[0],1)))\n",
    "    thresholds_baseline.append(np.empty((Y_true[i].shape[0],1)))\n",
    "    roc_auc_baseline.append(np.empty((Y_true[i].shape[0],1)))\n",
    "    fpr_baseline[i], tpr_baseline[i], thresholds_baseline[i] = roc_curve(Y_true[i], Y_baseline[i])\n",
    "    roc_auc_baseline[i] = auc(fpr_baseline[i], tpr_baseline[i])\n",
    "    if i == 0:\n",
    "        print(1)\n",
    "        #axes.plot(fpr_baseline[i], tpr_baseline[i], linestyle = '--', color = 'r', lw = 2, label = MC_files[i] + ' (AUC = %.8f)' % (roc_auc_baseline[i]))\n",
    "    elif i == 1 or i == 2 or i == 52:\n",
    "        print(1)\n",
    "        #axes.plot(fpr_baseline[i], tpr_baseline[i], linestyle = ':', lw = 2, label = MC_files[i] + ' (AUC = %.8f)' % (roc_auc_baseline[i]))\n",
    "    else:\n",
    "        axes.plot(fpr_baseline[i], tpr_baseline[i], linestyle = '-', lw = 2, label = MC_files[i] + ' (AUC = %.8f)' % (roc_auc_baseline[i]))\n",
    "#axes.plot([0, 1], [0, 1], linestyle = '--', lw = 2, color = 'black', label = 'random chance')\n",
    "axes.plot([0.002, 0.002], [0, 1], linestyle = '--', lw = 2, color = 'black', label = 'FPR = 0.2% ~ (100 kHz)/(ZB rate)')\n",
    "axes.set_xlim([0.0001, 1.0])\n",
    "#axes.set_xlim([0, 1.0])\n",
    "axes.set_ylim([0, 1.0])\n",
    "#axes.set_ylim([0.9, 1.0])\n",
    "axes.set_xscale(value = \"log\")\n",
    "#axes.set_yscale(value = \"log\")\n",
    "axes.set_xlabel('False Positive Rate (FPR)')\n",
    "axes.set_ylabel('True Positive Rate (TPR)')\n",
    "axes.set_title('Baseline ROC')\n",
    "axes.legend(loc='center left', bbox_to_anchor = (1.15, 0.5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot model ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (13, 13))\n",
    "axes = plt.subplot(2, 2, 1)\n",
    "fpr_model = []\n",
    "tpr_model = []\n",
    "thresholds_model = []\n",
    "roc_auc_model = []\n",
    "for i in range(len(MC)):\n",
    "    fpr_model.append(np.empty((Y_true[i].shape[0],1)))\n",
    "    tpr_model.append(np.empty((Y_true[i].shape[0],1)))\n",
    "    thresholds_model.append(np.empty((Y_true[i].shape[0],1)))\n",
    "    roc_auc_model.append(np.empty((Y_true[i].shape[0],1)))\n",
    "    fpr_model[i], tpr_model[i], thresholds_model[i] = roc_curve(Y_true[i], Y_model[i])\n",
    "    roc_auc_model[i] = auc(fpr_model[i], tpr_model[i])\n",
    "    if i == 0:\n",
    "        print(1)\n",
    "        #axes.plot(fpr_model[i], tpr_model[i], linestyle = '--', color = 'r', lw = 2, label = MC_files[i] + ' (AUC = %.8f)' % (roc_auc_model[i]))\n",
    "    elif i == 1 or i == 2 or i == 52:\n",
    "        print(1)\n",
    "        #axes.plot(fpr_model[i], tpr_model[i], linestyle = ':', lw = 2, label = MC_files[i] + ' (AUC = %.8f)' % (roc_auc_model[i]))\n",
    "    else:\n",
    "        axes.plot(fpr_model[i], tpr_model[i], linestyle = '-', lw = 2, label = MC_files[i] + ' (AUC = %.8f)' % (roc_auc_model[i]))\n",
    "#axes.plot([0, 1], [0, 1], linestyle = '--', lw = 2, color = 'black', label = 'random chance')\n",
    "axes.plot([0.002, 0.002], [0, 1], linestyle = '--', lw = 2, color = 'black', label = 'FPR = 0.2% ~ (100 kHz)/(ZB rate)')\n",
    "axes.set_xlim([0.0001, 1.0])\n",
    "#axes.set_xlim([0, 1.0])\n",
    "axes.set_ylim([0, 1.0])\n",
    "#axes.set_ylim([0.9, 1.0])\n",
    "axes.set_xscale(value = \"log\")\n",
    "#axes.set_yscale(value = \"log\")\n",
    "axes.set_xlabel('False Positive Rate (FPR)')\n",
    "axes.set_ylabel('True Positive Rate (TPR)')\n",
    "axes.set_title('Model ROC')\n",
    "axes.legend(loc='center left', bbox_to_anchor = (1.15, 0.5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (13, 13))\n",
    "axes = plt.subplot(2, 2, 1)\n",
    "fpr_model_acceptance = []\n",
    "tpr_model_acceptance = []\n",
    "thresholds_model_acceptance = []\n",
    "roc_auc_model_acceptance = []\n",
    "for i in range(len(MC)):\n",
    "    fpr_model_acceptance.append(np.empty((Y_true_acceptance[i].shape[0],1)))\n",
    "    tpr_model_acceptance.append(np.empty((Y_true_acceptance[i].shape[0],1)))\n",
    "    thresholds_model_acceptance.append(np.empty((Y_true_acceptance[i].shape[0],1)))\n",
    "    roc_auc_model_acceptance.append(np.empty((Y_true_acceptance[i].shape[0],1)))\n",
    "    fpr_model_acceptance[i], tpr_model_acceptance[i], thresholds_model_acceptance[i] = roc_curve(Y_true_acceptance[i], Y_model_acceptance[i])\n",
    "    roc_auc_model_acceptance[i] = auc(fpr_model_acceptance[i], tpr_model_acceptance[i])\n",
    "    if i == 0:\n",
    "        print(1)\n",
    "        #axes.plot(fpr_model_acceptance[i], tpr_model_acceptance[i], linestyle = '--', color = 'r', lw = 2, label = MC_files[i] + ' (AUC = %.8f)' % (roc_auc_model_acceptance[i]))\n",
    "    elif i == 1 or i == 2 or i == 52:\n",
    "        print(1)\n",
    "        #axes.plot(fpr_model_acceptance[i], tpr_model_acceptance[i], linestyle = ':', lw = 2, label = MC_files[i] + ' (AUC = %.8f)' % (roc_auc_model_acceptance[i]))\n",
    "    else:\n",
    "        axes.plot(fpr_model_acceptance[i], tpr_model_acceptance[i], linestyle = '-', lw = 2, label = MC_files[i] + ' (AUC = %.8f)' % (roc_auc_model_acceptance[i]))\n",
    "#axes.plot([0, 1], [0, 1], linestyle = '--', lw = 2, color = 'black', label = 'random chance')\n",
    "axes.plot([0.002, 0.002], [0, 1], linestyle = '--', lw = 2, color = 'black', label = 'FPR = 0.2% ~ (100 kHz)/(ZB rate)')\n",
    "axes.set_xlim([0.0001, 1.0])\n",
    "#axes.set_xlim([0, 1.0])\n",
    "axes.set_ylim([0, 1.0])\n",
    "#axes.set_ylim([0.9, 1.0])\n",
    "axes.set_xscale(value = \"log\")\n",
    "#axes.set_yscale(value = \"log\")\n",
    "axes.set_xlabel('False Positive Rate (FPR)')\n",
    "axes.set_ylabel('True Positive Rate (TPR)')\n",
    "axes.set_title('Model ROC')\n",
    "axes.legend(loc='center left', bbox_to_anchor = (1.15, 0.5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tabulate TPR at fixed FPR = 0.2% (baseline, model, change)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_tpr_baseline = []\n",
    "table_tpr_model = []\n",
    "table_tpr_model_acceptance = []\n",
    "table_tpr_change = []\n",
    "for i in range(3, len(fpr_baseline)):\n",
    "    for j in range(len(fpr_baseline[i])):\n",
    "        if fpr_baseline[i][j] > 0.002:\n",
    "            table_tpr_baseline.append(tpr_baseline[i][j] * 100)\n",
    "            break\n",
    "    for j in range(len(fpr_model[i])):\n",
    "        if fpr_model[i][j] > 0.002:\n",
    "            table_tpr_model.append(tpr_model[i][j] * 100)\n",
    "            break\n",
    "    '''\n",
    "    for j in range(len(fpr_model_acceptance[i])):\n",
    "        if fpr_model_acceptance[i][j] > 0.002:\n",
    "            table_tpr_model_acceptance.append(tpr_model_acceptance[i][j] * 100)\n",
    "            break\n",
    "    '''\n",
    "for i in range(len(MC)-3):\n",
    "    table_tpr_change.append(100 * (table_tpr_model[i] - table_tpr_baseline[i])/table_tpr_baseline[i])\n",
    "    #table_tpr_change.append(np.round(100 * (table_tpr_model_acceptance[i] - table_tpr_model[i])/table_tpr_model[i], 2))\n",
    "    \n",
    "\n",
    "table_tpr = pd.DataFrame({'Baseline TPR@FPR=0.2%': table_tpr_baseline,\n",
    "                          'Model TPR@FPR=0.2%': table_tpr_model,\n",
    "                          '% increase': table_tpr_change},\n",
    "                        index = MC_files[3:])\n",
    "table_tpr = table_tpr.sort_values(by = '% increase', ascending = False)\n",
    "'''\n",
    "table_tpr = pd.DataFrame({'before cut': table_tpr_model,\n",
    "                          'after cut': table_tpr_model_acceptance,\n",
    "                          '% increase': table_tpr_change},\n",
    "                        index = MC_files)\n",
    "table_tpr = table_tpr.sort_values(by = '% increase', ascending = False)\n",
    "'''\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "table_tpr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot ROC, split in TPR ranges at FPR = 0.003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split ROC by efficiency range\n",
    "sig_idx_eff_0to50 = []\n",
    "sig_idx_eff_50to60 = []\n",
    "sig_idx_eff_60to70 = []\n",
    "sig_idx_eff_70to80 = []\n",
    "sig_idx_eff_80to90 = []\n",
    "sig_idx_eff_90to95 = []\n",
    "sig_idx_eff_95to99 = []\n",
    "sig_idx_eff_99to100 = []\n",
    "\n",
    "for i in range(len(fpr_vs_zb)):\n",
    "    for j in range(len(fpr_vs_zb[i])):\n",
    "        if fpr_vs_zb[i][j] > 0.003:\n",
    "            if tpr_vs_zb[i][j] > 0.99:\n",
    "                sig_idx_eff_99to100.append(i)\n",
    "            elif tpr_vs_zb[i][j] > 0.95:\n",
    "                sig_idx_eff_95to99.append(i)\n",
    "            elif tpr_vs_zb[i][j] > 0.9:\n",
    "                sig_idx_eff_90to95.append(i)\n",
    "            elif tpr_vs_zb[i][j] > 0.8:\n",
    "                sig_idx_eff_80to90.append(i)\n",
    "            elif tpr_vs_zb[i][j] > 0.7:\n",
    "                sig_idx_eff_70to80.append(i)\n",
    "            elif tpr_vs_zb[i][j] > 0.6:\n",
    "                sig_idx_eff_60to70.append(i)\n",
    "            elif tpr_vs_zb[i][j] > 0.5:\n",
    "                sig_idx_eff_50to60.append(i)\n",
    "            else:\n",
    "                sig_idx_eff_0to50.append(i)\n",
    "            break\n",
    "sig_idx_eff_all = []\n",
    "sig_idx_eff_all.append(sig_idx_eff_0to50)\n",
    "sig_idx_eff_all.append(sig_idx_eff_50to60)\n",
    "sig_idx_eff_all.append(sig_idx_eff_60to70)\n",
    "sig_idx_eff_all.append(sig_idx_eff_70to80)\n",
    "sig_idx_eff_all.append(sig_idx_eff_80to90)\n",
    "sig_idx_eff_all.append(sig_idx_eff_90to95)\n",
    "sig_idx_eff_all.append(sig_idx_eff_95to99)\n",
    "sig_idx_eff_all.append(sig_idx_eff_99to100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot ROC by efficiency range\n",
    "for i in range(len(sig_idx_eff_all)):\n",
    "    plt.figure(figsize = (13, 13))\n",
    "    axes = plt.subplot(2, 2, 1)\n",
    "    for j in range(len(sig_idx_eff_all[i])):\n",
    "        axes.plot(fpr_vs_zb[sig_idx_eff_all[i][j]], tpr_vs_zb[sig_idx_eff_all[i][j]], lw = 2, label = signal_files[sig_idx_eff_all[i][j]] + ' (AUC = %.8f)' % (roc_auc_vs_zb[sig_idx_eff_all[i][j]]))\n",
    "    #axes.plot([0, 1], [0, 1], linestyle = '--', lw = 2, color = 'black', label = 'random chance')\n",
    "    axes.plot([0.003, 0.003], [0, 1], linestyle = '--', lw = 2, color = 'black', label = 'Max tolerable fake rate ~ 0.003 (ZB -> L1T rate)')\n",
    "    axes.set_xlim([0.0001, 1.0])\n",
    "    #axes.set_xlim([0, 1.0])\n",
    "    axes.set_ylim([0, 1.0])\n",
    "    #axes.set_ylim([0.99, 1.0])\n",
    "    axes.set_xscale(value = \"log\")\n",
    "    #axes.set_yscale(value = \"log\")\n",
    "    axes.set_xlabel('Fake positive rate (FPR)')\n",
    "    axes.set_ylabel('True positive rate (TPR)')\n",
    "    axes.set_title('ROC for signals vs ZB')\n",
    "    axes.legend(loc = 'center left', bbox_to_anchor = (1, 0.5))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
