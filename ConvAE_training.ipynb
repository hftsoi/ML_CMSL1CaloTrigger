{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Autoencoder Training for Anomaly Detection @ L1Trigger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "import keras_tuner\n",
    "from keras_tuner import Hyperband\n",
    "\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input files reading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All input files are already sorted in Calo regions (i, j) ~ (18, 14)<br>\n",
    "Where i = 0 -> 17 corresponds to GCT_Phi = 0 -> 17<br>\n",
    "Where j = 0 -> 13 corresponds to RCT_Eta = 4 -> 17\n",
    "\n",
    "Keep this ordering as is when feeding into neural nets. Also keep this in mind when generating/preparing new samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zerobias and MC signal files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ZeroBias = np.concatenate((h5py.File('bkg/ZeroBias_0.h5', 'r')['CaloRegions'][()],\n",
    "                           h5py.File('bkg/ZeroBias_1.h5', 'r')['CaloRegions'][()],\n",
    "                           h5py.File('bkg/ZeroBias_2.h5', 'r')['CaloRegions'][()]))\n",
    "ZeroBias = ZeroBias.astype(dtype = 'float32').reshape(-1, 18, 14, 1)\n",
    "print('ZeroBias shape: ' + str(ZeroBias.shape))\n",
    "\n",
    "MC_files = []\n",
    "MC_files.append('bkg/110X/QCD_0.h5')#i=0\n",
    "#MC_files.append('bkg/110X/QCD_1.h5')\n",
    "#MC_files.append('bkg/110X/QCD_2.h5')\n",
    "MC_files.append('bkg/120X/SingleNeutrino_E-10_0.h5')#i=1\n",
    "#MC_files.append('bkg/120X/SingleNeutrino_E-10_1.h5')\n",
    "#MC_files.append('bkg/120X/SingleNeutrino_E-10_2.h5')\n",
    "MC_files.append('bkg/120X/SingleNeutrino_Pt-2To20_0.h5')#i=2\n",
    "#MC_files.append('bkg/120X/SingleNeutrino_Pt-2To20_1.h5')\n",
    "#MC_files.append('bkg/120X/SingleNeutrino_Pt-2To20_2.h5')\n",
    "\n",
    "MC_files.append('sig/110X/GluGluToHHTo4B_node_SM_TuneCP5_14TeV.h5')#i=3\n",
    "'''\n",
    "MC_files.append('sig/110X/HTo2LongLivedTo4mu_MH-1000_MFF-450_CTau-10000mm_TuneCP5_14TeV.h5')\n",
    "MC_files.append('sig/110X/HTo2LongLivedTo4mu_MH-125_MFF-12_CTau-900mm_TuneCP5_14TeV.h5')\n",
    "MC_files.append('sig/110X/HTo2LongLivedTo4mu_MH-125_MFF-25_CTau-1500mm_TuneCP5_14TeV.h5')\n",
    "MC_files.append('sig/110X/HTo2LongLivedTo4mu_MH-125_MFF-50_CTau-3000mm_TuneCP5_14TeV.h5')\n",
    "MC_files.append('sig/110X/VBFHToTauTau_M125_TuneCUETP8M1_14TeV.h5')\n",
    "MC_files.append('sig/110X/VBF_HH_CV_1_C2V_1_C3_1_TuneCP5_PSweights_14TeV.h5')\n",
    "MC_files.append('sig/110X/VBF_HToInvisible_M125_TuneCUETP8M1_14TeV.h5')\n",
    "MC_files.append('sig/110X/VectorZPrimeToQQ_M100_pT300_TuneCP5_14TeV.h5')\n",
    "MC_files.append('sig/110X/VectorZPrimeToQQ_M200_pT300_TuneCP5_14TeV.h5')\n",
    "MC_files.append('sig/110X/VectorZPrimeToQQ_M50_pT300_TuneCP5_14TeV.h5')#i=13\n",
    "MC_files.append('sig/110X/ZprimeToZH_MZprime1000_MZ50_MH80_ZTouds_HTouds_narrow_TuneCP5_14TeV.h5')\n",
    "MC_files.append('sig/110X/ZprimeToZH_MZprime600_MZ50_MH80_ZTouds_HTouds_narrow_TuneCP5_14TeV.h5')\n",
    "MC_files.append('sig/110X/ZprimeToZH_MZprime800_MZ50_MH80_ZTouds_HTouds_narrow_TuneCP5_14TeV.h5')\n",
    "'''\n",
    "MC_files.append('sig/120X/GluGluHToTauTau_M-125_TuneCP5_14TeV.h5')\n",
    "'''\n",
    "MC_files.append('sig/120X/GluGluToHHTo4B_node_cHHH1_TuneCP5_14TeV.h5')\n",
    "MC_files.append('sig/120X/GluGluToHHTo4B_node_cHHH5_TuneCP5_14TeV.h5')\n",
    "MC_files.append('sig/120X/HTo2LongLivedTo4b_MH-1000_MFF-450_CTau-100000mm_TuneCP5_14TeV.h5')\n",
    "MC_files.append('sig/120X/HTo2LongLivedTo4b_MH-1000_MFF-450_CTau-10000mm_TuneCP5_14TeV.h5')\n",
    "MC_files.append('sig/120X/HTo2LongLivedTo4b_MH-125_MFF-12_CTau-9000mm_TuneCP5_14TeV.h5')\n",
    "MC_files.append('sig/120X/HTo2LongLivedTo4b_MH-125_MFF-12_CTau-900mm_TuneCP5_14TeV.h5')#i=23\n",
    "MC_files.append('sig/120X/HTo2LongLivedTo4b_MH-125_MFF-25_CTau-15000mm_TuneCP5_14TeV.h5')\n",
    "MC_files.append('sig/120X/HTo2LongLivedTo4b_MH-125_MFF-25_CTau-1500mm_TuneCP5_14TeV.h5')\n",
    "MC_files.append('sig/120X/HTo2LongLivedTo4b_MH-125_MFF-50_CTau-30000mm_TuneCP5_14TeV.h5')\n",
    "MC_files.append('sig/120X/HTo2LongLivedTo4b_MH-125_MFF-50_CTau-3000mm_TuneCP5_14TeV.h5')\n",
    "MC_files.append('sig/120X/HTo2LongLivedTo4b_MH-250_MFF-120_CTau-10000mm_TuneCP5_14TeV.h5')\n",
    "MC_files.append('sig/120X/HTo2LongLivedTo4b_MH-250_MFF-120_CTau-1000mm_TuneCP5_14TeV.h5')\n",
    "MC_files.append('sig/120X/HTo2LongLivedTo4b_MH-250_MFF-60_CTau-1000mm_TuneCP5_14TeV.h5')\n",
    "MC_files.append('sig/120X/HTo2LongLivedTo4b_MH-350_MFF-160_CTau-10000mm_TuneCP5_14TeV.h5')\n",
    "MC_files.append('sig/120X/HTo2LongLivedTo4b_MH-350_MFF-160_CTau-1000mm_TuneCP5_14TeV.h5')\n",
    "MC_files.append('sig/120X/HTo2LongLivedTo4b_MH-350_MFF-160_CTau-500mm_TuneCP5_14TeV.h5')#i=33\n",
    "'''\n",
    "MC_files.append('sig/120X/HTo2LongLivedTo4b_MH-350_MFF-80_CTau-10000mm_TuneCP5_14TeV.h5')\n",
    "'''\n",
    "MC_files.append('sig/120X/HTo2LongLivedTo4b_MH-350_MFF-80_CTau-1000mm_TuneCP5_14TeV.h5')\n",
    "MC_files.append('sig/120X/HTo2LongLivedTo4b_MH-350_MFF-80_CTau-500mm_TuneCP5_14TeV.h5')\n",
    "MC_files.append('sig/120X/HTo2LongLivedTo4mu_MH-1000_MFF-450_CTau-10000mm_TuneCP5_14TeV.h5')\n",
    "MC_files.append('sig/120X/HTo2LongLivedTo4mu_MH-125_MFF-12_CTau-900mm_TuneCP5_14TeV.h5')\n",
    "MC_files.append('sig/120X/HTo2LongLivedTo4mu_MH-125_MFF-25_CTau-1500mm_TuneCP5_14TeV.h5')\n",
    "MC_files.append('sig/120X/HTo2LongLivedTo4mu_MH-125_MFF-50_CTau-3000mm_TuneCP5_14TeV.h5')\n",
    "MC_files.append('sig/120X/SUSYGluGluToBBHToBB_NarrowWidth_M-1200_TuneCP5_13TeV-pythia814TeV.h5')\n",
    "MC_files.append('sig/120X/SUSYGluGluToBBHToBB_NarrowWidth_M-120_TuneCP5_14TeV.h5')\n",
    "MC_files.append('sig/120X/SUSYGluGluToBBHToBB_NarrowWidth_M-350_TuneCP5_14TeV.h5')#i=43\n",
    "MC_files.append('sig/120X/SUSYGluGluToBBHToBB_NarrowWidth_M-600_TuneCP5_14TeV.h5')\n",
    "MC_files.append('sig/120X/TprimeBToTH_M-650_LH_TuneCP5_14TeV.h5')\n",
    "MC_files.append('sig/120X/VBFHHTo4B_CV_1_C2V_2_C3_1_TuneCP5_14TeV.h5')\n",
    "MC_files.append('sig/120X/VBFHToInvisible_M125_TuneCP5_14TeV.h5')\n",
    "'''\n",
    "MC_files.append('sig/120X/VBFHToTauTau_M125_TuneCP5_14TeV.h5')\n",
    "'''\n",
    "MC_files.append('sig/120X/VectorZPrimeGammaToQQGamma_M-10_GPt-75_TuneCP5_14TeV.h5')\n",
    "MC_files.append('sig/120X/VectorZPrimeToQQ_M-100_Pt-300_TuneCP5_14TeV.h5')\n",
    "MC_files.append('sig/120X/VectorZPrimeToQQ_M-200_Pt-300_TuneCP5_14TeV.h5')#i=51\n",
    "'''\n",
    "MC = []\n",
    "AcceptanceFlag = []\n",
    "for i in range(len(MC_files)):\n",
    "    MC.append(h5py.File(MC_files[i], 'r')['CaloRegions'][()].astype(dtype = 'float32'))\n",
    "    MC[i] = MC[i].reshape(-1, 18, 14, 1)\n",
    "    #Read acceptance flag in MC signals\n",
    "    if i > 2:\n",
    "        AcceptanceFlag.append(h5py.File(MC_files[i], 'r')['AcceptanceFlag'][()])\n",
    "    else:\n",
    "        AcceptanceFlag.append(np.ones((MC[i].shape[0])))\n",
    "    \n",
    "MC[0] = MC[0][:10000,:,:,:]#QCD\n",
    "MC[1] = MC[1][:10000,:,:,:]#SingleNu_E10\n",
    "MC[2] = MC[2][:10000,:,:,:]#SingleNu_Pt2To20\n",
    "\n",
    "'''\n",
    "#/nfs_scratch/dasu/2022-02-04/L1TSignalZerobiasMixer/cms-vbfh.csv\n",
    "vbf = pd.read_csv('cms-vbfh.csv')\n",
    "vbf.columns = ['eta','phi','et','pos','ebit','tbit']\n",
    "vbf = vbf[251:]\n",
    "\n",
    "event_col = []\n",
    "for i in range(round(vbf.shape[0]/252)):\n",
    "    for j in range(252):\n",
    "        event_col.append(i)\n",
    "        \n",
    "vbf['event'] = event_col\n",
    "vbf = vbf.drop(['pos','ebit','tbit'],axis=1)\n",
    "vbf = vbf.sort_values(by=['event', 'phi', 'eta'], ascending = [True, True, True])\n",
    "vbf = vbf.reindex(columns=['event','phi','eta','et'])\n",
    "vbf = vbf.drop(['event'],axis=1)\n",
    "vbf = vbf.to_numpy()\n",
    "vbf = vbf.reshape((-1,18,14,3))\n",
    "vbf = vbf[:,:,:,2]\n",
    "vbf = vbf.reshape((-1,18,14,1))\n",
    "vbf.shape\n",
    "MC_files.append('/nfs_scratch/dasu/2022-02-04/L1TSignalZerobiasMixer/cms-vbfh.csv')\n",
    "MC.append(vbf)\n",
    "'''\n",
    "\n",
    "#Throw away MC signal events that failed to pass the acceptance cuts\n",
    "acceptance_filter = []\n",
    "for i in range(len(MC_files)):\n",
    "    acceptance_filter.append([])\n",
    "    for j in range(MC[i].shape[0]):\n",
    "        if AcceptanceFlag[i][j] == 1:\n",
    "            acceptance_filter[i].append(True)\n",
    "        else:\n",
    "            acceptance_filter[i].append(False)\n",
    "    MC[i] = MC[i][acceptance_filter[i],:,:,:]\n",
    "    print('i = ' + str(i) + ': ' + str(MC[i].shape) + '; accepted ' + str(np.round(np.mean(AcceptanceFlag[i]), 4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Throw away events with max pt > 1023 GeV, since the calo system cannot produce more than that (input pt is 10 bits)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filter1023_zb = []\n",
    "for i in range(ZeroBias.shape[0]):\n",
    "    if ZeroBias[i,:,:,0].max() > 1023:\n",
    "        filter1023_zb.append(False)\n",
    "    else:\n",
    "        filter1023_zb.append(True)\n",
    "ZeroBias = ZeroBias[filter1023_zb,:,:,:]\n",
    "print('ZeroBias shape = ' + str(ZeroBias.shape) + '; fraction left = ' + str(round(ZeroBias.shape[0]/len(filter1023_zb),4)))\n",
    "\n",
    "filter1023_mc = []\n",
    "for i in range(len(MC_files)):\n",
    "    filter1023_mc.append([])\n",
    "    for j in range(MC[i].shape[0]):\n",
    "        if MC[i][j,:,:,0].max() > 1023:\n",
    "            filter1023_mc[i].append(False)\n",
    "        else:\n",
    "            filter1023_mc[i].append(True)\n",
    "    MC[i] = MC[i][filter1023_mc[i],:,:,:]\n",
    "    print('i = ' + str(i) + ': ' + str(MC[i].shape) + '; fraction left = ' + str(round(MC[i].shape[0]/len(filter1023_mc[i]),4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MC samples are clean, so we need to overlay them with ZB to be more realistic before doing any training/testing. It can be achieved by simple region-by-region addition between the two: MC(i,j) = MC(i,j) + ZB(i,j), where the ZB can be chosen at random per MC event."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "MC_zb = []\n",
    "for i in range(len(MC)):\n",
    "    MC_zb.append(np.empty((MC[i].shape[0], 18, 14, 1)))\n",
    "    ZB_random_event = np.random.randint(low = 0, high = ZeroBias.shape[0], size = MC[i].shape[0])\n",
    "    for j in range(MC[i].shape[0]):\n",
    "        MC_zb[i][j, :, :, 0] = ZeroBias[ZB_random_event[j], :, :, 0] + MC[i][j, :, :, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the calo region plots before and after the overlay,\n",
    "\n",
    "where n = 0 (QCD), 1 (SingleNu_E10), 2 (SingleNu_Pt2To20), 3... (signals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 0\n",
    "for i in range(40,50):\n",
    "    fig, ax = plt.subplots(figsize = (10,10))\n",
    "    print(str(MC_files[n]))\n",
    "    ax = plt.subplot(2, 2, 1)\n",
    "    ax = sns.heatmap(MC[n][i,:,:,0].reshape(18, 14), vmin = 0, vmax = MC[n][i,:,:,0].max(), cmap = \"Reds\", cbar_kws = {'label': 'Normalized ET'})\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    ax.set_title('MC')\n",
    "    \n",
    "    ax = plt.subplot(2, 2, 2)\n",
    "    ax = sns.heatmap(MC_zb[n][i,:,:,0].reshape(18, 14), vmin = 0, vmax = MC[n][i,:,:,0].max(), cmap = \"Reds\", cbar_kws = {'label': 'Normalized ET'})\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    ax.set_title('MC+ZB')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at some ZB statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ZB_mean = np.mean(ZeroBias, axis = 0)\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (10,10))\n",
    "ax = plt.subplot(2, 2, 1)\n",
    "ax = sns.heatmap(ZB_mean.reshape(18, 14), vmin = 0, vmax = ZB_mean.max(), cmap = \"Reds\", cbar_kws = {'label': 'ET (GeV)'})\n",
    "ax.get_xaxis().set_visible(False)\n",
    "ax.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(ZeroBias.reshape((-1)), bins = 20, log = True)\n",
    "plt.xlabel(\"ZeroBias Et\")\n",
    "plt.show()\n",
    "\n",
    "print('Mean ZeroBias pT = ' + str(np.mean(ZeroBias.reshape(-1))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very few ZB events might have high pt regions since they could contain signal, do we want to put a cut on ZB before training?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_filter = []\n",
    "for i in range(ZeroBias.shape[0]):\n",
    "    if ZeroBias[i,:,:,0].max() < 30.0:\n",
    "        pt_filter.append(True)\n",
    "    else:\n",
    "        pt_filter.append(False)\n",
    "ZeroBias_ptcut = ZeroBias[pt_filter,:,:,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter searching (no quantization here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to train with custom loss functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "def custom_loss_for_train():\n",
    "    def func(y_true, y_pred):\n",
    "        #MSE(output, input)\n",
    "        loss = K.mean((y_pred - y_true)**2, axis = [1, 2, 3])\n",
    "        \n",
    "        #MSE(output, mean ZB)\n",
    "        #loss = K.mean((y_pred - ZB_mean)**2, axis = [1, 2, 3])\n",
    "        \n",
    "        #MSE(output, 0) for denoising\n",
    "        #loss = K.mean(y_pred**2, axis = [1, 2, 3])\n",
    "        \n",
    "        return loss\n",
    "    return func"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hypermodel for convolutional autoencoder (usually a teacher model for Knowledge Distillation later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hypermodel(hp):\n",
    "    hp_model = tf.keras.Sequential()\n",
    "    hp_model.add(tf.keras.layers.InputLayer(input_shape = (18, 14, 1)))\n",
    "    hp_model.add(layers.Conv2D(filters = hp.Int('filters_1',\n",
    "                                                min_value = 15,\n",
    "                                                max_value = 25,\n",
    "                                                step = 2),\n",
    "                               kernel_size = (3, 3),\n",
    "                               activation = 'relu',\n",
    "                               strides = 1,\n",
    "                               padding = 'same'))\n",
    "    \n",
    "    hp_model.add(layers.AveragePooling2D((2, 2)))\n",
    "    hp_model.add(layers.Conv2D(filters = hp.Int('filters_2',\n",
    "                                                min_value = 15,\n",
    "                                                max_value = 25,\n",
    "                                                step = 2),\n",
    "                               kernel_size = (3, 3),\n",
    "                               activation = 'relu',\n",
    "                               strides = 1,\n",
    "                               padding = 'same'))\n",
    "    \n",
    "    hp_model.add(layers.Conv2D(filters = 1,\n",
    "                               kernel_size = (3, 3),\n",
    "                               activation = 'relu',\n",
    "                               strides = 1,\n",
    "                               padding = 'same'))\n",
    "    \n",
    "    hp_model.add(layers.Conv2D(filters = hp.Int('filters_3',\n",
    "                                                min_value = 15,\n",
    "                                                max_value = 25,\n",
    "                                                step = 2),\n",
    "                               kernel_size = (3, 3),\n",
    "                               activation = 'relu',\n",
    "                               strides = 1,\n",
    "                               padding = 'same'))\n",
    "    \n",
    "    hp_model.add(layers.UpSampling2D((2, 2)))\n",
    "    hp_model.add(layers.Conv2D(filters = hp.Int('filters_4',\n",
    "                                                min_value = 15,\n",
    "                                                max_value = 25,\n",
    "                                                step = 2),\n",
    "                               kernel_size = (3, 3),\n",
    "                               activation = 'relu',\n",
    "                               strides = 1,\n",
    "                               padding = 'same'))\n",
    "    \n",
    "    hp_model.add(layers.Conv2D(filters = 1, kernel_size = (3, 3), activation = 'relu', strides = 1, padding = 'same'))\n",
    "    hp_model.compile(optimizer = 'adam', loss = custom_loss_for_train())\n",
    "    return hp_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hypermodel for shallowly dense (usually a student model for Knowledge Distillation later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hypermodel(hp):\n",
    "    hp_model = tf.keras.Sequential()\n",
    "    hp_model.add(tf.keras.layers.InputLayer(input_shape = (18, 14, 1)))\n",
    "    hp_model.add(tf.keras.layers.Flatten())\n",
    "    hp_model.add(tf.keras.layers.Dense(units = hp.Int('units_1',\n",
    "                                                      min_value = 10,\n",
    "                                                      max_value = 40,\n",
    "                                                      step = 2),\n",
    "                                       activation = 'relu'))\n",
    "    hp_model.add(tf.keras.layers.Dropout(rate = 0.3))\n",
    "    hp_model.add(tf.keras.layers.Dense(1, activation = 'relu'))\n",
    "    hp_model.compile(optimizer = 'adam', loss = 'mse')\n",
    "    return hp_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set configuration for tuner (Hyperband)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = Hyperband(hypermodel,\n",
    "                 objective = 'val_loss',\n",
    "                 max_epochs = 20,\n",
    "                 factor = 3, #number of models to train in a bracket = 1+log_factor(max_epochs)\n",
    "                 hyperband_iterations = 2, #number of times to iterate over the full Hyperband algorithm\n",
    "                 seed = 10,\n",
    "                 directory = 'hypertuning',\n",
    "                 project_name = 'tune',\n",
    "                 overwrite = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Partition the dataset into train/val/test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = ZeroBias\n",
    "\n",
    "train_ratio = 0.7\n",
    "val_ratio = 0.1\n",
    "test_ratio = 1 - train_ratio - val_ratio\n",
    "X_train_val, X_test = train_test_split(X, test_size = test_ratio, random_state = 123)\n",
    "X_train, X_val = train_test_split(X_train_val, test_size = val_ratio/(val_ratio + train_ratio), random_state = 123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the search. Mind the label when training for reconstruction or something else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.search(X_train, X_train,\n",
    "            epochs = 20,\n",
    "            validation_data = (X_val, X_val),\n",
    "            batch_size = 256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the best models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.results_summary(num_trials = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take one of them for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hp = tuner.get_best_hyperparameters()[0]\n",
    "model = tuner.hypermodel.build(best_hp)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conv AE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolutional autoencoder to be trained for input reconstruction (to be used as a teacher model for Knowledge Distillation later).\n",
    "\n",
    "The encoder part, transforming the (18, 14) region input into a smaller latent space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input = tf.keras.Input(shape = (18, 14, 1))\n",
    "#encoding = layers.Conv2D(21, (3, 3), activation = 'relu', strides = 1, padding = 'same')(encoder_input)\n",
    "#encoding = layers.AveragePooling2D((2, 2))(encoding)\n",
    "#encoding = layers.Conv2D(19, (3, 3), activation = 'relu', strides = 1, padding = 'same')(encoding)\n",
    "\n",
    "encoding = layers.Conv2D(20, (3, 3), strides = 1, padding = 'same')(encoder_input)\n",
    "encoding = layers.Activation('relu')(encoding)\n",
    "encoding = layers.AveragePooling2D((2, 2))(encoding)\n",
    "encoding = layers.Conv2D(40, (3, 3), strides = 1, padding = 'same')(encoding)\n",
    "encoding = layers.Activation('relu')(encoding)\n",
    "encoding = layers.Flatten()(encoding)\n",
    "\n",
    "encoder_output = layers.Dense(100, activation = 'relu')(encoding)\n",
    "#encoder_output = layers.Conv2D(1, (3, 3), activation = 'relu', strides = 1, padding = 'same')(encoding)\n",
    "\n",
    "encoder = tf.keras.models.Model(encoder_input, encoder_output)\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decoder part, reconstructing from latent space back to the (18, 14) region input. Note the Conv2DTranspose is not yet supported in hls4ml, but ok to use if it is going to be distilled to another network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#decoding = layers.Conv2D(25, (3, 3), activation = 'relu', strides = 1, padding = 'same')(encoder_output)\n",
    "#decoding = layers.UpSampling2D((2, 2))(decoding)\n",
    "#decoding = layers.Conv2D(25, (3, 3), activation = 'relu', strides = 1, padding = 'same')(decoding)\n",
    "\n",
    "decoding = layers.Dense(9 * 7 * 20)(encoder_output)\n",
    "decoding = layers.Reshape((9, 7, 20))(decoding)\n",
    "decoding = layers.Activation('relu')(decoding)\n",
    "decoding = layers.Conv2D(40, (3, 3), strides = 1, padding = 'same')(decoding)\n",
    "#decoding = layers.Conv2D(25, (3, 3), strides = 1, padding = 'same')(encoder_output)\n",
    "decoding = layers.Activation('relu')(decoding)\n",
    "decoding = layers.UpSampling2D((2, 2))(decoding)\n",
    "decoding = layers.Conv2D(20, (3, 3), strides = 1, padding = 'same')(decoding)\n",
    "decoding = layers.Activation('relu')(decoding)\n",
    "\n",
    "decoder_output = layers.Conv2D(1, (3, 3), activation = 'relu', strides = 1, padding = 'same')(decoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Model(encoder_input, decoder_output)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = 'adam', loss = 'mse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conv VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampling(layers.Layer):\n",
    "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfd = tfp.distributions\n",
    "tfb = tfp.bijectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flow(layers.Layer):\n",
    "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z = inputs\n",
    "        batch = tf.shape(z)[0]\n",
    "        dim = tf.shape(z)[1]\n",
    "        tfp.layers.AutoregressiveTransform(tfb.AutoregressiveNetwork(params=10, hidden_units=[10], activation='relu'))\n",
    "        #return 2*z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 60\n",
    "encoder_inputs = tf.keras.Input(shape = (28, 28, 1))\n",
    "\n",
    "encoding = layers.Conv2D(20, (3, 3), strides = 1, padding = 'same')(encoder_inputs)\n",
    "encoding = layers.Activation('relu')(encoding)\n",
    "encoding = layers.AveragePooling2D((2, 2))(encoding)\n",
    "encoding = layers.Conv2D(20, (3, 3), strides = 1, padding = 'same')(encoding)\n",
    "encoding = layers.Activation('relu')(encoding)\n",
    "encoding = layers.Flatten()(encoding)\n",
    "\n",
    "z_mean = layers.Dense(latent_dim, name = 'z_mean')(encoding)\n",
    "z_log_var = layers.Dense(latent_dim, name = 'z_log_var')(encoding)\n",
    "z = Sampling()([z_mean, z_log_var])\n",
    "\n",
    "encoder = tf.keras.Model(encoder_inputs, [z_mean, z_log_var, z], name = 'encoder')\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_inputs = tf.keras.Input(shape = (latent_dim,))\n",
    "\n",
    "decoding = layers.Dense(14 * 14 * 20)(latent_inputs)\n",
    "decoding = layers.Reshape((14, 14, 20))(decoding)\n",
    "decoding = layers.Activation('relu')(decoding)\n",
    "decoding = layers.Conv2D(20, (3, 3), strides = 1, padding = 'same')(decoding)\n",
    "decoding = layers.Activation('relu')(decoding)\n",
    "decoding = layers.UpSampling2D((2, 2))(decoding)\n",
    "decoding = layers.Conv2D(20, (3, 3), strides = 1, padding = 'same')(decoding)\n",
    "decoding = layers.Activation('relu')(decoding)\n",
    "\n",
    "decoder_outputs = layers.Conv2D(1, (3, 3), activation = 'sigmoid', strides = 1, padding = 'same')(decoding)\n",
    "\n",
    "decoder = tf.keras.Model(latent_inputs, decoder_outputs, name = 'decoder')\n",
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(keras.Model):\n",
    "    def __init__(self, encoder, decoder, **kwargs):\n",
    "        super(VAE, self).__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
    "        self.reconstruction_loss_tracker = keras.metrics.Mean(\n",
    "            name=\"reconstruction_loss\"\n",
    "        )\n",
    "        self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [\n",
    "            self.total_loss_tracker,\n",
    "            self.reconstruction_loss_tracker,\n",
    "            self.kl_loss_tracker,\n",
    "        ]\n",
    "\n",
    "    def train_step(self, data):\n",
    "        with tf.GradientTape() as tape:\n",
    "            z_mean, z_log_var, z = self.encoder(data)\n",
    "            reconstruction = self.decoder(z)\n",
    "            reconstruction_loss = tf.reduce_mean(tf.reduce_sum(keras.losses.binary_crossentropy(data, reconstruction), axis = (1, 2)))\n",
    "            #reconstruction_loss = tf.reduce_mean(tf.square(data - reconstruction), axis = (1, 2, 3))\n",
    "            kl_loss = tf.reduce_mean(tf.reduce_sum(-0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var)), axis = 1))\n",
    "            total_loss = reconstruction_loss + kl_loss\n",
    "            #total_loss = reconstruction_loss\n",
    "            #total_loss = kl_loss\n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "        return {\n",
    "            \"loss\": self.total_loss_tracker.result(),\n",
    "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
    "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = VAE(encoder, decoder)\n",
    "vae.compile(optimizer = 'adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, _), (x_test, _) = keras.datasets.mnist.load_data()\n",
    "mnist_digits = np.concatenate([x_train, x_test], axis=0)\n",
    "mnist_digits = np.expand_dims(mnist_digits, -1).astype(\"float32\") / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, _), (x_test, _) = keras.datasets.fashion_mnist.load_data()\n",
    "mnist_digits = np.concatenate([x_train, x_test], axis=0)\n",
    "mnist_digits = np.expand_dims(mnist_digits, -1).astype(\"float32\") / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = vae.fit(mnist_digits, epochs = 30,\n",
    "                    #validation_data = X_val,\n",
    "                  batch_size = 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15,10))\n",
    "axes_left = plt.subplot(2, 2, 1)\n",
    "axes_left.plot(history.history['loss'], label = 'Total loss = Reco + KL', c = 'g', linestyle = 'solid')\n",
    "axes_left.plot(history.history['reconstruction_loss'], label = 'Reconstruction loss (BCE)', c = 'g' , linestyle = 'dashed')\n",
    "axes_left.legend(loc = \"upper left\")\n",
    "axes_left.set_xlabel('Epoch')\n",
    "axes_left.set_ylabel('Total loss', c = 'g')\n",
    "\n",
    "axes_right = axes_left.twinx()\n",
    "axes_right.plot(history.history['kl_loss'], label = 'KL loss', c = 'r', linestyle = 'dashed')\n",
    "axes_right.legend(loc = \"upper right\")\n",
    "axes_right.set_ylabel('KL loss', c = 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_label_clusters(vae, data, labels):\n",
    "    # display a 2D plot of the digit classes in the latent space\n",
    "    z_mean, _, _ = vae.encoder.predict(data)\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    plt.scatter(z_mean[:, 0], z_mean[:, 1], c=labels)\n",
    "    #plt.scatter(tf.exp(z_mean[:, 0]/2), tf.exp(z_mean[:, 1]/2), c=labels)\n",
    "    plt.colorbar()\n",
    "    plt.xlabel(\"z[0]\")\n",
    "    plt.ylabel(\"z[1]\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "#(x_train, y_train), _ = keras.datasets.mnist.load_data()\n",
    "(x_train, y_train), _ = keras.datasets.fashion_mnist.load_data()\n",
    "x_train = np.expand_dims(x_train, -1).astype(\"float32\") / 255\n",
    "\n",
    "plot_label_clusters(vae, x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, mnist_digits_predict = vae.encoder.predict(mnist_digits)\n",
    "mnist_digits_predict = vae.decoder.predict(mnist_digits_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(50,80):\n",
    "    fig, ax = plt.subplots(figsize = (5,5))\n",
    "    ax = plt.subplot(2, 2, 1)\n",
    "    ax = sns.heatmap(mnist_digits[i,:,:,0].reshape(28, 28), vmin = 0, vmax = 1, cmap = \"Reds\")\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    ax.set_title('Input')\n",
    "    \n",
    "    ax = plt.subplot(2, 2, 2)\n",
    "    ax = sns.heatmap(mnist_digits_predict[i,:,:,0].reshape(28, 28), vmin = 0, vmax = 1, cmap = \"Reds\")\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    ax.set_title('Output')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//////////@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampling(layers.Layer):\n",
    "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 60\n",
    "encoder_inputs = tf.keras.Input(shape = (18, 14, 1))\n",
    "\n",
    "encoding = layers.Conv2D(20, (3, 3), strides = 1, padding = 'same')(encoder_inputs)\n",
    "encoding = layers.Activation('relu')(encoding)\n",
    "encoding = layers.AveragePooling2D((2, 2))(encoding)\n",
    "encoding = layers.Conv2D(20, (3, 3), strides = 1, padding = 'same')(encoding)\n",
    "encoding = layers.Activation('relu')(encoding)\n",
    "encoding = layers.Flatten()(encoding)\n",
    "\n",
    "z_mean = layers.Dense(latent_dim, name = 'z_mean')(encoding)\n",
    "z_log_var = layers.Dense(latent_dim, name = 'z_log_var')(encoding)\n",
    "z = Sampling()([z_mean, z_log_var])\n",
    "\n",
    "encoder = tf.keras.Model(encoder_inputs, [z_mean, z_log_var, z], name = 'encoder')\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_inputs = tf.keras.Input(shape = (latent_dim,))\n",
    "\n",
    "decoding = layers.Dense(9 * 7 * 20)(latent_inputs)\n",
    "decoding = layers.Reshape((9, 7, 20))(decoding)\n",
    "decoding = layers.Activation('relu')(decoding)\n",
    "decoding = layers.Conv2D(20, (3, 3), strides = 1, padding = 'same')(decoding)\n",
    "decoding = layers.Activation('relu')(decoding)\n",
    "decoding = layers.UpSampling2D((2, 2))(decoding)\n",
    "decoding = layers.Conv2D(20, (3, 3), strides = 1, padding = 'same')(decoding)\n",
    "decoding = layers.Activation('relu')(decoding)\n",
    "\n",
    "decoder_outputs = layers.Conv2D(1, (3, 3), activation = 'sigmoid', strides = 1, padding = 'same')(decoding)\n",
    "\n",
    "decoder = tf.keras.Model(latent_inputs, decoder_outputs, name = 'decoder')\n",
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(keras.Model):\n",
    "    def __init__(self, encoder, decoder, **kwargs):\n",
    "        super(VAE, self).__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
    "        self.reconstruction_loss_tracker = keras.metrics.Mean(\n",
    "            name=\"reconstruction_loss\"\n",
    "        )\n",
    "        self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [\n",
    "            self.total_loss_tracker,\n",
    "            self.reconstruction_loss_tracker,\n",
    "            self.kl_loss_tracker,\n",
    "        ]\n",
    "\n",
    "    def train_step(self, data):\n",
    "        with tf.GradientTape() as tape:\n",
    "            z_mean, z_log_var, z = self.encoder(data)\n",
    "            reconstruction = self.decoder(z)\n",
    "            reconstruction_loss = 10*tf.reduce_mean(tf.reduce_sum(keras.losses.binary_crossentropy(data, reconstruction), axis = (1, 2)))\n",
    "            #reconstruction_loss = 100*tf.reduce_mean(tf.square(data - reconstruction), axis = (1, 2, 3))\n",
    "            kl_loss = tf.reduce_mean(tf.reduce_sum(-0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var)), axis = 1))\n",
    "            total_loss = reconstruction_loss + kl_loss\n",
    "            #total_loss = reconstruction_loss\n",
    "            #total_loss = kl_loss\n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "        return {\n",
    "            \"loss\": self.total_loss_tracker.result(),\n",
    "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
    "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = VAE(encoder, decoder)\n",
    "vae.compile(optimizer = 'adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "et_scale = 80\n",
    "history = vae.fit(X_train/et_scale, epochs = 25,\n",
    "                  #validation_data = (X_val/et_scale,X_val/et_scale),\n",
    "                  batch_size = 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15,10))\n",
    "axes_left = plt.subplot(2, 2, 1)\n",
    "axes_left.plot(history.history['loss'], label = 'Total loss = Reco + KL', c = 'g', linestyle = 'solid')\n",
    "axes_left.plot(history.history['reconstruction_loss'], label = 'Reconstruction loss (BCE)', c = 'g' , linestyle = 'dashed')\n",
    "axes_left.legend(loc = \"upper left\")\n",
    "axes_left.set_xlabel('Epoch')\n",
    "axes_left.set_ylabel('Total loss', c = 'g')\n",
    "\n",
    "axes_right = axes_left.twinx()\n",
    "axes_right.plot(history.history['kl_loss'], label = 'KL loss', c = 'r', linestyle = 'dashed')\n",
    "axes_right.legend(loc = \"upper right\")\n",
    "axes_right.set_ylabel('KL loss', c = 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_mu, X_test_logvar, X_test_z = vae.encoder.predict(X_test/et_scale)\n",
    "X_test_predict = vae.decoder.predict(X_test_z)\n",
    "\n",
    "MC_zb_mu = []\n",
    "MC_zb_logvar = []\n",
    "MC_zb_z = []\n",
    "MC_zb_predict = []\n",
    "for i in range(len(MC_zb)):\n",
    "    mu, logvar, z = vae.encoder.predict(MC_zb[i]/et_scale)\n",
    "    predict = vae.decoder.predict(z)\n",
    "    MC_zb_mu.append(mu)\n",
    "    MC_zb_logvar.append(logvar)\n",
    "    MC_zb_z.append(z)\n",
    "    MC_zb_predict.append(predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "i = 8\n",
    "j = 9\n",
    "n = 5\n",
    "\n",
    "df_z_mu_sig = pd.DataFrame(MC_zb_mu[n][1:9000, i], columns = [\"z_mu[{}]\".format(i)])\n",
    "df_z_mu_sig[\"z_mu[{}]\".format(j)] = MC_zb_mu[n][1:9000, j]\n",
    "df_z_mu_sig[\"dataset\"] = MC_files[n]\n",
    "df_z_mu_zb = pd.DataFrame(X_test_mu[1:9000, i], columns = [\"z_mu[{}]\".format(i)])\n",
    "df_z_mu_zb[\"z_mu[{}]\".format(j)] = X_test_mu[1:9000, j]\n",
    "df_z_mu_zb[\"dataset\"] = \"test (ZB)\"\n",
    "df_z_mu = pd.concat([df_z_mu_sig,df_z_mu_zb])\n",
    "\n",
    "df_z_sigma_sig = pd.DataFrame(np.exp(MC_zb_logvar[n][1:9000, i]/2), columns = [\"z_sigma[{}]\".format(i)])\n",
    "df_z_sigma_sig[\"z_sigma[{}]\".format(j)] = np.exp(MC_zb_logvar[n][1:9000, j]/2)\n",
    "df_z_sigma_sig[\"dataset\"] = MC_files[n]\n",
    "df_z_sigma_zb = pd.DataFrame(np.exp(X_test_logvar[1:9000, i]/2), columns = [\"z_sigma[{}]\".format(i)])\n",
    "df_z_sigma_zb[\"z_sigma[{}]\".format(j)] = np.exp(X_test_logvar[1:9000, j]/2)\n",
    "df_z_sigma_zb[\"dataset\"] = \"test (ZB)\"\n",
    "df_z_sigma = pd.concat([df_z_sigma_sig,df_z_sigma_zb])\n",
    "\n",
    "df_z_sig = pd.DataFrame(np.exp(MC_zb_z[n][1:9000, i]/2), columns = [\"z[{}]\".format(i)])\n",
    "df_z_sig[\"z[{}]\".format(j)] = np.exp(MC_zb_z[n][1:9000, j]/2)\n",
    "df_z_sig[\"dataset\"] = MC_files[n]\n",
    "df_z_zb = pd.DataFrame(np.exp(X_test_z[1:9000, i]/2), columns = [\"z[{}]\".format(i)])\n",
    "df_z_zb[\"z[{}]\".format(j)] = np.exp(X_test_z[1:9000, j]/2)\n",
    "df_z_zb[\"dataset\"] = \"test (ZB)\"\n",
    "df_z = pd.concat([df_z_sig,df_z_zb])\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "sns.jointplot(x = df_z_mu[\"z_mu[{}]\".format(i)], y = df_z_mu[\"z_mu[{}]\".format(j)], hue = df_z_mu[\"dataset\"], height = 8, ratio = 5,\n",
    "              #xlim = (-0.001, 0.001), ylim = (-0.001, 0.001),\n",
    "              marker = '.', alpha = 1)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "sns.jointplot(x = df_z_sigma[\"z_sigma[{}]\".format(i)], y = df_z_sigma[\"z_sigma[{}]\".format(j)], hue = df_z_sigma[\"dataset\"], height = 8, ratio = 5,\n",
    "              #xlim = (-8, 8), ylim = (-8, 8),\n",
    "              marker = '.', alpha = 1)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "sns.jointplot(x = df_z[\"z[{}]\".format(i)], y = df_z[\"z[{}]\".format(j)], hue = df_z[\"dataset\"], height = 8, ratio = 5,\n",
    "              #xlim = (-8, 8), ylim = (-8, 8),\n",
    "              marker = '.', alpha = 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss_for_pred(y_true, y_pred, choice):\n",
    "    #MSE\n",
    "    if choice == 0:\n",
    "        loss = np.mean((y_true - y_pred)**2, axis = (1, 2, 3))\n",
    "        return loss\n",
    "    \n",
    "    #MSE for de-noising model\n",
    "    if choice == 1:\n",
    "        loss = np.mean(y_pred**2, axis = (1, 2, 3))\n",
    "        return loss\n",
    "    \n",
    "    #VAE radius loss\n",
    "    if choice == 2:\n",
    "        loss = np.sqrt(np.sum(y_pred**2, axis = 1))\n",
    "        return loss\n",
    "    \n",
    "    #VAE KL loss\n",
    "    if choice == 3:\n",
    "        loss = -0.5 * np.sum(1.0 + y_pred - y_true**2 - np.exp(y_pred), axis = 1)\n",
    "        return loss\n",
    "    \n",
    "    #BCE loss\n",
    "    if choice == 4:\n",
    "        #loss = np.mean(np.sum(keras.losses.binary_crossentropy(y_true, y_pred), axis = (1, 2)))\n",
    "        loss = np.sum(keras.losses.binary_crossentropy(y_true, y_pred), axis = (1, 2))\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For VAE\n",
    "X_test_vaeloss_mse = custom_loss_for_pred(X_test/et_scale, X_test_predict, 0)\n",
    "MC_zb_vaeloss_mse = []\n",
    "for i in range(len(MC_zb)):\n",
    "    MC_zb_vaeloss_mse.append(custom_loss_for_pred(MC_zb[i]/et_scale, MC_zb_predict[i], 0))\n",
    "    \n",
    "X_test_vaeloss_bce = custom_loss_for_pred(X_test/et_scale, X_test_predict, 4)\n",
    "MC_zb_vaeloss_bce = []\n",
    "for i in range(len(MC_zb)):\n",
    "    MC_zb_vaeloss_bce.append(custom_loss_for_pred(MC_zb[i]/et_scale, MC_zb_predict[i], 4))\n",
    "\n",
    "X_test_vaeloss_radius = custom_loss_for_pred(X_test, X_test_mu, 2)\n",
    "MC_zb_vaeloss_radius = []\n",
    "for i in range(len(MC_zb)):\n",
    "    MC_zb_vaeloss_radius.append(custom_loss_for_pred(MC_zb[i], MC_zb_mu[i], 2))\n",
    "    \n",
    "X_test_vaeloss_kl = custom_loss_for_pred(X_test_mu, X_test_logvar, 3)\n",
    "MC_zb_vaeloss_kl = []\n",
    "for i in range(len(MC_zb)):\n",
    "    MC_zb_vaeloss_kl.append(custom_loss_for_pred(MC_zb_mu[i], MC_zb_logvar[i], 3))\n",
    "\n",
    "weight_mse = 0\n",
    "weight_bce = 0.5\n",
    "weight_radius = 0\n",
    "weight_kl = 0.5\n",
    "\n",
    "X_test_vaeloss = weight_mse*X_test_vaeloss_mse + weight_bce*X_test_vaeloss_bce + weight_radius*X_test_vaeloss_radius + weight_kl*X_test_vaeloss_kl\n",
    "MC_zb_vaeloss = []\n",
    "for i in range(len(MC_zb)):\n",
    "    MC_zb_vaeloss.append(weight_mse*MC_zb_vaeloss_mse[i] + weight_bce*MC_zb_vaeloss_bce[i] + weight_radius*MC_zb_vaeloss_radius[i] + weight_kl*MC_zb_vaeloss_kl[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Original vs Reconstructed\n",
    "#show_ZB = True\n",
    "show_ZB = False\n",
    "n = 5\n",
    "for i in range(580,590):\n",
    "    fig, ax = plt.subplots(figsize = (17,17))\n",
    "    if show_ZB == True:\n",
    "        print('ZB test\\nloss = ' + str(X_test_vaeloss[i]))\n",
    "    else:\n",
    "        print(str(MC_files[n]) + '\\nloss = ' + str(MC_zb_vaeloss[n][i]))\n",
    "    ax = plt.subplot(3, 3, 1)\n",
    "    if show_ZB == True:\n",
    "        ax = sns.heatmap(X_test[i,:,:,0].reshape(18, 14)/et_scale, vmin = 0, vmax = X_test[i,:,:,0].max()/et_scale, cmap = \"Blues\", cbar_kws = {'label': 'Scaled ET'})\n",
    "    else:\n",
    "        ax = sns.heatmap(MC_zb[n][i,:,:,0].reshape(18, 14)/et_scale, vmin = 0, vmax = MC_zb[n][i,:,:,0].max()/et_scale, cmap = \"Blues\", cbar_kws = {'label': 'Scaled ET'})\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    ax.set_title('Original')\n",
    "    \n",
    "    ax = plt.subplot(3, 3, 2)\n",
    "    if show_ZB == True:\n",
    "        ax = sns.heatmap(X_test_predict[i,:,:,0].reshape(18, 14), vmin = 0, vmax = X_test[i,:,:,0].max()/et_scale, cmap = \"Blues\", cbar_kws = {'label': 'Scaled ET'})\n",
    "    else:\n",
    "        ax = sns.heatmap(MC_zb_predict[n][i,:,:,0].reshape(18, 14), vmin = 0, vmax = MC_zb[n][i,:,:,0].max()/et_scale, cmap = \"Blues\", cbar_kws = {'label': 'Scaled ET'})\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    ax.set_title('Reconstructed')\n",
    "    \n",
    "    ax = plt.subplot(3, 3, 3)\n",
    "    if show_ZB == True:\n",
    "        ax = sns.heatmap(np.absolute(X_test_predict[i,:,:,0] - X_test[i,:,:,0]/et_scale).reshape(18, 14), vmin = 0, vmax = X_test[i,:,:,0].max()/et_scale, cmap = \"Blues\", cbar_kws = {'label': 'Scaled ET'})\n",
    "    else:\n",
    "        ax = sns.heatmap(np.absolute(MC_zb_predict[n][i,:,:,0] - MC_zb[n][i,:,:,0]/et_scale).reshape(18, 14), vmin = 0, vmax = MC_zb[n][i,:,:,0].max()/et_scale, cmap = \"Blues\", cbar_kws = {'label': 'Scaled ET'})\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    ax.set_title('abs(original-reconstructed)')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nbins = 20\n",
    "rmin = 0\n",
    "rmax = 0.05\n",
    "plt.hist(X_test_vaeloss_mse, density = 1, bins = nbins, alpha = 0.3, label = 'test (ZeroBias)', range = (rmin, rmax), log = True)\n",
    "plt.hist(MC_zb_vaeloss_mse[0], density = 1, bins = nbins, label = 'QCD', alpha = 0.1, histtype = 'stepfilled', range = (rmin, rmax))\n",
    "for i in range(3,7):\n",
    "    plt.hist(MC_zb_vaeloss_mse[i], density = 1, bins = nbins, label = MC_files[i], histtype = 'step', range = (rmin, rmax))\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "plt.xlabel(\"Reconstruction MSE loss\")\n",
    "plt.show()\n",
    "\n",
    "nbins = 20\n",
    "rmin = 0\n",
    "rmax = 140\n",
    "plt.hist(X_test_vaeloss_bce, density = 1, bins = nbins, alpha = 0.3, label = 'test (ZeroBias)', range = (rmin, rmax), log = True)\n",
    "plt.hist(MC_zb_vaeloss_bce[0], density = 1, bins = nbins, label = 'QCD', alpha = 0.1, histtype = 'stepfilled', range = (rmin, rmax))\n",
    "for i in range(3,7):\n",
    "    plt.hist(MC_zb_vaeloss_bce[i], density = 1, bins = nbins, label = MC_files[i], histtype = 'step', range = (rmin, rmax))\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "plt.xlabel(\"Reconstruction BCE loss\")\n",
    "plt.show()\n",
    "\n",
    "nbins = 20\n",
    "rmin = 0\n",
    "rmax = 20\n",
    "plt.hist(X_test_vaeloss_radius, density = 1, bins = nbins, alpha = 0.3, label = 'test (ZeroBias)', range = (rmin, rmax), log = True)\n",
    "plt.hist(MC_zb_vaeloss_radius[0], density = 1, bins = nbins, label = 'QCD', alpha = 0.1, histtype = 'stepfilled', range = (rmin, rmax))\n",
    "for i in range(3,7):\n",
    "    plt.hist(MC_zb_vaeloss_radius[i], density = 1, bins = nbins, label = MC_files[i], histtype = 'step', range = (rmin, rmax))\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "plt.xlabel(\"Radius loss\")\n",
    "plt.show()\n",
    "\n",
    "nbins = 20\n",
    "rmin = 0\n",
    "rmax = 20\n",
    "plt.hist(X_test_vaeloss_kl, density = 1, bins = nbins, alpha = 0.3, label = 'test (ZeroBias)', range = (rmin, rmax), log = True)\n",
    "plt.hist(MC_zb_vaeloss_kl[0], density = 1, bins = nbins, label = 'QCD', alpha = 0.1, histtype = 'stepfilled', range = (rmin, rmax))\n",
    "for i in range(3,7):\n",
    "    plt.hist(MC_zb_vaeloss_kl[i], density = 1, bins = nbins, label = MC_files[i], histtype = 'step', range = (rmin, rmax))\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "plt.xlabel(\"KL loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n = 5\n",
    "\n",
    "df_vaeloss_sig = pd.DataFrame(MC_zb_vaeloss_mse[n][1:9000], columns = [\"MSE loss\"])\n",
    "df_vaeloss_sig[\"BCE loss\"] = MC_zb_vaeloss_bce[n][1:9000]\n",
    "df_vaeloss_sig[\"Radius loss\"] = MC_zb_vaeloss_radius[n][1:9000]\n",
    "df_vaeloss_sig[\"KL loss\"] = MC_zb_vaeloss_kl[n][1:9000]\n",
    "df_vaeloss_sig[\"dataset\"] = MC_files[n]\n",
    "df_vaeloss_zb = pd.DataFrame(X_test_vaeloss_mse[1:9000], columns = [\"MSE loss\"])\n",
    "df_vaeloss_zb[\"BCE loss\"] = X_test_vaeloss_bce[1:9000]\n",
    "df_vaeloss_zb[\"Radius loss\"] = X_test_vaeloss_radius[1:9000]\n",
    "df_vaeloss_zb[\"KL loss\"] = X_test_vaeloss_kl[1:9000]\n",
    "df_vaeloss_zb[\"dataset\"] = \"test (ZB)\"\n",
    "\n",
    "df_vaeloss = pd.concat([df_vaeloss_sig,df_vaeloss_zb])\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "sns.jointplot(x = df_vaeloss[\"MSE loss\"], y = df_vaeloss[\"BCE loss\"], height = 8, ratio = 5, hue = df_vaeloss[\"dataset\"],\n",
    "              xlim = (0.0, 0.05), ylim = (0, 100),\n",
    "              marker = '.', alpha = 1)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "sns.jointplot(x = df_vaeloss[\"Radius loss\"], y = df_vaeloss[\"KL loss\"], height = 8, ratio = 5, hue = df_vaeloss[\"dataset\"],\n",
    "              xlim = (0, 10), ylim = (0, 50),\n",
    "              marker = '.', alpha = 1)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "sns.jointplot(x = df_vaeloss[\"MSE loss\"], y = df_vaeloss[\"KL loss\"], height = 8, ratio = 5, hue = df_vaeloss[\"dataset\"],\n",
    "              xlim = (0, 0.05), ylim = (0, 50),\n",
    "              marker = '.', alpha = 1)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "sns.jointplot(x = df_vaeloss[\"BCE loss\"], y = df_vaeloss[\"KL loss\"], height = 8, ratio = 5 ,hue = df_vaeloss[\"dataset\"],\n",
    "              xlim = (0, 100), ylim = (0, 40),\n",
    "              marker = '.', alpha = 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ZeroBias_mean = np.mean(ZeroBias, axis = 0)\n",
    "\n",
    "baseline_zb = np.mean((X_test - ZeroBias_mean)**2, axis = (1, 2))\n",
    "baseline_mc = []\n",
    "for i in range(len(MC_zb)):\n",
    "    baseline_mc.append(np.mean((MC_zb[i] - ZeroBias_mean)**2, axis = (1, 2)))\n",
    "\n",
    "Y_zb = np.zeros((X_test.shape[0], 1))\n",
    "Y_mc = []\n",
    "for i in range(len(MC)):\n",
    "    Y_mc.append(np.ones((MC_zb[i].shape[0], 1)))\n",
    "\n",
    "Y_true = []\n",
    "Y_baseline = []\n",
    "Y_mse = []\n",
    "Y_bce = []\n",
    "Y_radius = []\n",
    "Y_kl = []\n",
    "Y_total = []\n",
    "for i in range(len(MC)):\n",
    "    Y_true.append(np.concatenate((Y_mc[i], Y_zb)))\n",
    "    Y_baseline.append(np.concatenate((baseline_mc[i], baseline_zb)))\n",
    "    Y_mse.append(np.concatenate((MC_zb_vaeloss_mse[i], X_test_vaeloss_mse)))\n",
    "    Y_bce.append(np.concatenate((MC_zb_vaeloss_bce[i], X_test_vaeloss_bce)))\n",
    "    Y_radius.append(np.concatenate((MC_zb_vaeloss_radius[i], X_test_vaeloss_radius)))\n",
    "    Y_kl.append(np.concatenate((MC_zb_vaeloss_kl[i], X_test_vaeloss_kl)))\n",
    "    Y_total.append(np.concatenate((MC_zb_vaeloss[i], X_test_vaeloss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 5\n",
    "\n",
    "plt.figure(figsize = (13, 13))\n",
    "axes = plt.subplot(2, 2, 1)\n",
    "\n",
    "fpr_baseline = np.empty((Y_true[n].shape[0],1))\n",
    "tpr_baseline = np.empty((Y_true[n].shape[0],1))\n",
    "thresholds_baseline = np.empty((Y_true[n].shape[0],1))\n",
    "roc_auc_baseline = np.empty((Y_true[n].shape[0],1))\n",
    "\n",
    "fpr_mse = np.empty((Y_true[n].shape[0],1))\n",
    "tpr_mse = np.empty((Y_true[n].shape[0],1))\n",
    "thresholds_mse = np.empty((Y_true[n].shape[0],1))\n",
    "roc_auc_mse = np.empty((Y_true[n].shape[0],1))\n",
    "\n",
    "fpr_bce = np.empty((Y_true[n].shape[0],1))\n",
    "tpr_bce = np.empty((Y_true[n].shape[0],1))\n",
    "thresholds_bce = np.empty((Y_true[n].shape[0],1))\n",
    "roc_auc_bce = np.empty((Y_true[n].shape[0],1))\n",
    "\n",
    "fpr_radius = np.empty((Y_true[n].shape[0],1))\n",
    "tpr_radius = np.empty((Y_true[n].shape[0],1))\n",
    "thresholds_radius = np.empty((Y_true[n].shape[0],1))\n",
    "roc_auc_radius = np.empty((Y_true[n].shape[0],1))\n",
    "\n",
    "fpr_kl = np.empty((Y_true[n].shape[0],1))\n",
    "tpr_kl = np.empty((Y_true[n].shape[0],1))\n",
    "thresholds_kl = np.empty((Y_true[n].shape[0],1))\n",
    "roc_auc_kl = np.empty((Y_true[n].shape[0],1))\n",
    "\n",
    "fpr_total = np.empty((Y_true[n].shape[0],1))\n",
    "tpr_total = np.empty((Y_true[n].shape[0],1))\n",
    "thresholds_total = np.empty((Y_true[n].shape[0],1))\n",
    "roc_auc_total = np.empty((Y_true[n].shape[0],1))\n",
    "\n",
    "fpr_baseline, tpr_baseline, thresholds_baseline = roc_curve(Y_true[n], Y_baseline[n])\n",
    "roc_auc_baseline = auc(fpr_baseline, tpr_baseline)\n",
    "\n",
    "fpr_mse, tpr_mse, thresholds_mse = roc_curve(Y_true[n], Y_mse[n])\n",
    "roc_auc_mse = auc(fpr_mse, tpr_mse)\n",
    "\n",
    "fpr_bce, tpr_bce, thresholds_bce = roc_curve(Y_true[n], Y_bce[n])\n",
    "roc_auc_bce = auc(fpr_bce, tpr_bce)\n",
    "\n",
    "fpr_radius, tpr_radius, thresholds_radius = roc_curve(Y_true[n], Y_radius[n])\n",
    "roc_auc_radius = auc(fpr_radius, tpr_radius)\n",
    "\n",
    "fpr_kl, tpr_kl, thresholds_kl = roc_curve(Y_true[n], Y_kl[n])\n",
    "roc_auc_kl = auc(fpr_kl, tpr_kl)\n",
    "\n",
    "fpr_total, tpr_total, thresholds_total = roc_curve(Y_true[n], Y_total[n])\n",
    "roc_auc_total = auc(fpr_total, tpr_total)\n",
    "\n",
    "lw = 2\n",
    "\n",
    "axes.plot(fpr_baseline, tpr_baseline, linestyle = '--', lw = lw, color = 'red', label = 'Cut-flow baseline (AUC = %.4f)' % (roc_auc_baseline))\n",
    "axes.plot(fpr_mse, tpr_mse, linestyle = '-', lw = lw, label = 'VAE anomaly score = MSE loss (AUC = %.4f)' % (roc_auc_mse))\n",
    "axes.plot(fpr_bce, tpr_bce, linestyle = '-', lw = lw, label = 'VAE anomaly score = BCE loss (AUC = %.4f)' % (roc_auc_bce))\n",
    "axes.plot(fpr_radius, tpr_radius, linestyle = '-', lw = lw, label = 'VAE anomaly score = radius loss (AUC = %.4f)' % (roc_auc_radius))\n",
    "axes.plot(fpr_kl, tpr_kl, linestyle = '-', lw = lw, label = 'VAE anomaly score = KL loss (AUC = %.4f)' % (roc_auc_kl))\n",
    "axes.plot(fpr_total, tpr_total, linestyle = '--', lw = lw, label = 'VAE anomaly score = BCE + KL (AUC = %.4f)' % (roc_auc_total))\n",
    "\n",
    "axes.plot([0.002, 0.002], [0, 1], linestyle = '--', lw = 1, color = 'black', label = 'FPR = 0.2% ~ (100 kHz)/(ZB rate)')\n",
    "axes.set_xlim([0.00001, 1.0])\n",
    "#axes.set_xlim([0, 1.0])\n",
    "axes.set_ylim([0, 1.0])\n",
    "#axes.set_ylim([0.9, 1.0])\n",
    "axes.set_xscale(value = \"log\")\n",
    "#axes.set_yscale(value = \"log\")\n",
    "axes.set_xlabel('False Positive Rate (FPR)')\n",
    "axes.set_ylabel('True Positive Rate (TPR)')\n",
    "axes.set_title(MC_files[n] + ' vs ZB')\n",
    "axes.legend(loc='center left', bbox_to_anchor = (0.6, 0.5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dense model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fully connected dense model to be trained for input reconstruction (less useful since region correlation is lost to some degree in the Flatten layer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.InputLayer(input_shape = (18, 14, 1)))\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dense(20, activation = 'relu'))\n",
    "model.add(tf.keras.layers.Dense(20, activation = 'relu'))\n",
    "model.add(tf.keras.layers.Dense(10, activation = 'relu'))\n",
    "model.add(tf.keras.layers.Dense(20 , activation = 'relu'))\n",
    "model.add(tf.keras.layers.Dense(20 , activation = 'relu'))\n",
    "model.add(tf.keras.layers.Dense(252 , activation = 'sigmoid'))\n",
    "model.add(tf.keras.layers.Reshape((18, 14, 1)))\n",
    "model.summary()\n",
    "model.compile(optimizer = 'adam', loss = 'mse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# De-noising model (De-ZeroBias model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolutional autoencoder to be trained for ZB pattern removal. Experimental and for fun only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input = tf.keras.Input(shape = (18, 14, 1))\n",
    "encoding = layers.Conv2D(20, (3, 3), activation = 'relu', strides = 1, padding = 'same')(encoder_input)\n",
    "#encoding = layers.AveragePooling2D((2, 2))(encoding)\n",
    "encoder_output = layers.Conv2D(20, (3, 3), activation = 'relu', strides = 1, padding = 'same')(encoding)\n",
    "\n",
    "#encoder_output = layers.Conv2D(2, (3, 3), activation = 'relu', strides = 1, padding = 'same')(encoding)\n",
    "\n",
    "encoder = tf.keras.models.Model(encoder_input, encoder_output)\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoding = layers.Conv2D(20, (3, 3), activation = 'relu', strides = 1, padding = 'same')(encoder_output)\n",
    "#decoding = layers.UpSampling2D((2, 2))(decoding)\n",
    "decoding = layers.Conv2D(20, (3, 3), activation = 'relu', strides = 1, padding = 'same')(decoding)\n",
    "\n",
    "decoder_output = layers.Conv2D(1, (3, 3), activation = 'relu', strides = 1, padding = 'same')(decoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Model(encoder_input, decoder_output)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = 'adam', loss = 'binary_crossentropy')\n",
    "#model.compile(optimizer = 'adam', loss = 'mse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Partition the dataset into train/val/test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = ZeroBias\n",
    "\n",
    "train_ratio = 0.8\n",
    "val_ratio = 0.10\n",
    "test_ratio = 1 - train_ratio - val_ratio\n",
    "X_train_val, X_test = train_test_split(X, test_size = test_ratio, random_state = 1234)\n",
    "X_train, X_val = train_test_split(X_train_val, test_size = val_ratio/(val_ratio + train_ratio), random_state = 1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!!! Only for de-noising model. Preparation of noisy and clean training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "RandomTowers = np.zeros((ZeroBias.shape[0], 18, 14, 1))\n",
    "\n",
    "random_phi1 = np.random.randint(low = 1, high = 17, size = np.int32(RandomTowers.shape[0]/2))\n",
    "random_eta1 = np.random.randint(low = 1, high = 13, size = np.int32(RandomTowers.shape[0]/2))\n",
    "random_pt1 = np.random.randint(low = 20, high = 100, size = np.int32(RandomTowers.shape[0]/2))\n",
    "random_phi2 = np.random.randint(low = 1, high = 17, size = np.int32(RandomTowers.shape[0]/2))\n",
    "random_eta2 = np.random.randint(low = 1, high = 13, size = np.int32(RandomTowers.shape[0]/2))\n",
    "random_pt2 = np.random.randint(low = 20, high = 100, size = np.int32(RandomTowers.shape[0]/2))\n",
    "\n",
    "for i in range(np.int32(RandomTowers.shape[0]/2)):\n",
    "    RandomTowers[i, random_phi1[i], random_eta1[i], 0] = random_pt1[i]/1023\n",
    "    RandomTowers[i, random_phi1[i]+1, random_eta1[i]+1, 0] = random_pt2[i]/1023\n",
    "    RandomTowers[i, random_phi2[i], random_eta2[i], 0] = random_pt2[i]/1023\n",
    "    RandomTowers[i, random_phi2[i]+1, random_eta2[i], 0] = random_pt1[i]/1023\n",
    "\n",
    "Xnoise = 3*X+RandomTowers\n",
    "Xclean = X+RandomTowers\n",
    "\n",
    "train_ratio = 0.8\n",
    "val_ratio = 0.10\n",
    "test_ratio = 1 - train_ratio - val_ratio\n",
    "\n",
    "Xnoise_train_val, Xnoise_test = train_test_split(Xnoise, test_size = test_ratio, random_state = 123)\n",
    "Xnoise_train, Xnoise_val = train_test_split(Xnoise_train_val, test_size = val_ratio/(val_ratio + train_ratio), random_state = 123)\n",
    "\n",
    "Xclean_train_val, Xclean_test = train_test_split(Xclean, test_size = test_ratio, random_state = 123)\n",
    "Xclean_train, Xclean_val = train_test_split(Xclean_train_val, test_size = val_ratio/(val_ratio + train_ratio), random_state = 123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!!! Only for de-noising model. Plot and compare the noisy and clean training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(230,240):\n",
    "    fig, ax = plt.subplots(figsize = (10,10))\n",
    "    ax = plt.subplot(2, 2, 1)\n",
    "    ax = sns.heatmap(Xnoise_train[i,:,:,0].reshape(18, 14), vmin = 0, vmax = Xnoise_train[i,:,:,0].max(), cmap = \"Reds\", cbar_kws = {'label': 'Normalized ET'})\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    ax.set_title('X_noise')\n",
    "    \n",
    "    ax = plt.subplot(2, 2, 2)\n",
    "    ax = sns.heatmap(Xclean_train[i,:,:,0].reshape(18, 14), vmin = 0, vmax = Xnoise_train[i,:,:,0].max(), cmap = \"Reds\", cbar_kws = {'label': 'Normalized ET'})\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    ax.set_title('X_clean')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the training. Mind the label when training for reconstruction or something else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, X_train,\n",
    "                    epochs = 3,\n",
    "                    validation_data = (X_val, X_val),\n",
    "                    batch_size = 1024,\n",
    "                    callbacks = [\n",
    "                        tf.keras.callbacks.EarlyStopping(monitor = \"val_loss\", patience = 10, mode = \"min\")\n",
    "                    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot loss vs epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15,10))\n",
    "axes = plt.subplot(2, 2, 1)\n",
    "axes.plot(history.history['loss'], label = 'train loss')\n",
    "#axes.set_yscale(value = \"log\")\n",
    "axes.plot(history.history['val_loss'], label = 'val loss')\n",
    "axes.legend(loc = \"upper right\")\n",
    "axes.set_xlabel('Epoch')\n",
    "axes.set_ylabel('Loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving/loading trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('saved_models/teacher/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_student.save('saved_models/student_quantized/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('saved_models/teacher')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_student = tf.keras.models.load_model('saved_models/student_quantized')\n",
    "model_student.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qmodel.save('saved_models/qmodel1/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qmodel = tf.keras.models.load_model('saved_models/qmodel1')\n",
    "qmodel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('saved_models/compare_flatbig100/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('saved_models/compare_flatbig120')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feed all datasets into the trained model to compute prediction outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_predict = model.predict(X_train)\n",
    "X_test_predict = model.predict(X_test)\n",
    "MC_zb_predict = []\n",
    "for i in range(len(MC_zb)):\n",
    "    MC_zb_predict.append(model.predict(MC_zb[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define loss function to use for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss_for_pred(y_true, y_pred, choice):\n",
    "    #MSE\n",
    "    if choice == 0:\n",
    "        loss = np.mean((y_true - y_pred)**2, axis = (1, 2, 3))\n",
    "        return loss\n",
    "    \n",
    "    #MSE for de-noising model\n",
    "    if choice == 1:\n",
    "        loss = np.mean(y_pred**2, axis = (1, 2, 3))\n",
    "        return loss\n",
    "    \n",
    "    #VAE radius loss\n",
    "    if choice == 2:\n",
    "        loss = np.sqrt(np.sum(y_pred**2, axis = 1))\n",
    "        return loss\n",
    "    \n",
    "    #VAE KL loss\n",
    "    if choice == 3:\n",
    "        loss = -0.5 * np.sum(1.0 + y_pred - y_true**2 - np.exp(y_pred), axis = 1)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute loss for all samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_choice = 0\n",
    "\n",
    "X_train_loss = custom_loss_for_pred(X_train, X_train_predict, loss_choice)\n",
    "X_test_loss = custom_loss_for_pred(X_test, X_test_predict, loss_choice)\n",
    "\n",
    "MC_zb_loss = []\n",
    "for i in range(len(MC_zb)):\n",
    "    MC_zb_loss.append(custom_loss_for_pred(MC_zb[i], MC_zb_predict[i], loss_choice))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot loss distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbins = 40\n",
    "rmin = 0\n",
    "rmax = 50\n",
    "#plt.hist(X_train_loss, density = 1, bins = nbins, alpha = 0.3, label = 'train (ZeroBias)', range = (rmin, rmax), log = True)\n",
    "plt.hist(X_test_loss, density = 1, bins = nbins, alpha = 0.3, label = 'test (ZeroBias)', range = (rmin, rmax), log = True)\n",
    "plt.hist(MC_zb_loss[0], density = 1, bins = nbins, label = 'QCD', alpha = 0.1, histtype = 'stepfilled', range = (rmin, rmax))\n",
    "for i in range(3,7):\n",
    "    plt.hist(MC_zb_loss[i], density = 1, bins = nbins, label = MC_files[i], histtype = 'step', range = (rmin, rmax))\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "plt.xlabel(\"loss\")\n",
    "#plt.xticks(np.arange(rmin, rmax, step = 0.0002))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison between original and reconstructed inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Original vs Reconstructed\n",
    "#show_ZB = True\n",
    "show_ZB = False\n",
    "n = 5\n",
    "for i in range(780,790):\n",
    "    fig, ax = plt.subplots(figsize = (17,17))\n",
    "    if show_ZB == True:\n",
    "        print('ZB test\\nloss = ' + str(X_test_loss[i]))\n",
    "    else:\n",
    "        print(str(MC_files[n]) + '\\nloss = ' + str(MC_zb_loss[n][i]))\n",
    "    ax = plt.subplot(3, 3, 1)\n",
    "    if show_ZB == True:\n",
    "        ax = sns.heatmap(X_test[i,:,:,0].reshape(18, 14), vmin = 0, vmax = X_test[i,:,:,0].max(), cmap = \"Reds\", cbar_kws = {'label': 'ET (GeV)'})\n",
    "    else:\n",
    "        ax = sns.heatmap(MC_zb[n][i,:,:,0].reshape(18, 14), vmin = 0, vmax = MC_zb[n][i,:,:,0].max(), cmap = \"Reds\", cbar_kws = {'label': 'ET (GeV)'})\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    ax.set_title('Original')\n",
    "    \n",
    "    ax = plt.subplot(3, 3, 2)\n",
    "    if show_ZB == True:\n",
    "        ax = sns.heatmap(X_test_predict[i,:,:,0].reshape(18, 14), vmin = 0, vmax = X_test[i,:,:,0].max(), cmap = \"Reds\", cbar_kws = {'label': 'ET (GeV)'})\n",
    "    else:\n",
    "        ax = sns.heatmap(MC_zb_predict[n][i,:,:,0].reshape(18, 14), vmin = 0, vmax = MC_zb[n][i,:,:,0].max(), cmap = \"Reds\", cbar_kws = {'label': 'ET (GeV)'})\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    ax.set_title('Reconstructed')\n",
    "    \n",
    "    ax = plt.subplot(3, 3, 3)\n",
    "    if show_ZB == True:\n",
    "        ax = sns.heatmap(np.absolute(X_test_predict[i,:,:,0] - X_test[i,:,:,0]).reshape(18, 14), vmin = 0, vmax = X_test[i,:,:,0].max(), cmap = \"Reds\", cbar_kws = {'label': 'ET (GeV)'})\n",
    "    else:\n",
    "        ax = sns.heatmap(np.absolute(MC_zb_predict[n][i,:,:,0] - MC_zb[n][i,:,:,0]).reshape(18, 14), vmin = 0, vmax = MC_zb[n][i,:,:,0].max(), cmap = \"Reds\", cbar_kws = {'label': 'ET (GeV)'})\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    ax.set_title('abs(original-reconstructed)')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Knowledge Distillation (+ quantizing with QKeras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qkeras import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct student model without quantization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_in = layers.Input(shape = (18, 14, 1), name = \"In\")\n",
    "x = layers.Flatten(name = \"Flatten\")(x_in)\n",
    "x = layers.Dense(15, use_bias = False, name = \"Dense_1\")(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Activation('relu', name = \"Activation_1\")(x)\n",
    "x = layers.Dense(1, name = \"Out\")(x)\n",
    "\n",
    "model_student = tf.keras.models.Model(x_in, x)\n",
    "model_student.summary()\n",
    "model_student.compile(optimizer = 'adam', loss = 'mse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct student model with pre-defined quantization configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For quantization-aware training\n",
    "x_in = layers.Input(shape = (18, 14, 1))\n",
    "x = layers.Flatten()(x_in)\n",
    "x = QDense(26,\n",
    "           kernel_quantizer = quantized_bits(10, 5, 1),\n",
    "           bias_quantizer = quantized_bits(6, 3, 1))(x)\n",
    "x = QActivation('quantized_relu(bits=10, integer=5)')(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Dropout(rate = 0.3)(x)\n",
    "x = QDense(1,\n",
    "           kernel_quantizer = quantized_bits(10, 5, 1),\n",
    "           bias_quantizer = quantized_bits(6, 3, 1))(x)\n",
    "x = QActivation('quantized_relu(bits=10, integer=6)')(x)\n",
    "\n",
    "model_student = tf.keras.models.Model(x_in, x)\n",
    "model_student.summary()\n",
    "model_student.compile(optimizer = 'adam', loss = 'mse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the student model with knowledge distilled from a pre-trained teacher model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history_student = model_student.fit(X_train, X_train_loss,\n",
    "                                    epochs = 50,\n",
    "                                    validation_data = (X_val, X_val_loss),\n",
    "                                    batch_size = 1024,\n",
    "                                    callbacks = [\n",
    "                                        #tensorboard_callback,\n",
    "                                        tf.keras.callbacks.EarlyStopping(monitor = \"val_loss\", patience = 5, mode = \"min\")\n",
    "                                    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot loss vs epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15,10))\n",
    "axes = plt.subplot(2, 2, 1)\n",
    "axes.plot(history_student.history['loss'], label = 'train loss')\n",
    "#axes.set_yscale(value = \"log\")\n",
    "axes.plot(history_student.history['val_loss'], label = 'val loss')\n",
    "axes.legend(loc = \"upper right\")\n",
    "axes.set_xlabel('Epoch')\n",
    "axes.set_ylabel('Loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feed all datasets into the trained model to compute prediction outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_predict_student = model_student.predict(X_train)\n",
    "X_test_predict_student = model_student.predict(X_test)\n",
    "MC_zb_predict_student = []\n",
    "for i in range(len(MC_zb)):\n",
    "    MC_zb_predict_student.append(model_student.predict(MC_zb[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot loss distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbins = 40\n",
    "rmin = 0\n",
    "rmax = 70\n",
    "plt.hist(X_train_predict_student, density = 1, bins = nbins, alpha = 0.3, label = 'train (ZeroBias)', range = (rmin, rmax), log = True)\n",
    "plt.hist(X_test_predict_student, density = 1, bins = nbins, alpha = 0.3, label = 'test (ZeroBias)', range = (rmin, rmax))\n",
    "plt.hist(MC_zb_predict_student[0], density = 1, bins = nbins, label = 'QCD', alpha = 0.1, histtype = 'stepfilled', range = (rmin, rmax))\n",
    "plt.hist(MC_zb_predict_student[1], density = 1, bins = nbins, label = 'SingleNu', alpha = 0.1, histtype = 'stepfilled', range = (rmin, rmax))\n",
    "for i in range(40,50):\n",
    "    plt.hist(MC_zb_predict_student[i], density = 1, bins = nbins, label = MC_files[i], histtype = 'step', range = (rmin, rmax))\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "plt.xlabel(\"loss\")\n",
    "#plt.xticks(np.arange(rmin, rmax, step = 0.0002))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Knowledge Distillation (+ quantizing with AutoQKeras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qkeras import *\n",
    "from qkeras.autoqkeras import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric_for_autoqk(y_true, y_pred):\n",
    "    loss = 1.5 - tf.reduce_mean(tf.square(y_true - y_pred), axis = -1)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x_in = layers.Input(shape=(18,14,1),name=\"In\")\n",
    "x = layers.Flatten(name=\"Flatten\")(x_in)\n",
    "x = layers.Dense(15,use_bias=False,name=\"Dense_1\")(x)\n",
    "x = layers.Activation('relu',name=\"Activation_1\")(x)\n",
    "x = QBatchNormalization(name=\"QBN_1\")(x)\n",
    "x = layers.Dense(1,use_bias=False,name=\"Out\")(x)\n",
    "\n",
    "qmodel_original = tf.keras.models.Model(x_in, x)\n",
    "qmodel_original.summary()\n",
    "qmodel_original.compile(optimizer = 'adam', loss = 'mse', metrics = [metric_for_autoqk])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantization_config = {\n",
    "        \"kernel\": {\n",
    "                \"quantized_bits(2,0,1,alpha=1.0)\": 2,\n",
    "                \"quantized_bits(2,1,1,alpha=1.0)\": 2,\n",
    "                \"quantized_bits(3,0,1,alpha=1.0)\": 3,\n",
    "                \"quantized_bits(3,1,1,alpha=1.0)\": 3,\n",
    "                \"quantized_bits(4,0,1,alpha=1.0)\": 4,\n",
    "                \"quantized_bits(4,1,1,alpha=1.0)\": 4,\n",
    "                \"quantized_bits(4,2,1,alpha=1.0)\": 4,\n",
    "                \"quantized_bits(5,0,1,alpha=1.0)\": 5,\n",
    "                \"quantized_bits(5,2,1,alpha=1.0)\": 5,\n",
    "                \"quantized_bits(6,0,1,alpha=1.0)\": 6,\n",
    "                \"quantized_bits(6,2,1,alpha=1.0)\": 6\n",
    "        },\n",
    "        \"activation\": {\n",
    "                \"quantized_relu(2,0)\": 2,\n",
    "                \"quantized_relu(2,1)\": 2,\n",
    "                \"quantized_relu(3,0)\": 3,\n",
    "                \"quantized_relu(3,1)\": 3,\n",
    "                \"quantized_relu(4,0)\": 4,\n",
    "                \"quantized_relu(4,1)\": 4,\n",
    "                \"quantized_relu(5,0)\": 5,\n",
    "                \"quantized_relu(5,2)\": 5,\n",
    "                \"quantized_relu(6,2)\": 6\n",
    "        }\n",
    "}\n",
    "\n",
    "limit = {\n",
    "    \"Dense\": [6,6,6], # format for Dense is max bits for [kernel,bias,activation] \n",
    "    \"Activation\": [6] # format for Activation is max bits for [activation]\n",
    "    #\"BatchNormalization\": []\n",
    "}\n",
    "\n",
    "goal = {\n",
    "    \"type\": \"bits\", # energy, bits\n",
    "    \"params\": {\n",
    "        \"delta_p\": 8.0,\n",
    "        \"delta_n\": 8.0,\n",
    "        \"rate\": 2.0,\n",
    "        \"stress\": 1.0,\n",
    "        #\"process\": \"horowitz\",\n",
    "        #\"parameters_on_memory\": [\"sram\", \"sram\"],\n",
    "        #\"activations_on_memory\": [\"sram\", \"sram\"],\n",
    "        #\"rd_wr_on_io\": [False, False],\n",
    "        #\"min_sram_size\": [0, 0],\n",
    "        #\"source_quantizers\": [\"quantized_bits(bits=10,integer=10,symmetric=0,keep_negative=False)\"],\n",
    "        #\"reference_internal\": \"int8\",\n",
    "        #\"reference_accumulator\": \"int32\"\n",
    "        \"input_bits\": 8,\n",
    "        \"output_bits\": 8,\n",
    "        \"ref_bits\": 8,\n",
    "        \"config\": {\n",
    "            \"default\": [\"parameters\", \"activations\"]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "run_config = {\n",
    "  \"output_dir\": \"run_config\",\n",
    "  \"goal\": goal,\n",
    "  \"quantization_config\": quantization_config,\n",
    "  \"learning_rate_optimizer\": False, # False since still experimental\n",
    "  \"transfer_weights\": False, # False for the #filters/neurons to float\n",
    "  \"mode\": \"hyperband\", # random/bayesian/hyperband\n",
    "  \"seed\": 123,\n",
    "  \"limit\": limit,\n",
    "  \"tune_filters\": \"none\", # layer/block/none(no filter tunning at all)\n",
    "  \"tune_filters_exceptions\": \"\",\n",
    "  #\"layer_indexes\": range(1 + 1, len(qmodel_original.layers))\n",
    "  \"layer_indexes\": (2,3,5)\n",
    "}\n",
    "\n",
    "print(\"quantizing layers:\", [qmodel_original.layers[i].name for i in run_config[\"layer_indexes\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "autoqk = AutoQKeras(qmodel_original, metrics=[metric_for_autoqk], custom_objects={}, **run_config)\n",
    "autoqk.fit(X_train, X_train_loss, validation_data=(X_val, X_val_loss), batch_size=1024, epochs=15)\n",
    "# i = log(reference_size / trial_size) / log(rate)\n",
    "# delta = i * ( (i < 0) * delta_n + (i >= 0) * delta_p )\n",
    "# objective to maximize in the search is\n",
    "# adjusted score =  metric * (1 + delta), as formulated in the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qmodel = autoqk.get_best_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qmodel.save_weights(\"qmodel1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qmodel.load_weights(\"qmodel1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qmodel.compile(optimizer='adam', loss=\"mse\")\n",
    "history_qmodel = qmodel.fit(X_train, X_train_loss, epochs=50, batch_size=1024, validation_data=(X_val, X_val_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15,10))\n",
    "axes = plt.subplot(2, 2, 1)\n",
    "axes.plot(history_qmodel.history['loss'], label = 'train loss')\n",
    "#axes.set_yscale(value = \"log\")\n",
    "axes.plot(history_qmodel.history['val_loss'], label = 'val loss')\n",
    "axes.legend(loc = \"upper right\")\n",
    "axes.set_xlabel('Epoch')\n",
    "axes.set_ylabel('Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_predict_qmodel = qmodel.predict(X_train)\n",
    "X_test_predict_qmodel = qmodel.predict(X_test)\n",
    "MC_zb_predict_qmodel = []\n",
    "for i in range(len(MC_zb)):\n",
    "    MC_zb_predict_qmodel.append(qmodel.predict(MC_zb[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbins = 20\n",
    "rmin = 0\n",
    "rmax = 7\n",
    "#plt.hist(X_train_predict_qmodel, density = 1, bins = nbins, alpha = 0.3, label = 'train (ZeroBias)', range = (rmin, rmax), log = True)\n",
    "plt.hist(X_test_predict_qmodel, density = 1, bins = nbins, alpha = 0.3, label = 'test (ZeroBias)', range = (rmin, rmax), log = False)\n",
    "#plt.hist(MC_zb_predict_qmodel[0], density = 1, bins = nbins, label = 'QCD', alpha = 0.1, histtype = 'stepfilled', range = (rmin, rmax))\n",
    "#plt.hist(MC_zb_predict_qmodel[1], density = 1, bins = nbins, label = 'SingleNu', alpha = 0.1, histtype = 'stepfilled', range = (rmin, rmax))\n",
    "plt.hist(MC_zb_predict_qmodel[3], density = 1, bins = nbins, label = 'SM HH->4b', histtype = 'step', range = (rmin, rmax))\n",
    "plt.hist(MC_zb_predict_qmodel[4], density = 1, bins = nbins, label = 'ZPrime->qq', histtype = 'step', range = (rmin, rmax))\n",
    "plt.hist(MC_zb_predict_qmodel[5], density = 1, bins = nbins, label = 'H->2LongLived->4b', histtype = 'step', range = (rmin, rmax))\n",
    "for i in range(3,3):\n",
    "    plt.hist(MC_zb_predict_qmodel[i], density = 1, bins = nbins, label = MC_files[i], histtype = 'step', range = (rmin, rmax))\n",
    "#plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "#plt.legend(loc='center left', bbox_to_anchor=(0.57, 0.5))\n",
    "plt.legend(loc='upper right')\n",
    "plt.xlabel(\"loss\")\n",
    "#plt.xticks(np.arange(rmin, rmax, step = 0.0002))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorBoard (less useful)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "rm -rf ./logs/\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir = \"logs/fit\", histogram_freq = 1)\n",
    "%tensorboard --logdir logs/fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROC plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assigning labels and arranging for ROC plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Baseline\n",
    "#Assuming only the mean ZB is learned\n",
    "#Take mean ZB as outputs no matter what inputs are\n",
    "#Classifier of baseline = MSE(inputs, ZeroBias_mean)\n",
    "ZeroBias_mean = np.mean(ZeroBias, axis = 0)\n",
    "\n",
    "baseline_zb = np.mean((X_test - ZeroBias_mean)**2, axis = (1, 2))\n",
    "baseline_mc = []\n",
    "for i in range(len(MC_zb)):\n",
    "    baseline_mc.append(np.mean((MC_zb[i] - ZeroBias_mean)**2, axis = (1, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assign labels for various signals (y = 1) and backgrounds (y = 0)\n",
    "Y_zb = np.zeros((X_test.shape[0], 1))\n",
    "Y_mc = []\n",
    "for i in range(len(MC)):\n",
    "    Y_mc.append(np.ones((MC_zb[i].shape[0], 1)))\n",
    "\n",
    "#Concatenate datasets to make ROC curves, i.e. QCD/SingleNu/signals vs ZB\n",
    "\n",
    "#True labels\n",
    "Y_true = []\n",
    "#Baseline scores\n",
    "Y_baseline = []\n",
    "#Model scores\n",
    "Y_model = []\n",
    "#Student model scores\n",
    "Y_student = []\n",
    "#Qmodel scores\n",
    "#Y_qmodel = []\n",
    "for i in range(len(MC)):\n",
    "    Y_true.append(np.concatenate((Y_mc[i], Y_zb)))\n",
    "    Y_baseline.append(np.concatenate((baseline_mc[i], baseline_zb)))\n",
    "    Y_model.append(np.concatenate((MC_zb_loss[i], X_test_loss)))\n",
    "    #Y_student.append(np.concatenate((MC_zb_predict_student[i], X_test_predict_student)))\n",
    "    #Y_qmodel.append(np.concatenate((MC_zb_predict_qmodel[i], X_test_predict_qmodel)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (13, 13))\n",
    "axes = plt.subplot(2, 2, 1)\n",
    "fpr_baseline = []\n",
    "tpr_baseline = []\n",
    "thresholds_baseline = []\n",
    "roc_auc_baseline = []\n",
    "for i in range(len(MC)):\n",
    "    fpr_baseline.append(np.empty((Y_true[i].shape[0],1)))\n",
    "    tpr_baseline.append(np.empty((Y_true[i].shape[0],1)))\n",
    "    thresholds_baseline.append(np.empty((Y_true[i].shape[0],1)))\n",
    "    roc_auc_baseline.append(np.empty((Y_true[i].shape[0],1)))\n",
    "    fpr_baseline[i], tpr_baseline[i], thresholds_baseline[i] = roc_curve(Y_true[i], Y_baseline[i])\n",
    "    roc_auc_baseline[i] = auc(fpr_baseline[i], tpr_baseline[i])\n",
    "    if i == 0:\n",
    "        axes.plot(fpr_baseline[i], tpr_baseline[i], linestyle = '--', color = 'r', lw = 1, label = MC_files[i] + ' (AUC = %.8f)' % (roc_auc_baseline[i]))\n",
    "    elif i == 1 or i == 2:\n",
    "        print(1)\n",
    "        #axes.plot(fpr_baseline[i], tpr_baseline[i], linestyle = ':', lw = 1, label = MC_files[i] + ' (AUC = %.8f)' % (roc_auc_baseline[i]))\n",
    "    else:\n",
    "        axes.plot(fpr_baseline[i], tpr_baseline[i], linestyle = '-', lw = 1, label = MC_files[i] + ' (AUC = %.8f)' % (roc_auc_baseline[i]))\n",
    "#axes.plot([0, 1], [0, 1], linestyle = '--', lw = 1, color = 'black', label = 'random chance')\n",
    "axes.plot([0.002, 0.002], [0, 1], linestyle = '--', lw = 1, color = 'black', label = 'FPR = 0.2% ~ (100 kHz)/(ZB rate)')\n",
    "axes.set_xlim([0.00001, 1.0])\n",
    "#axes.set_xlim([0, 1.0])\n",
    "axes.set_ylim([0, 1.0])\n",
    "#axes.set_ylim([0.9, 1.0])\n",
    "axes.set_xscale(value = \"log\")\n",
    "#axes.set_yscale(value = \"log\")\n",
    "axes.set_xlabel('False Positive Rate (FPR)')\n",
    "axes.set_ylabel('True Positive Rate (TPR)')\n",
    "axes.set_title('Baseline ROC')\n",
    "axes.legend(loc='center left', bbox_to_anchor = (1.15, 0.5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teacher model ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (13, 13))\n",
    "axes = plt.subplot(2, 2, 1)\n",
    "fpr_model = []\n",
    "tpr_model = []\n",
    "thresholds_model = []\n",
    "roc_auc_model = []\n",
    "for i in range(len(MC)):\n",
    "    fpr_model.append(np.empty((Y_true[i].shape[0],1)))\n",
    "    tpr_model.append(np.empty((Y_true[i].shape[0],1)))\n",
    "    thresholds_model.append(np.empty((Y_true[i].shape[0],1)))\n",
    "    roc_auc_model.append(np.empty((Y_true[i].shape[0],1)))\n",
    "    fpr_model[i], tpr_model[i], thresholds_model[i] = roc_curve(Y_true[i], Y_model[i])\n",
    "    roc_auc_model[i] = auc(fpr_model[i], tpr_model[i])\n",
    "    if i == 0:\n",
    "        axes.plot(fpr_model[i], tpr_model[i], linestyle = '--', color = 'r', lw = 1, label = MC_files[i] + ' (AUC = %.8f)' % (roc_auc_model[i]))\n",
    "    elif i == 1 or i == 2:\n",
    "        print(1)\n",
    "        #axes.plot(fpr_model[i], tpr_model[i], linestyle = ':', lw = 1, label = MC_files[i] + ' (AUC = %.8f)' % (roc_auc_model[i]))\n",
    "    else:\n",
    "        axes.plot(fpr_model[i], tpr_model[i], linestyle = '-', lw = 1, label = MC_files[i] + ' (AUC = %.8f)' % (roc_auc_model[i]))\n",
    "#axes.plot([0, 1], [0, 1], linestyle = '--', lw = 1, color = 'black', label = 'random chance')\n",
    "axes.plot([0.002, 0.002], [0, 1], linestyle = '--', lw = 1, color = 'black', label = 'FPR = 0.2% ~ (100 kHz)/(ZB rate)')\n",
    "axes.set_xlim([0.00001, 1.0])\n",
    "#axes.set_xlim([0, 1.0])\n",
    "axes.set_ylim([0, 1.0])\n",
    "#axes.set_ylim([0.9, 1.0])\n",
    "axes.set_xscale(value = \"log\")\n",
    "#axes.set_yscale(value = \"log\")\n",
    "axes.set_xlabel('False Positive Rate (FPR)')\n",
    "axes.set_ylabel('True Positive Rate (TPR)')\n",
    "axes.set_title('Model ROC')\n",
    "axes.legend(loc='center left', bbox_to_anchor = (1.15, 0.5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Student model ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (13, 13))\n",
    "axes = plt.subplot(2, 2, 1)\n",
    "fpr_student = []\n",
    "tpr_student = []\n",
    "thresholds_student = []\n",
    "roc_auc_student = []\n",
    "for i in range(len(MC)):\n",
    "    fpr_student.append(np.empty((Y_true[i].shape[0],1)))\n",
    "    tpr_student.append(np.empty((Y_true[i].shape[0],1)))\n",
    "    thresholds_student.append(np.empty((Y_true[i].shape[0],1)))\n",
    "    roc_auc_student.append(np.empty((Y_true[i].shape[0],1)))\n",
    "    fpr_student[i], tpr_student[i], thresholds_student[i] = roc_curve(Y_true[i], Y_student[i])\n",
    "    roc_auc_student[i] = auc(fpr_student[i], tpr_student[i])\n",
    "    if i == 0:\n",
    "        axes.plot(fpr_student[i], tpr_student[i], linestyle = '--', color = 'r', lw = 1, label = MC_files[i] + ' (AUC = %.8f)' % (roc_auc_student[i]))\n",
    "    elif i == 1 or i == 2:\n",
    "        axes.plot(fpr_student[i], tpr_student[i], linestyle = ':', lw = 1, label = MC_files[i] + ' (AUC = %.8f)' % (roc_auc_student[i]))\n",
    "    else:\n",
    "        axes.plot(fpr_student[i], tpr_student[i], linestyle = '-', lw = 1, label = MC_files[i] + ' (AUC = %.8f)' % (roc_auc_student[i]))\n",
    "#axes.plot([0, 1], [0, 1], linestyle = '--', lw = 1, color = 'black', label = 'random chance')\n",
    "axes.plot([0.002, 0.002], [0, 1], linestyle = '--', lw = 1, color = 'black', label = 'FPR = 0.2% ~ (100 kHz)/(ZB rate)')\n",
    "axes.set_xlim([0.0001, 1.0])\n",
    "#axes.set_xlim([0, 1.0])\n",
    "axes.set_ylim([0, 1.0])\n",
    "#axes.set_ylim([0.1, 1.0])\n",
    "axes.set_xscale(value = \"log\")\n",
    "#axes.set_yscale(value = \"log\")\n",
    "axes.set_xlabel('False Positive Rate (FPR)')\n",
    "axes.set_ylabel('True Positive Rate (TPR)')\n",
    "axes.set_title('Student model ROC')\n",
    "axes.legend(loc='center left', bbox_to_anchor = (1.15, 0.5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantized student model ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (13, 13))\n",
    "axes = plt.subplot(2, 2, 1)\n",
    "fpr_qmodel = []\n",
    "tpr_qmodel = []\n",
    "thresholds_qmodel = []\n",
    "roc_auc_qmodel = []\n",
    "#for i in range(len(MC)-1):\n",
    "for i in range(len(MC)):\n",
    "    fpr_qmodel.append(np.empty((Y_true[i].shape[0],1)))\n",
    "    tpr_qmodel.append(np.empty((Y_true[i].shape[0],1)))\n",
    "    thresholds_qmodel.append(np.empty((Y_true[i].shape[0],1)))\n",
    "    roc_auc_qmodel.append(np.empty((Y_true[i].shape[0],1)))\n",
    "    fpr_qmodel[i], tpr_qmodel[i], thresholds_qmodel[i] = roc_curve(Y_true[i], Y_qmodel[i])\n",
    "    roc_auc_qmodel[i] = auc(fpr_qmodel[i], tpr_qmodel[i])\n",
    "    if i == 0:\n",
    "        print(1)\n",
    "        #axes.plot(fpr_qmodel[i], tpr_qmodel[i], linestyle = '--', color = 'r', lw = 2, label = MC_files[i] + ' (AUC = %.8f)' % (roc_auc_qmodel[i]))\n",
    "    elif i == 1 or i == 2:\n",
    "        #axes.plot(fpr_qmodel[i], tpr_qmodel[i], linestyle = ':', lw = 2, label = MC_files[i] + ' (AUC = %.8f)' % (roc_auc_qmodel[i]))\n",
    "        print(1)\n",
    "    else:\n",
    "        #axes.plot(fpr_qmodel[i], tpr_qmodel[i], linestyle = '-', lw = 2, label = MC_files[i] + ' (AUC = %.8f)' % (roc_auc_qmodel[i]))\n",
    "        print(1)\n",
    "axes.plot(fpr_qmodel[3], tpr_qmodel[3], linestyle = '-', lw = 2, label = 'SM HH->4b' + ' (AUC = %.5f)' % (roc_auc_qmodel[3]))\n",
    "axes.plot(fpr_qmodel[4], tpr_qmodel[4], linestyle = '-', lw = 2, label = 'ZPrime->qq' + ' (AUC = %.5f)' % (roc_auc_qmodel[4]))\n",
    "axes.plot(fpr_qmodel[5], tpr_qmodel[5], linestyle = '-', lw = 2, label = 'H->2LongLived->4b' + ' (AUC = %.5f)' % (roc_auc_qmodel[5]))\n",
    "#axes.plot([0, 1], [0, 1], linestyle = '--', lw = 2, color = 'black', label = 'random chance')\n",
    "axes.plot([0.002, 0.002], [0, 1], linestyle = '--', lw = 2, color = 'black', label = 'FPR = 0.2% ~ (100 kHz)/(ZB rate)')\n",
    "axes.set_xlim([0.00001, 1.0])\n",
    "#axes.set_xlim([0, 1.0])\n",
    "axes.set_ylim([0.0, 1.0])\n",
    "#axes.set_ylim([0.1, 1.0])\n",
    "axes.set_xscale(value = \"log\")\n",
    "#axes.set_yscale(value = \"log\")\n",
    "axes.set_xlabel('False Positive Rate (FPR)')\n",
    "axes.set_ylabel('True Positive Rate (TPR)')\n",
    "axes.set_title('Quantized student model ROC')\n",
    "#axes.legend(loc='center left', bbox_to_anchor = (1.15, 0.5))\n",
    "axes.legend(loc='center left', bbox_to_anchor = (0.33, 0.5))\n",
    "plt.show()\n",
    "\n",
    "### Tabulating TPR at fixed FPR = 0.2% (baseline, model, change)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_tpr_baseline = []\n",
    "table_tpr_model = []\n",
    "table_tpr_student = []\n",
    "table_tpr_qmodel = []\n",
    "table_tpr_change = []\n",
    "for i in range(len(fpr_baseline)):\n",
    "    for j in range(len(fpr_baseline[i])):\n",
    "        if fpr_baseline[i][j] > 0.002:\n",
    "            table_tpr_baseline.append(tpr_baseline[i][j] * 100)\n",
    "            break\n",
    "    for j in range(len(fpr_model[i])):\n",
    "        if fpr_model[i][j] > 0.002:\n",
    "            table_tpr_model.append(tpr_model[i][j] * 100)\n",
    "            break\n",
    "    for j in range(len(fpr_student[i])):\n",
    "        if fpr_student[i][j] > 0.002:\n",
    "            table_tpr_student.append(tpr_student[i][j] * 100)\n",
    "            break\n",
    "    for j in range(len(fpr_qmodel[i])):\n",
    "        if fpr_qmodel[i][j] > 0.002:\n",
    "            table_tpr_qmodel.append(tpr_qmodel[i][j] * 100)\n",
    "            break\n",
    "\n",
    "for i in range(len(MC)):\n",
    "    #table_tpr_change.append(100 * (table_tpr_model[i] - table_tpr_baseline[i])/table_tpr_baseline[i])\n",
    "    table_tpr_change.append(-table_tpr_student[i] + table_tpr_qmodel[i])\n",
    "\n",
    "table_tpr = pd.DataFrame({'Baseline': table_tpr_baseline,\n",
    "                          'CNN AE (teacher)': table_tpr_model,\n",
    "                          'Dense (student)': table_tpr_student,\n",
    "                          'qDense (qstudent)': table_tpr_qmodel,\n",
    "                          'delta(qDense, Dense)': table_tpr_change},\n",
    "                        index = MC_files)\n",
    "table_tpr = table_tpr.sort_values(by = 'delta(qDense, Dense)', ascending = False)\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "table_tpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@@@@@@@@@@@@"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "compare_old = tf.keras.models.load_model('saved_models/compare_old')\n",
    "compare_old.summary()\n",
    "compare_old_flat = tf.keras.models.load_model('saved_models/compare_old_flat')\n",
    "compare_old_flat.summary()\n",
    "compare_flat40 = tf.keras.models.load_model('saved_models/compare_flat40')\n",
    "compare_flat40.summary()\n",
    "compare_flat80 = tf.keras.models.load_model('saved_models/compare_flat80')\n",
    "compare_flat80.summary()\n",
    "compare_flat120 = tf.keras.models.load_model('saved_models/compare_flat120')\n",
    "compare_flat120.summary()\n",
    "compare_flat160 = tf.keras.models.load_model('saved_models/compare_flat160')\n",
    "compare_flat160.summary()\n",
    "compare_flat200 = tf.keras.models.load_model('saved_models/compare_flat200')\n",
    "compare_flat200.summary()\n",
    "compare_flatbig100 = tf.keras.models.load_model('saved_models/compare_flatbig100')\n",
    "compare_flatbig100.summary()\n",
    "compare_flatbig120 = tf.keras.models.load_model('saved_models/compare_flatbig120')\n",
    "compare_flatbig120.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_old = compare_old.predict(X_test)\n",
    "X_test_old_flat = compare_old_flat.predict(X_test)\n",
    "X_test_flat40 = compare_flat40.predict(X_test)\n",
    "X_test_flat80 = compare_flat80.predict(X_test)\n",
    "X_test_flat120 = compare_flat120.predict(X_test)\n",
    "X_test_flat160 = compare_flat160.predict(X_test)\n",
    "X_test_flat200 = compare_flat200.predict(X_test)\n",
    "X_test_flatbig100 = compare_flatbig100.predict(X_test)\n",
    "X_test_flatbig120 = compare_flatbig120.predict(X_test)\n",
    "MC_zb_old = []\n",
    "MC_zb_old_flat = []\n",
    "MC_zb_flat40 = []\n",
    "MC_zb_flat80 = []\n",
    "MC_zb_flat120 = []\n",
    "MC_zb_flat160 = []\n",
    "MC_zb_flat200 = []\n",
    "MC_zb_flatbig100 = []\n",
    "MC_zb_flatbig120 = []\n",
    "for i in range(len(MC_zb)):\n",
    "    MC_zb_old.append(compare_old.predict(MC_zb[i]))\n",
    "    MC_zb_old_flat.append(compare_old_flat.predict(MC_zb[i]))\n",
    "    MC_zb_flat40.append(compare_flat40.predict(MC_zb[i]))\n",
    "    MC_zb_flat80.append(compare_flat80.predict(MC_zb[i]))\n",
    "    MC_zb_flat120.append(compare_flat120.predict(MC_zb[i]))\n",
    "    MC_zb_flat160.append(compare_flat160.predict(MC_zb[i]))\n",
    "    MC_zb_flat200.append(compare_flat200.predict(MC_zb[i]))\n",
    "    MC_zb_flatbig100.append(compare_flatbig100.predict(MC_zb[i]))\n",
    "    MC_zb_flatbig120.append(compare_flatbig120.predict(MC_zb[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_choice = 0\n",
    "\n",
    "X_test_loss_old = custom_loss_for_pred(X_test, X_test_old, loss_choice)\n",
    "X_test_loss_old_flat = custom_loss_for_pred(X_test, X_test_old_flat, loss_choice)\n",
    "X_test_loss_flat40 = custom_loss_for_pred(X_test, X_test_flat40, loss_choice)\n",
    "X_test_loss_flat80 = custom_loss_for_pred(X_test, X_test_flat80, loss_choice)\n",
    "X_test_loss_flat120 = custom_loss_for_pred(X_test, X_test_flat120, loss_choice)\n",
    "X_test_loss_flat160 = custom_loss_for_pred(X_test, X_test_flat160, loss_choice)\n",
    "X_test_loss_flat200 = custom_loss_for_pred(X_test, X_test_flat200, loss_choice)\n",
    "X_test_loss_flatbig100 = custom_loss_for_pred(X_test, X_test_flatbig100, loss_choice)\n",
    "X_test_loss_flatbig120 = custom_loss_for_pred(X_test, X_test_flatbig120, loss_choice)\n",
    "\n",
    "MC_zb_loss_old = []\n",
    "MC_zb_loss_old_flat = []\n",
    "MC_zb_loss_flat40 = []\n",
    "MC_zb_loss_flat80 = []\n",
    "MC_zb_loss_flat120 = []\n",
    "MC_zb_loss_flat160 = []\n",
    "MC_zb_loss_flat200 = []\n",
    "MC_zb_loss_flatbig100 = []\n",
    "MC_zb_loss_flatbig120 = []\n",
    "for i in range(len(MC_zb)):\n",
    "    MC_zb_loss_old.append(custom_loss_for_pred(MC_zb[i], MC_zb_old[i], loss_choice))\n",
    "    MC_zb_loss_old_flat.append(custom_loss_for_pred(MC_zb[i], MC_zb_old_flat[i], loss_choice))\n",
    "    MC_zb_loss_flat40.append(custom_loss_for_pred(MC_zb[i], MC_zb_flat40[i], loss_choice))\n",
    "    MC_zb_loss_flat80.append(custom_loss_for_pred(MC_zb[i], MC_zb_flat80[i], loss_choice))\n",
    "    MC_zb_loss_flat120.append(custom_loss_for_pred(MC_zb[i], MC_zb_flat120[i], loss_choice))\n",
    "    MC_zb_loss_flat160.append(custom_loss_for_pred(MC_zb[i], MC_zb_flat160[i], loss_choice))\n",
    "    MC_zb_loss_flat200.append(custom_loss_for_pred(MC_zb[i], MC_zb_flat200[i], loss_choice))\n",
    "    MC_zb_loss_flatbig100.append(custom_loss_for_pred(MC_zb[i], MC_zb_flatbig100[i], loss_choice))\n",
    "    MC_zb_loss_flatbig120.append(custom_loss_for_pred(MC_zb[i], MC_zb_flatbig120[i], loss_choice))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ZeroBias_mean = np.mean(ZeroBias, axis = 0)\n",
    "\n",
    "baseline_zb = np.mean((X_test - ZeroBias_mean)**2, axis = (1, 2))\n",
    "baseline_mc = []\n",
    "for i in range(len(MC_zb)):\n",
    "    baseline_mc.append(np.mean((MC_zb[i] - ZeroBias_mean)**2, axis = (1, 2)))\n",
    "\n",
    "Y_zb = np.zeros((X_test.shape[0], 1))\n",
    "Y_mc = []\n",
    "for i in range(len(MC)):\n",
    "    Y_mc.append(np.ones((MC_zb[i].shape[0], 1)))\n",
    "\n",
    "Y_true = []\n",
    "Y_baseline = []\n",
    "Y_old = []\n",
    "Y_old_flat = []\n",
    "Y_flat40 = []\n",
    "Y_flat80 = []\n",
    "Y_flat120 = []\n",
    "Y_flat160 = []\n",
    "Y_flat200 = []\n",
    "Y_flatbig100 = []\n",
    "Y_flatbig120 = []\n",
    "for i in range(len(MC)):\n",
    "    Y_true.append(np.concatenate((Y_mc[i], Y_zb)))\n",
    "    Y_baseline.append(np.concatenate((baseline_mc[i], baseline_zb)))\n",
    "    Y_old.append(np.concatenate((MC_zb_loss_old[i], X_test_loss_old)))\n",
    "    Y_old_flat.append(np.concatenate((MC_zb_loss_old_flat[i], X_test_loss_old_flat)))\n",
    "    Y_flat40.append(np.concatenate((MC_zb_loss_flat40[i], X_test_loss_flat40)))\n",
    "    Y_flat80.append(np.concatenate((MC_zb_loss_flat80[i], X_test_loss_flat80)))\n",
    "    Y_flat120.append(np.concatenate((MC_zb_loss_flat120[i], X_test_loss_flat120)))\n",
    "    Y_flat160.append(np.concatenate((MC_zb_loss_flat160[i], X_test_loss_flat160)))\n",
    "    Y_flat200.append(np.concatenate((MC_zb_loss_flat200[i], X_test_loss_flat200)))\n",
    "    Y_flatbig100.append(np.concatenate((MC_zb_loss_flatbig100[i], X_test_loss_flatbig100)))\n",
    "    Y_flatbig120.append(np.concatenate((MC_zb_loss_flatbig120[i], X_test_loss_flatbig120)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 6\n",
    "\n",
    "plt.figure(figsize = (13, 13))\n",
    "axes = plt.subplot(2, 2, 1)\n",
    "\n",
    "fpr_baseline = np.empty((Y_true[n].shape[0],1))\n",
    "tpr_baseline = np.empty((Y_true[n].shape[0],1))\n",
    "thresholds_baseline = np.empty((Y_true[n].shape[0],1))\n",
    "roc_auc_baseline = np.empty((Y_true[n].shape[0],1))\n",
    "\n",
    "fpr_old = np.empty((Y_true[n].shape[0],1))\n",
    "tpr_old = np.empty((Y_true[n].shape[0],1))\n",
    "thresholds_old = np.empty((Y_true[n].shape[0],1))\n",
    "roc_auc_old = np.empty((Y_true[n].shape[0],1))\n",
    "\n",
    "fpr_old_flat = np.empty((Y_true[n].shape[0],1))\n",
    "tpr_old_flat = np.empty((Y_true[n].shape[0],1))\n",
    "thresholds_old_flat = np.empty((Y_true[n].shape[0],1))\n",
    "roc_auc_old_flat = np.empty((Y_true[n].shape[0],1))\n",
    "\n",
    "fpr_flat40 = np.empty((Y_true[n].shape[0],1))\n",
    "tpr_flat40 = np.empty((Y_true[n].shape[0],1))\n",
    "thresholds_flat40 = np.empty((Y_true[n].shape[0],1))\n",
    "roc_auc_flat40 = np.empty((Y_true[n].shape[0],1))\n",
    "\n",
    "fpr_flat80 = np.empty((Y_true[n].shape[0],1))\n",
    "tpr_flat80 = np.empty((Y_true[n].shape[0],1))\n",
    "thresholds_flat80 = np.empty((Y_true[n].shape[0],1))\n",
    "roc_auc_flat80 = np.empty((Y_true[n].shape[0],1))\n",
    "\n",
    "fpr_flat120 = np.empty((Y_true[n].shape[0],1))\n",
    "tpr_flat120 = np.empty((Y_true[n].shape[0],1))\n",
    "thresholds_flat120 = np.empty((Y_true[n].shape[0],1))\n",
    "roc_auc_flat120 = np.empty((Y_true[n].shape[0],1))\n",
    "\n",
    "fpr_flat160 = np.empty((Y_true[n].shape[0],1))\n",
    "tpr_flat160 = np.empty((Y_true[n].shape[0],1))\n",
    "thresholds_flat160 = np.empty((Y_true[n].shape[0],1))\n",
    "roc_auc_flat160 = np.empty((Y_true[n].shape[0],1))\n",
    "\n",
    "fpr_flat200 = np.empty((Y_true[n].shape[0],1))\n",
    "tpr_flat200 = np.empty((Y_true[n].shape[0],1))\n",
    "thresholds_flat200 = np.empty((Y_true[n].shape[0],1))\n",
    "roc_auc_flat200 = np.empty((Y_true[n].shape[0],1))\n",
    "\n",
    "fpr_flatbig100 = np.empty((Y_true[n].shape[0],1))\n",
    "tpr_flatbig100 = np.empty((Y_true[n].shape[0],1))\n",
    "thresholds_flatbig100 = np.empty((Y_true[n].shape[0],1))\n",
    "roc_auc_flatbig100 = np.empty((Y_true[n].shape[0],1))\n",
    "\n",
    "fpr_flatbig120 = np.empty((Y_true[n].shape[0],1))\n",
    "tpr_flatbig120 = np.empty((Y_true[n].shape[0],1))\n",
    "thresholds_flatbig120 = np.empty((Y_true[n].shape[0],1))\n",
    "roc_auc_flatbig120 = np.empty((Y_true[n].shape[0],1))\n",
    "\n",
    "fpr_baseline, tpr_baseline, thresholds_baseline = roc_curve(Y_true[n], Y_baseline[n])\n",
    "roc_auc_baseline = auc(fpr_baseline, tpr_baseline)\n",
    "\n",
    "fpr_old, tpr_old, thresholds_old = roc_curve(Y_true[n], Y_old[n])\n",
    "roc_auc_old = auc(fpr_old, tpr_old)\n",
    "\n",
    "fpr_old_flat, tpr_old_flat, thresholds_old_flat = roc_curve(Y_true[n], Y_old_flat[n])\n",
    "roc_auc_old_flat = auc(fpr_old_flat, tpr_old_flat)\n",
    "\n",
    "fpr_flat40, tpr_flat40, thresholds_flat40 = roc_curve(Y_true[n], Y_flat40[n])\n",
    "roc_auc_flat40 = auc(fpr_flat40, tpr_flat40)\n",
    "\n",
    "fpr_flat80, tpr_flat80, thresholds_flat80 = roc_curve(Y_true[n], Y_flat80[n])\n",
    "roc_auc_flat80 = auc(fpr_flat80, tpr_flat80)\n",
    "\n",
    "fpr_flat120, tpr_flat120, thresholds_flat120 = roc_curve(Y_true[n], Y_flat120[n])\n",
    "roc_auc_flat120 = auc(fpr_flat120, tpr_flat120)\n",
    "\n",
    "fpr_flat160, tpr_flat160, thresholds_flat160 = roc_curve(Y_true[n], Y_flat160[n])\n",
    "roc_auc_flat160 = auc(fpr_flat160, tpr_flat160)\n",
    "\n",
    "fpr_flat200, tpr_flat200, thresholds_flat200 = roc_curve(Y_true[n], Y_flat200[n])\n",
    "roc_auc_flat200 = auc(fpr_flat200, tpr_flat200)\n",
    "\n",
    "fpr_flatbig100, tpr_flatbig100, thresholds_flatbig100 = roc_curve(Y_true[n], Y_flatbig100[n])\n",
    "roc_auc_flatbig100 = auc(fpr_flatbig100, tpr_flatbig100)\n",
    "\n",
    "fpr_flatbig120, tpr_flatbig120, thresholds_flatbig120 = roc_curve(Y_true[n], Y_flatbig120[n])\n",
    "roc_auc_flatbig120 = auc(fpr_flatbig120, tpr_flatbig120)\n",
    "\n",
    "lw=3.5\n",
    "\n",
    "axes.plot(fpr_baseline, tpr_baseline, linestyle = '--', lw = lw, label = 'Cut-flow baseline (AUC = %.4f)' % (roc_auc_baseline))\n",
    "axes.plot(fpr_old, tpr_old, linestyle = '--', lw = lw, label = 'Old CNN AE, 2D latent space = 9x7 (AUC = %.4f)' % (roc_auc_old))\n",
    "axes.plot(fpr_flat40, tpr_flat40, linestyle = '-', lw = lw, label = 'CNN AE, flattened latent space = 40 (AUC = %.4f)' % (roc_auc_flat40))\n",
    "axes.plot(fpr_old_flat, tpr_old_flat, linestyle = '-', lw = lw, label = 'CNN AE, flattened latent space = 63 (AUC = %.4f)' % (roc_auc_old_flat))\n",
    "axes.plot(fpr_flat80, tpr_flat80, linestyle = '-', lw = lw, label = 'CNN AE, flattened latent space = 80 (AUC = %.4f)' % (roc_auc_flat80))\n",
    "#axes.plot(fpr_flat120, tpr_flat120, linestyle = '-', lw = lw, label = 'CNN AE, flattened latent space = 120 (AUC = %.4f)' % (roc_auc_flat120))\n",
    "axes.plot(fpr_flat160, tpr_flat160, linestyle = '-', color = 'blue', lw = lw, label = 'CNN AE, flattened latent space = 160 (AUC = %.4f)' % (roc_auc_flat160))\n",
    "axes.plot(fpr_flat200, tpr_flat200, linestyle = '-', color = 'pink', lw = lw, label = 'CNN AE, flattened latent space = 200 (AUC = %.4f)' % (roc_auc_flat200))\n",
    "#axes.plot(fpr_flatbig100, tpr_flatbig100, linestyle = '-', lw = lw, label = 'Big CNN AE, flattened latent space = 100 (AUC = %.5f)' % (roc_auc_flatbig100))\n",
    "#axes.plot(fpr_flatbig120, tpr_flatbig120, linestyle = '-', lw = lw, label = 'Big CNN AE, flattened latent space = 120 (AUC = %.5f)' % (roc_auc_flatbig120))\n",
    "\n",
    "axes.plot([0.002, 0.002], [0, 1], linestyle = '--', lw = 1, color = 'black', label = 'FPR = 0.2% ~ (100 kHz)/(ZB rate)')\n",
    "axes.set_xlim([0.00001, 1.0])\n",
    "#axes.set_xlim([0, 1.0])\n",
    "axes.set_ylim([0., 1.0])\n",
    "#axes.set_ylim([0.9, 1.0])\n",
    "axes.set_xscale(value = \"log\")\n",
    "#axes.set_yscale(value = \"log\")\n",
    "axes.set_xlabel('False Positive Rate (FPR)')\n",
    "axes.set_ylabel('True Positive Rate (TPR)')\n",
    "axes.set_title(MC_files[n] + ' vs ZB')\n",
    "axes.legend(loc='center left', bbox_to_anchor = (0.6, 0.5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
